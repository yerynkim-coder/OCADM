{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e0b870",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 4: Iterative Linear Quadratic Regulator**\n",
    "\n",
    "In this chapter, we present the implementation of the Iterative Linear Quadratic Regulator (ILQR) and demonstrate its effectiveness when applied to nonlinear control problems. The ILQR algorithm starts from an initial state and input trajectory and iteratively refines it by linearizing the system dynamics and quadratizing the cost function along the current trajectory. At each iteration, a time-varying local LQR problem is solved via a forward rollout followed by a backward pass, leading to an updated nominal trajectory. This process is repeated until convergence and is typically performed offline due to its batch nature and computational cost.\n",
    "\n",
    "We then proceed to highlight several important properties of the ILQR. First, we demonstrate the connection between ILQR and the infinite-horizon LQR controller, illustrating how ILQR generalizes LQR to nonlinear systems. Next, we evaluate the optimality of ILQR in terms of cost minimization. Finally, we will discuss the limitations of ILQR, including its sensitivity to initial guesses, and its limited ability to handle hard constraints. This motivate the exploration of more robust or constraint-aware extensions such as Model Predictive Control (MPC) in the next chapter.\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 4 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Iterative LQR</td>\n",
    "    <td>Iterative LQR Implementation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simulation and Visualization</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">ILQR's Properties</td>\n",
    "    <td>Relationship between LQR and ILQR</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ILQR's Optimality</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Limitations of ILQR</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "First, we need to set up our Python environment and import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1e8fa-a5c2-4219-a5d0-d6cc0ade876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e855fae",
   "metadata": {},
   "source": [
    "### **Problem Setup:**\n",
    "\n",
    "- Task: start from given initial position $p_0$, to reach a given terget position $p_T$ (Stabilization)\n",
    "\n",
    "- Slope profile (height $h$ with reference to horizontal displacement $p$):  \n",
    "   - case 1: zero slope (linear case), $h(p) = c$\n",
    "   - case 2: constant slope (linear case), $h(p) = \\frac{\\pi}{18} \\cdot p$\n",
    "   - case 3: varying slope for small disturbances (nonlinear case), $h(p) = k \\cdot \\cos(18 p)$\n",
    "   - case 4: varying slope for under actuated case (nonlinear case), $h(p) = \\begin{cases} k \\cdot \\sin(3 p), & p \\in [- \\frac{\\pi}{2}, \\frac{\\pi}{6}] \\\\ k, & p \\in (-\\infty, -\\frac{\\pi}{2}) \\cup (\\frac{\\pi}{6}, \\infty) \\end{cases}$\n",
    "\n",
    "- System dynmaics of 1d mountain car model (in State space representation): \n",
    "   - state vector $\\boldsymbol{x} = [p, v]^T$\n",
    "   - input vector $u$\n",
    "   - system dynamics:\n",
    "   \\begin{align*}\n",
    "     \\begin{bmatrix} \\dot{p} \\\\ \\dot{v} \\end{bmatrix} = \\begin{bmatrix} v \\\\ - g \\sin(\\theta) \\cos(\\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\cos(\\theta)  \\end{bmatrix} u\n",
    "   \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5ead5",
   "metadata": {},
   "source": [
    "### **Preparation: Define the Mountain Car Environment and the System Dynamics**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi symbolic system, including defining the profile over a slope $h(p)$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establishing the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "**Step 1: Specify the arguments for class `Env` and instantiate the class**\n",
    "\n",
    "- To start with the simpler case (also more compatible with LQR), we will initially focus on a linear system in an unconstrained scenario\n",
    "\n",
    "- Parameters for this task:  \n",
    "   - case: 1 (linear case)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "**Step 2: Call function `test_env()` to plot the mountain profile $h(p)$ and curve of inclination angle $\\theta(p)$**\n",
    "\n",
    "**Step 3: Specify the arguments for class `Dynmaics` and instantiate the class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d782c95-de15-49ec-a999-37ee410d8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2 or 3 or 4\n",
    "\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "# Arguments (without constraints): \n",
    "#   1) case: $n \\in [1, 2, 3, 4]$, type: int\n",
    "#   2) initial state: x_0 = [p_0, v_0], type: np.array\n",
    "#   3) terminal state: x_T = [p_T, v_T], type: np.array\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env.test_env() #  shape of slope (left side) and theta curve (right side) \n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "# Arguments: \n",
    "#   1) an object of class `Env`, type: Env  \n",
    "dynamics = Dynamics(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f7729",
   "metadata": {},
   "source": [
    "---\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993c0d4",
   "metadata": {},
   "source": [
    "### **Part (a): Iterative LQR Implementation**\n",
    "\n",
    "In this section, we will provide the implementation of the iterative LQR and demonstrate how it can be applied to a specific task (stabilization task). We will also show the resulting performance and the effects of using the ILQR controller in achieving optimal control.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b13ad3",
   "metadata": {},
   "source": [
    "##### **Recall from infinite-horizon LQR:**\n",
    "\n",
    "For a linear system:\n",
    "\n",
    "$$\n",
    "\\bm{x}_{k+1} = \\bm{A}_k \\bm{x}_k + \\bm{B}_k \\bm{u}_k\n",
    "$$\n",
    "\n",
    "The infinite-horizon discrete-time quadratic cost function is given by:\n",
    "\n",
    "$$\n",
    "J(\\bm{x}_0) = \\sum_{k=0}^{\\infty} \\left( (\\bm{x}_k - \\bm{x}_{ref})^T \\bm{Q} (\\bm{x}_k - \\bm{x}_{ref}) + \\bm{u}_k^T \\bm{R} \\bm{u}_k \\right)\n",
    "$$\n",
    "\n",
    "The solution to this problem is obtained by solving the **Discrete Algebraic Riccati Equation (DARE):**\n",
    "\n",
    "$$\n",
    "\\bm{S} = \\bm{Q} + \\bm{A}^T \\bm{S} \\bm{A} - (\\bm{A}^T \\bm{S} \\bm{B})(\\bm{R} + \\bm{B}^T \\bm{S} \\bm{B})^{-1} (\\bm{B}^T \\bm{S} \\bm{A})\n",
    "$$\n",
    "\n",
    "The optimal feedback control policy (LQR policy) is given by:\n",
    "\n",
    "$$\n",
    "\\bm{u^*} = \\bm{K} (\\bm{x}_k - \\bm{x}_{ref}), \\quad \\bm{K} = -\\left( \\bm{R} + \\bm{B}^T \\bm{S} \\bm{B} \\right)^{-1} \\left( \\bm{B}^T \\bm{S} \\bm{A} \\right)\n",
    "$$\n",
    "\n",
    "In the last section of the LQR chapter, we have already introduced the three major limitations of LQR controller: quadratic cost, linear dynamics and unconstrained. In the remainder of this chapter, we will introduce the iterative linear quadratic regulator (ILQR), which relaxes the first two limitations to a nonquadratic cost and nonlinear dynamcis.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da13586",
   "metadata": {},
   "source": [
    "##### **Problem Formulation of ILQR:**\n",
    "\n",
    "\n",
    "Consider a general time-invariant discrete-time nonlinear system:\n",
    "\n",
    "$$\n",
    "\\bm{x}_{k+1} = \\bm{f}(\\bm{x}_k, \\bm{u}_k)\n",
    "$$\n",
    "\n",
    "The general discrete-time cost (not necessarily in a quadratic form) with finite-horizon $N$ is given by:\n",
    "\n",
    "$$\n",
    "J(\\bm{x_0}) = g_N(\\bm{x}_N) + \\sum_{k=0}^{N-1} g_k(\\bm{x}_k, \\bm{u}_k)\n",
    "$$\n",
    "\n",
    "We want to solve the discrete-time optimal control policy for the problem defined ablove.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab1f3d",
   "metadata": {},
   "source": [
    "##### **ILQR Algorithm:**\n",
    "\n",
    "- **Initialization**: An initial (stabilizing) control policy $\\{\\bm{\\mu}_k^0(\\bm{x}_k)\\}_{k=0}^{N-1}$\n",
    "\n",
    "- **Recursion** $ l = \\{0, 1, ...\\} $:\n",
    "\n",
    "  1. **\"Forward pass\"**: Apply the current control policy to the nonlinear system and obtain state and input trajectories\n",
    "\n",
    "     $$\n",
    "     \\{ \\bm{\\bar{x}}_0^l, \\bm{\\bar{x}}_1^l, ..., \\bm{\\bar{x}}_N^l \\} \\quad \\text{and} \\quad \\{ \\bm{\\bar{u}}_0^l, \\bm{\\bar{u}}_1^l, ..., \\bm{\\bar{u}}_{N-1}^l \\}\n",
    "     $$\n",
    "  \n",
    "  2. Initialize **\"backward pass\"**:  \n",
    "     $$\n",
    "     \\bar{s}_N = \\bar{g}_N = g_N(\\bm{\\bar{x}}_N),\n",
    "     $$\n",
    "     $$\n",
    "     \\bm{s}_N = \\bm{q}_N = \\nabla_{\\bm{x}_N} g_N(\\bm{\\bar{x}}_N),\n",
    "     $$\n",
    "     $$\n",
    "     \\bm{S}_N = \\bm{Q}_N = \\nabla^2_{\\bm{x}_N} g_N(\\bm{\\bar{x}}_N),\n",
    "     $$\n",
    "\n",
    "  3. **\"Backward pass\"** $ k = \\{N-1, N-2, ..., 0\\} $:\n",
    "\n",
    "     i) Linearize the dynamics about $(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k)$ to obtain $ \\bm{A}_k $ and $ \\bm{B}_k $ in $\\bm{\\delta x}_{k+1} = \\bm{A}_k \\bm{\\delta x}_k + \\bm{B}_k \\bm{\\delta u}_k$:\n",
    "        $$\n",
    "        \\bm{A}_k = \\nabla_{\\bm{x}_k} \\bm{f}(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{B}_k = \\nabla_{\\bm{u}_k} \\bm{f}(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        where $ \\bm{\\delta x}_k = \\bm{x}_k - \\bm{\\bar{x}}_k $, $ \\quad \\bm{\\delta x}_{k+1} = \\bm{x}_{k+1} - \\bm{\\bar{x}}_{k+1} $, $\\quad$ and $ \\bm{\\delta u}_k = \\bm{u}_k - \\bm{\\bar{u}}_k $.\n",
    "\n",
    "        Hint: the derivation above is typically for discrete system dynamcis in the form of $\\bm{x}_{k+1} = \\bm{f}(\\bm{x}_k, \\bm{u}_k)$. For a continuous time system dynamcis $\\dot{\\bm{x}} = \\bm{f}(\\bm{x}, \\bm{u})$, we first have to do the linearization and then the discretization as shown in chapter 1, part (B).\n",
    "\n",
    "     ii) Approximate the stage cost $ g_k(\\bm{x}_k, \\bm{u}_k) $ with a second-order Taylor expansion about $ (\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k) $ to construct:\n",
    "        $$\n",
    "        \\bar{g}_k = g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{q}_k = \\nabla_{\\bm{x}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{Q}_k = \\nabla^2_{\\bm{x}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{r}_k = \\nabla_{\\bm{u}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{R}_k = \\nabla^2_{\\bm{u}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{P}_k = \\nabla_{\\bm{u}_k} \\left( \\nabla^T_{\\bm{x}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k) \\right),\n",
    "        $$\n",
    "\n",
    "     iii) Approximate the cost-to-go $ J_k^*(\\bm{x}_k) $ and $ J_{k+1}^*(\\bm{x}_{k+1}) $ with a second-order Taylor expansion to construct:\n",
    "        $$\n",
    "        \\bm{l}_k = \\bm{r}_k + \\bm{B}_k^T s_{k+1},\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{G}_k = \\bm{P}_k + \\bm{B}_k^T \\bm{S}_{k+1} \\bm{A}_k,\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{H}_k = \\bm{R}_k + \\bm{B}_k^T \\bm{S}_{k+1} \\bm{B}_k,\n",
    "        $$\n",
    "\n",
    "     iv) Solve the **Bellman Equation (DPA)**, which is quadratic in $ \\bm{\\delta u}_k $, and update the policy's coefficients:\n",
    "        $$\n",
    "        \\bm{\\delta u}_{k,ff}^* = - \\bm{H}_k^{-1} \\bm{l}_k,\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{K}_k = -\\bm{H}_k^{-1} \\bm{G}_k,\n",
    "        $$\n",
    "\n",
    "     v) Update the variance and the covariance matrixes:\n",
    "        $$\n",
    "        \\bar{s}_k = \\bar{g}_k + \\bar{s}_{k+1} + \\frac{1}{2} \\bm{\\delta u}_{k,ff}^{*T} \\bm{H}_k \\bm{\\delta u}_{k,ff}^{*} + \\bm{\\delta u}_{k,ff}^{*T} l_k,\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{s}_k = \\bm{q}_k + \\bm{A}_k^T \\bm{s}_{k+1} + \\bm{K}_k^T \\bm{H}_k \\bm{\\delta u}_{k,ff}^{*} + \\bm{K}_k^T \\bm{l}_k + \\bm{G}_k^T \\bm{\\delta u}_{k,ff}^{*},\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{S}_k = \\bm{}Q_k + \\bm{A}_k^T \\bm{S}_{k+1} \\bm{A}_k + \\bm{K}_k^T \\bm{H}_k \\bm{K}_k + \\bm{K}_k^T \\bm{G}_k + \\bm{G}_k^T \\bm{K}_k,\n",
    "        $$\n",
    "\n",
    "  4. Repeat until a termination condition is satisfied and return $ \\{\\bm{\\mu}_k^l(\\bm{x}_k)\\}_{k=0}^{N-1} $.\n",
    "\n",
    "- **Optimal Policy** :\n",
    "        $$\n",
    "        \\bm{\\mu}_k^l(\\bm{x}_k) = \\bm{u}_{k,ff} + \\bm{K}_k \\bm{x}_k\n",
    "        $$\n",
    "        with $ \\bm{u}_{k,ff} = \\bm{\\bar{u}}_k - \\bm{H}_k^{-1} \\bm{l}_k $ and $ \\bm{K}_k = -\\bm{H}_k^{-1} \\bm{G}_k $.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d49cc",
   "metadata": {},
   "source": [
    "##### **Step 1: Define the setup function for ILQR**  \n",
    "\n",
    "In this step, we will implement the setup function `setup_external()`, which will be manually called after a initial trajectory is obtained using LQR. For each iteration, you need to:\n",
    "\n",
    "1\\) Forward Pass: roll out the trajectory: apply the current policy to get the state series $\\{\\bm{x}_k^l\\}_{k=0}^{N}$ and input series $\\{\\bm{u}_k^l\\}_{k=0}^{N-1}$;  \n",
    "    Hint: you may use the method `one_step_forward()` from the class `Dynamics` to compute the next state (refer to chapter 1, part (B))\n",
    "\n",
    "2\\) Initialize Backward Pass: initialize $\\bar{s}_k, \\bm{s}_k, \\bm{S}_k$ with $\\bar{s}_N, \\bm{s}_N, \\bm{S}_N$;  \n",
    "\n",
    "3\\) Backward Pass Loop:  retrieve the linearized $\\bm{A}$ and $\\bm{B}$ matrices from the discrete-time system dynamics at the operating point;  \n",
    "    Hint: you may use the method `get_linearized_AB_discrete()` from the class `Dynamics` to get the discretized system matrices\n",
    "\n",
    "4\\) Backward Pass Loop: update $\\bar{g}_k, \\bm{q}_k, \\bm{Q}_k, \\bm{r}_k, \\bm{R}_k, \\bm{P}_k, \\bm{l}_k, \\bm{G}_k, \\bm{H}_k, \\bm{\\delta u}_{k,ff}^*, \\bm{K}_k, \\bar{s}_k, \\bm{s}_k, \\bm{S}_k$ sequentially;  \n",
    "    Hint: you may use the method `numpy.linalg.inv()` to solve the inverse matrix\n",
    "\n",
    "5\\) Check policy convergence: $\\max(\\{\\bm{u}_k^{l+1}\\}_{k=0}^{N-1} - \\{\\bm{u}_k^l\\}_{k=0}^{N-1}) \\leq \\Delta_{\\max}$ or $\\|\\{\\bm{u}_k^{l+1}\\}_{k=0}^{N-1} - \\{\\bm{u}_k^l\\}_{k=0}^{N-1}\\| \\leq \\Delta_{\\text{norm}}$\n",
    "\n",
    "Note that in this implementation, for a better performance comparison with the LQR controller, the stage cost and terminal cost is still considered to be a quadratic function, and the task is considered to be a stabilization task, i.e.:\n",
    "\n",
    "$$\n",
    "g_k(\\bm{x}_k, \\bm{u}_k) = (\\bm{x}_k - \\bm{x}_{ref})^T \\bm{Q} (\\bm{x}_k - \\bm{x}_{ref}) + \\bm{u}_k^T \\bm{R} \\bm{u}_k, \\quad k \\in [0, ..., N-1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_N(\\bm{x}_N) = (\\bm{x}_N - \\bm{x}_{ref})^T \\bm{Q}_f (\\bm{x}_N - \\bm{x}_{ref})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_external(self, input_traj: np.ndarray):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform ILQR to compute the optimal control sequence.\n",
    "    -----------------------------------------------------\n",
    "    Argument: input_traj (np.ndarray), the initial control sequence (typically generated by LQR).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start ILQR\n",
    "    N = len(input_traj)\n",
    "\n",
    "    # Initialize state and control trajectories\n",
    "    x_traj = np.zeros((self.dim_states, N+1))  # State trajector\n",
    "    u_traj = np.copy(input_traj)  # Control trajectory\n",
    "    x_traj[:, 0] = self.init_state  # Initial state\n",
    "    \n",
    "    # Initialize fb and ff gain\n",
    "    self.K_k_arr = np.zeros((self.dim_states, N))\n",
    "    self.u_kff_arr = np.zeros((N))\n",
    "\n",
    "    for n in range(self.max_iter):\n",
    "\n",
    "        # Forward pass: Simulate system using current control sequence\n",
    "        for k in range(N):\n",
    "            next_state = self.dynamics.one_step_forward(current_state=x_traj[:, k], current_input=u_traj[k], dt=self.dt)\n",
    "            x_traj[:, k + 1] = next_state\n",
    "\n",
    "        # Backward pass: Compute cost-to-go and update control\n",
    "        x_N_det = x_traj[:, -1] - self.target_state\n",
    "        x_N_det = x_N_det.reshape(-1, 1) # reshape into column vector\n",
    "        #print(f\"x_N_det: {x_N_det}\")\n",
    "\n",
    "        s_k_bar = (x_N_det.T @ self.Qf @ x_N_det) / 2 # Terminal cost\n",
    "        s_k = self.Qf @ x_N_det # Terminal cost gradient\n",
    "        S_k = self.Qf # Terminal cost Hessian\n",
    "\n",
    "        for k in range(N - 1, -1, -1):\n",
    "\n",
    "            # Linearize dynamics: f(x, u) ≈ A*x + B*u\n",
    "            A_lin, B_lin = self.dynamics.get_linearized_AB_discrete(current_state=x_traj[:, k], current_input=u_traj[k], dt=self.dt)\n",
    "\n",
    "            # Compute Q matrices\n",
    "            x_k_det = x_traj[:, k] - self.target_state\n",
    "            x_k_det = x_k_det.reshape(-1, 1) # reshape into column vector\n",
    "            \n",
    "            g_k_bar = (x_k_det.T @ self.Q @ x_k_det + self.R * u_traj[k] ** 2) * self.dt / 2\n",
    "            q_k = (self.Q @ x_k_det) * self.dt\n",
    "            Q_k = (self.Q) * self.dt\n",
    "            r_k = (self.R * u_traj[k]) * self.dt\n",
    "            R_k = (self.R) * self.dt\n",
    "            P_k = np.zeros((2,)) * self.dt # should be row vector\n",
    "\n",
    "            l_k = (r_k + B_lin.T @ s_k)\n",
    "            G_k = (P_k + B_lin.T @ S_k @ A_lin) # should be row vector\n",
    "            H_k = (R_k + B_lin.T @ S_k @ B_lin)\n",
    "\n",
    "            det_u_kff = - np.linalg.inv(H_k) @ l_k\n",
    "            K_k = - np.linalg.inv(H_k) @ G_k  # should be row vector\n",
    "            u_kff = u_traj[k] + det_u_kff - (K_k @ x_traj[:, k])\n",
    "\n",
    "            self.K_k_arr[:, k] = (K_k.T).flatten()\n",
    "            self.u_kff_arr[k] = u_kff.item()\n",
    "\n",
    "            s_k_bar = g_k_bar + s_k_bar + (det_u_kff.T @ H_k @ det_u_kff) / 2 + det_u_kff.T @ l_k\n",
    "            s_k = q_k + A_lin.T @ s_k + K_k.T @ H_k @ det_u_kff + K_k.T @ l_k + G_k.T @ det_u_kff\n",
    "            S_k = Q_k + A_lin.T @ S_k @ A_lin + K_k.T @ H_k @ K_k + K_k.T @ G_k + G_k.T @ K_k\n",
    "            \n",
    "        # Update control sequence\n",
    "        new_u_traj = np.zeros_like(u_traj)\n",
    "        new_x_traj = np.zeros_like(x_traj)\n",
    "        new_x_traj[:, 0] = self.init_state\n",
    "        \n",
    "        # Simulation forward to get input sequence\n",
    "        for k in range(N):\n",
    "            new_u_traj[k] = self.u_kff_arr[k] + self.K_k_arr[:, k].T @ new_x_traj[:, k]\n",
    "            next_state = self.dynamics.one_step_forward(current_state=new_x_traj[:, k], current_input=new_u_traj[k], dt=self.dt)\n",
    "            new_x_traj[:, k + 1] = next_state\n",
    "\n",
    "        # Compute total cost for this iteration\n",
    "        total_cost = 0.0\n",
    "        for k in range(N):\n",
    "            x_k_det = x_traj[:, k] - self.target_state\n",
    "            total_cost += 0.5 * (x_k_det.T @ self.Q @ x_k_det + self.R * u_traj[k] ** 2)\n",
    "        x_N_det = x_traj[:, -1] - self.target_state\n",
    "        total_cost += 0.5 * (x_N_det.T @ self.Qf @ x_N_det)\n",
    "        self.total_cost_list.append(total_cost.item())\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.max(np.abs(new_u_traj - u_traj)) < self.tol:\n",
    "            print(f\"Use {n} iteration until converge.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Iteration {n}: residual error is {np.max(np.abs(new_u_traj - u_traj))}\")\n",
    "            #print(f\"Old input trajectory: {u_traj.flatten()}\")\n",
    "            #print(f\"New input trajectory: {new_u_traj.flatten()}\")\n",
    "\n",
    "        u_traj = new_u_traj\n",
    "        x_traj = new_x_traj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6160",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### **Step 2: Bind the setup function to the class \"ILQRController\", and run the simulation to see the performance**  \n",
    "\n",
    "1\\) Bind the defined setup function `setup_external()` to class `ILQRController`;\n",
    "\n",
    "2\\) Specify the arguments and instantiate the controller class `LQRController` as `LQR`; \n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "    i) weight for state $\\bm{Q} = \\bm{Q}_f = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\bm{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "3\\) Instantiate the class `Simulator` for LQR and call function `run_simulation()` and `get_trajectories()` to get the simulated state- and input-trajectory;\n",
    "\n",
    "4\\) Instantiate the controller class `ILQRController` as `ILQR_0` and call function `setup()` to generate the optimal ILQR policy; \n",
    "\n",
    "5\\) Instantiate the class `Simulator` for ILQR and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "6\\) Instantiate the class `Visualizer` for ILQR, call function `display_final_results()` and `display_animation()` to show the simulations;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68387edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the defined DP algorithm to the corresponding class\n",
    "iLQRController.setup = setup_external\n",
    "\n",
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 6 # time length of simulation\n",
    "\n",
    "# Instantiate the LQR controller class to initialize ILQR controller\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) name: the name of current coltroller displayed in plots, type: string\n",
    "controller_lqr = LQRController(env, dynamics, Q, R, freq, name='LQR')\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr = Simulator(dynamics, controller_lqr, env, 1/freq, t_terminal)\n",
    "simulator_lqr.run_simulation()\n",
    "\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr = simulator_lqr.get_trajectories()\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state in stage cost, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input in stage cost, type: np.array  \n",
    "#                                         iii) `Qf`: weight metrix of state in terminal cost, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) name: the name of current coltroller displayed in plots, type: string\n",
    "controller_ilqr_0 = iLQRController(env, dynamics, Q, R, Qf, freq, name='ILQR_0')\n",
    "\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_0.setup(input_traj_lqr)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_0 = Simulator(dynamics, controller_ilqr_0, env, 1/freq, t_terminal)\n",
    "simulator_ilqr_0.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_0 = Visualizer(simulator_ilqr_0)\n",
    "visualizer_ilqr_0.display_plots()\n",
    "visualizer_ilqr_0.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c67a",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "---\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86082b",
   "metadata": {},
   "source": [
    "### **Part (C): LQR vs. ILQR**\n",
    "\n",
    "In this section, we will explore several aspects of how the ILQR algorithm compares to the LQR algorithm. We will begin by examining the relationship between LQR and ILQR under both linear and nonlinear system dynamics. Then we will delve deeper into the advantages of ILQR especially in the nonlinear case and provide visualizations to enhance understanding.  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86082b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **1. Relationship between LQR and ILQR**\n",
    "\n",
    "We will explore the relationship between LQR and ILQR through a comparative study. We design two test cases:  \n",
    "\n",
    " - `xxx_linear`: Using a purely linear system for the dynamics (EG: case 1)\n",
    "\n",
    " - `xxx_nonlinear`: Using a nonlinear system dynamics (EG: case 4)\n",
    "\n",
    "while keeping the same task definitions and cost design as in the linear case. By analyzing the results, we aim to understand how nonlinearity impacts the effectiveness of the LQR controller under identical conditions. You need to:  \n",
    "\n",
    "1\\) Define the parameters in stage and terminal cost and for simulation;\n",
    "\n",
    "2\\) For the linear case: Specify the case index as 1, instantiate classes `Env`, `Dynamcis`, `LQRController`, `ILQRController`, `Simulator` and `Visualizer`, and call function `display_contrast_plots()` to see the difference between LQR and ILQR policy; \n",
    "\n",
    "3\\) For the nonlinear case: Specify the case index as 4, instantiate classes `Env`, `Dynamcis`, `LQRController`, `ILQRController`, `Simulator` and `Visualizer`, and call function `display_contrast_plots()` to see the difference between LQR and ILQR policy; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20242aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([10, 10])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 8 # time length of simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear case\n",
    "# Define the index for linear case\n",
    "case_linear = 1\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_linear = Env(case_linear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env_linear.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_linear = Dynamics(env_linear)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize ILQR controller\n",
    "controller_lqr_linear = LQRController(env_linear, dynamics_linear, Q, R, freq, name='LQR_linear')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_linear = Simulator(dynamics_linear, controller_lqr_linear, env_linear, 1/freq, t_terminal)\n",
    "simulator_lqr_linear.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr_linear = simulator_lqr_linear.get_trajectories()\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_linear = iLQRController(env_linear, dynamics_linear, Q, R, Qf, freq, name='ILQR_linear')\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_linear.setup(input_traj_lqr_linear)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_linear = Simulator(dynamics_linear, controller_ilqr_linear, env_linear, 1/freq, t_terminal)\n",
    "simulator_ilqr_linear.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_linear = Visualizer(simulator_ilqr_linear)\n",
    "visualizer_ilqr_linear.display_contrast_plots(simulator_lqr_linear)\n",
    "visualizer_ilqr_linear.display_contrast_animation_same(simulator_lqr_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nonlinear case\n",
    "# Define the index for linear case\n",
    "case_nonlinear = 4\n",
    "\n",
    "# linearization point of the LQR\n",
    "state_lin = np.array([0.0, 0.0])\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env_nonlinear.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize ILQR controller\n",
    "controller_lqr_nonlinear = LQRController(env_nonlinear, dynamics_nonlinear, Q, R, freq, name='LQR_nonlinear')\n",
    "# Refresh the algorithm\n",
    "controller_lqr_nonlinear.set_lin_point(state_lin)\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear = Simulator(dynamics_nonlinear, controller_lqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr_nonlinear = simulator_lqr_nonlinear.get_trajectories()\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_nonlinear = iLQRController(env_nonlinear, dynamics_nonlinear, Q, R, Qf, freq, name='ILQR_nonlinear')\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear.setup(input_traj_lqr_nonlinear)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear = Simulator(dynamics_nonlinear, controller_ilqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_nonlinear = Visualizer(simulator_ilqr_nonlinear)\n",
    "visualizer_ilqr_nonlinear.display_contrast_plots(simulator_lqr_nonlinear)\n",
    "visualizer_ilqr_nonlinear.display_contrast_animation_same(simulator_lqr_nonlinear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671287ca",
   "metadata": {},
   "source": [
    "#### **Results Analysis:**\n",
    "\n",
    "From the results shown in the figure, a significant difference can be observed when applying the LQR control law to a system with nonlinear dynamics versus a linear system:  \n",
    "\n",
    "- **Linear Dynamics** (`LQR_linear`, `ILQR_linear`): In the linear-quadratic setting, where system dynamics are strictly linear (case 1 or case 2) and the cost function is purely quadratic, ILQR coincides with the LQR solution, leading to the same optimal policy.  \n",
    "\n",
    "- **Nonlinear Dynamics** (`LQR_nonlinear`, `ILQR_nonlinear`): However, when the system dynamics become nonlinear (case 3 or case 4), ILQR adapts by iteratively approximating both the dynamics and the cost around a nominal trajectory, where LQR can only linearize the system dynamics around a single operating point. From the position curve we observe that the LQR overshoots while the ILQR controller can gradually drive the car to the target position.\n",
    "\n",
    "#### **Main Conclusion:**\n",
    "\n",
    "Compared to the LQR algorithm, this iterative procedure allows for finding a locally optimal policy that is no longer captured by classical LQR. In this sense, ILQR serves as a more general framework, reducing to LQR in the simpler linear-quadratic case but extending to broader classes of nonlinear systems and costs.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055c152",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **2. How does ILQR outperform LQR in a nonlinear case?**\n",
    "\n",
    "In the last section we have already seen that LQR is a special case of ILQR. For nonlinear problems, ILQR will lead to a more optimal policy compared to the LQR controller. The optimality here is subject to the optimizatioin problem, which is determined by all the components (dynamcis, cost, and constraints). In this section, we provide you with some quantitative results to further address this consequece. You need to:  \n",
    "\n",
    "1\\) Define the index for the nonlinear test case and the parameters for the simulation;\n",
    "\n",
    "2\\) For LQR and ILQR: Instantiate classes `Env`, `Dynamcis`, `LQRController`, `ILQRController` and `Simulator`; \n",
    "\n",
    "3\\) For LQR and ILQR: call function `compute_cost2go()` to evaluate the cost-to-go backwards;\n",
    "\n",
    "4\\) Call function `display_contrast_cost2go()` to visualize the comparison between the LQR cost and the ILQR cost w.r.t. time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nonlinear case\n",
    "# Define the index for linear case\n",
    "case_nonlinear_1 = 4 \n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_1 = Env(case_nonlinear_1, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "#env_nonlinear_1.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear_1 = Dynamics(env_nonlinear_1)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize ILQR controller\n",
    "controller_lqr_nonlinear_1 = LQRController(env_nonlinear_1, dynamics_nonlinear_1, Q, R, freq, name='LQR_nonlinear_1')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear_1 = Simulator(dynamics_nonlinear_1, controller_lqr_nonlinear_1, env_nonlinear_1, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear_1.run_simulation()\n",
    "\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr_nonlinear_1 = simulator_lqr_nonlinear_1.get_trajectories()\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_nonlinear_1 = iLQRController(env_nonlinear_1, dynamics_nonlinear_1, Q, R, Qf, freq, name='ILQR_nonlinear_1')\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear_1.setup(input_traj_lqr_nonlinear_1)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear_1 = Simulator(dynamics_nonlinear_1, controller_ilqr_nonlinear_1, env_nonlinear_1, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear_1.run_simulation()\n",
    "\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_lqr_nonlinear_1 = simulator_lqr_nonlinear_1.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of LQR controller: {cost2go_lqr_nonlinear_1[0]}\")\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_ilqr_nonlinear_1 = simulator_ilqr_nonlinear_1.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of iLQR controller: {cost2go_ilqr_nonlinear_1[0]}\")\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_1 = Visualizer(simulator_ilqr_nonlinear_1)\n",
    "visualizer_ilqr_nonlinear_1.display_contrast_cost2go(simulator_lqr_nonlinear_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6d926",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "From the results, we observe that in the case of nonlinear dynamics, the policy computed by ILQR yields a lower cost-to-go compared to the LQR policy, indicating a more optimal solution. This is because ILQR better accounts for system nonlinearity during the optimization process, rather than relying on a single linearization around a reference point.\n",
    "\n",
    "----\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6fa53",
   "metadata": {},
   "source": [
    "#### **3. Limitations of ILQR**\n",
    "\n",
    "This section illustrates several limitations of the ILQR algorithm, particularly its sensitivity to the choice of the initial trajectory. Although ILQR performs iterative refinement, an ill-conditioned initialization may lead to divergence or failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09164cc",
   "metadata": {},
   "source": [
    "##### **Limitation 1: Requires goal-reaching initialization**\n",
    "\n",
    "When using ILQR to stabilize a system around a target state (e.g., a fixed point), the initial control trajectory must be capable of roughly reaching the target. Otherwise, the local approximations used by ILQR may become invalid, and the algorithm may fail to converge.\n",
    "\n",
    "In this example, we consider the effect of input constraints by applying clipping to the initial trajectory generated by the LQR policy. As demonstrated in Chapter 2, when the control input is insufficient, the car is unable to climb to the top and instead oscillates within the valley. We use such a trajectory as the initialization for the ILQR algorithm, run the simulation, and observe the resulting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([10, 10])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define the case\n",
    "case_nonlinear = 4\n",
    "\n",
    "# Define a upper bound of input a\n",
    "input_ubs = 8.0\n",
    "input_lbs = -5.0\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_clipped = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]), \n",
    "                           input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear_clipped)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear_clipped = LQRController(env_nonlinear_clipped, dynamics_nonlinear, Q, R, freq, name='LQR_case4_clipped')\n",
    "controller_lqr_nonlinear_clipped.set_lin_point(np.array([-0.5, 0.0]))\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear_clipped = Simulator(dynamics_nonlinear, controller_lqr_nonlinear_clipped, env_nonlinear_clipped, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear_clipped.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr_nonlinear_clipped = simulator_lqr_nonlinear_clipped.get_trajectories()\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_nonlinear_clipped = iLQRController(env_nonlinear_clipped, dynamics_nonlinear, Q, R, Qf, freq, name='ILQR_case4_clipped')\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear_clipped.setup(input_traj_lqr_nonlinear_clipped)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear_clipped = Simulator(dynamics_nonlinear, controller_ilqr_nonlinear_clipped, env_nonlinear_clipped, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear_clipped.run_simulation()\n",
    "# Instantiate the visualizer\n",
    "visualizer_ilqr_nonlinear_clipped = Visualizer(simulator_ilqr_nonlinear_clipped)\n",
    "\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_lqr_nonlinear_clipped = simulator_lqr_nonlinear_clipped.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of LQR controller: {cost2go_lqr_nonlinear_1[0]}\")\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_ilqr_nonlinear_clipped = simulator_ilqr_nonlinear_clipped.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of iLQR controller: {cost2go_ilqr_nonlinear_1[0]}\")\n",
    "\n",
    "# Plot contrast on cost\n",
    "visualizer_ilqr_nonlinear_clipped.display_contrast_cost2go(simulator_lqr_nonlinear_clipped)\n",
    "\n",
    "# Display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_clipped.display_contrast_plots(simulator_lqr_nonlinear_clipped)\n",
    "visualizer_ilqr_nonlinear_clipped.display_contrast_animation_same(simulator_lqr_nonlinear_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecff86",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "From the experimental results, we observe that the initial LQR trajectory (dashed red) fails to guide the system toward the target at the top of the hill. Due to input clipping, the control authority is insufficient to overcome the slope. As a result, the trajectory remains confined to the valley, oscillating around a suboptimal equilibrium far from the goal.\n",
    "\n",
    "The ILQR algorithm is initialized with this clipped trajectory and attempts to optimize it. However, because the initial trajectory never approaches the goal region, the backward pass of ILQR—which linearizes the dynamics and quadratizes the cost along the reference—constructs its local approximations entirely within the valley. This limits the algorithm’s ability to discover and correct toward the true optimal path.\n",
    "\n",
    "Despite several iterations, the ILQR-refined trajectory (solid blue) remains trapped within the same local basin as the initial trajectory. The system fails to generate the necessary momentum to escape the valley and reach the target. Notably, both the position and velocity profiles stay well below the desired final state, and the resulting input remains within bounds but lacks the structure needed to initiate a climb.\n",
    "\n",
    "This experiment further supports the conclusion that ILQR’s success is tightly coupled to the quality of its initialization. When the initial trajectory does not reach the vicinity of the optimal solution, especially under nonlinear dynamics and input constraints, ILQR may converge to a locally optimal but task-irrelevant policy.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef3e2c",
   "metadata": {},
   "source": [
    "##### **Limitation 2: Hyperparameter sensitivity**\n",
    "\n",
    "When LQR is applied to linear dynamical systems, its convergence is typically global—meaning that, regardless of hyperparameter tuning, it can drive the system from any initial state to the target equilibrium. In contrast, ILQR, which is used for nonlinear systems, generally exhibits only local convergence. The size of its region of attraction (RoA) is sensitive to hyperparameter choices: **with well-tuned parameters, the RoA may be large enough to include the given initial state, allowing ILQR to successfully converge to the target; otherwise, convergence cannot be guaranteed and the algorithm may even converge to an unintended equilibrium state that does not align with the control objective.** Therefore, when using the ILQR algorithm, careful hyperparameter tuning is essential to ensure convergence to the desired equilibrium state.\n",
    "\n",
    "To illustrate this point, we again use the unconstrained Profile 4 as the testing environment, but this time with a different choice of the weighting matrix $\\boldsymbol{Q}$ compared to the first example (smaller $\\boldsymbol{Q}$), in order to demonstrate how hyperparameter selection affects the performance of the ILQR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "## Nonlinear case\n",
    "# Define the index for linear case\n",
    "case_nonlinear_2 = 4\n",
    "\n",
    "# linearization point of the LQR\n",
    "state_lin = np.array([0.0, 0.0])\n",
    "\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_2 = Env(case_nonlinear_2, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "#env_nonlinear_2.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear_2 = Dynamics(env_nonlinear_2)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize ILQR controller\n",
    "controller_lqr_nonlinear_2 = LQRController(env_nonlinear_2, dynamics_nonlinear_2, Q, R, freq, name='LQR_nonlinear_2')\n",
    "# Refresh the algorithm\n",
    "controller_lqr_nonlinear_2.set_lin_point(state_lin)\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear_2 = Simulator(dynamics_nonlinear_2, controller_lqr_nonlinear_2, env_nonlinear_2, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear_2.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr_nonlinear_2 = simulator_lqr_nonlinear_2.get_trajectories()\n",
    "\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_nonlinear_2 = iLQRController(env_nonlinear_2, dynamics_nonlinear_2, Q, R, Qf, freq, name='ILQR_nonlinear_2')\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear_2.setup(input_traj_lqr_nonlinear_2)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear_2 = Simulator(dynamics_nonlinear_2, controller_ilqr_nonlinear_2, env_nonlinear_2, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear_2.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_2 = Visualizer(simulator_ilqr_nonlinear_2)\n",
    "\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_lqr_nonlinear_2 = simulator_lqr_nonlinear_2.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of LQR controller: {cost2go_lqr_nonlinear_2[0]}\")\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_ilqr_nonlinear_2 = simulator_ilqr_nonlinear_2.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of iLQR controller: {cost2go_ilqr_nonlinear_2[0]}\")\n",
    "\n",
    "# Plot contrast\n",
    "visualizer_ilqr_nonlinear_2.display_contrast_cost2go(simulator_lqr_nonlinear_2)\n",
    "\n",
    "# Display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_2.display_contrast_plots(simulator_lqr_nonlinear_2)\n",
    "visualizer_ilqr_nonlinear_2.display_contrast_animation_same(simulator_lqr_nonlinear_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013ce35",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "From the simulation results, we observe that although LQR provides a dynamically feasible initial trajectory, ILQR converges to an incorrect equilibrium—the cart remains near the bottom of the valley with almost no movement. This outcome is caused by improper hyperparameter tuning: by reducing the weights in the $\\boldsymbol{Q}$ matrix while keeping the $\\boldsymbol{R}$ matrix unchanged, ILQR prioritizes minimizing control effort over reducing the deviation between the current state and the desired target. As a result, the algorithm settles for a solution that requires less input but fails to reach the goal, highlighting the sensitivity of ILQR's performance to the choice of cost weights in nonlinear settings.\n",
    "\n",
    "----\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb8cdb",
   "metadata": {},
   "source": [
    "#### **Wrap-up:**\n",
    "\n",
    "In this chapter, we introduced the principles of the ILQR controller, provided a simple implementation, and visualized its performance in comparison with the LQR controller. The results demonstrate that ILQR is better suited for handling:\n",
    "\n",
    "- Nonlinearty in system dynamcis\n",
    "\n",
    "- Non-quadratic cost function\n",
    "\n",
    "However, as a drawback, ILQR:\n",
    "\n",
    "- Requires greater computational effort\n",
    "\n",
    "- Typically limited to offline computation\n",
    "\n",
    "- Similar to LQR, it also do not support explicit definition of the state- and input-constraints. \n",
    "\n",
    "In the following chapters, we will introduce Model Predictive Control (MPC), which approximates an infinite-horizon optimal control problem with a finite and receding horizon formulation. This enables constraint handling and significantly broadens its applicability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
