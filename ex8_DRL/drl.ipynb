{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0980c3b",
   "metadata": {},
   "source": [
    "### **Chapter 8: Deep Reinforcement Learning**\n",
    "\n",
    "In this chapter, we introduce **deep reinforcement learning (DRL)** ‚Äî a powerful framework for solving complex optimal control problems in nonlinear and high-dimensional systems. Our focus is on **Proximal Policy Optimization (PPO)**, one of the most popular and effective policy gradient algorithms in modern DRL. Later in this chapter, we will compare PPO with NMPC to highlight their respective strengths and limitations in practical control scenarios.\n",
    "\n",
    "First, we need to set up our Python environment and import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9921b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ppo_utils import PPOController\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595ba8b",
   "metadata": {},
   "source": [
    "### **Preparation: specify some common task parameters**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi, including the definition of the mountain profile as a function of $p$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establishing the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "   - case: 4 (hilly terrain)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "   - state space: $ \\mathcal{X}_1 = [-1.7, 1.3]$,  $ \\mathcal{X}_2 = [-3.5, 3.5]$\n",
    "\n",
    "   - input space: $ \\mathcal{U} = [-5.0, 5.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f258ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 4\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "state_lbs = np.array([initial_position-1.2, -3.5])\n",
    "state_ubs = np.array([target_position+0.7, 3.5])\n",
    "input_lbs = -5.0\n",
    "input_ubs = 5.0\n",
    "\n",
    "# Define time length and freq for simulation\n",
    "t_terminal = 10\n",
    "freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69129a9e",
   "metadata": {},
   "source": [
    "Meanwhile, later in this chapter, we will compare the performance of PPO controller with traditional model-based control methods represented by MPC. Therefore, we can predefine a few MPC controller configurations here for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43928347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMPC parameters (as reference)\n",
    "# To find a feasible solution, the MPC requires a bit difference discretization and larger bounds\n",
    "freq_mpc = 20\n",
    "dt_mpc = 1.0/freq\n",
    "state_lbs_mpc = np.array([-2.0, -4.0])\n",
    "state_ubs_mpc = np.array([2.0, 4.0])\n",
    "\n",
    "# Define weight matrix in stage and terminal cost and the horizon for MPC (reference controller)\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "N = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea993624",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Proximal Policy Optimization (PPO)**\n",
    "\n",
    "Proximal Policy Optimization (PPO), as introduced by Schulman et al. in their original paper *‚ÄúProximal Policy Optimization Algorithms‚Äù* (2017), is one of the most popular policy gradient methods in Reinforcement Learning. It was proposed by OpenAI as a **simplified and more stable alternative** to Trust Region Policy Optimization (TRPO). PPO belongs to the family of **on-policy actor-critic methods**, and it‚Äôs widely used in both research and practical applications (e.g., training agents to play Dota 2, as shown in the core course slide). The core idea is to **avoid large destructive updates** to the policy by **constraining or clipping** how much the policy is allowed to change at each step, thereby preventing performance degradation or training instability caused by overly aggressive policy updates ‚Äî this is precisely where the term \"Proximal\" comes from.\n",
    "\n",
    "* Schulman et al. (2017): [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Core Components of PPO**\n",
    "\n",
    "* **Actor-Critic Structure**: \n",
    "\n",
    "   PPO adopts an actor-critic architecture, which is a foundational design pattern in modern policy gradient reinforcement learning methods. This structure separates the roles of **decision making (actor)** and **value estimation (critic)**.\n",
    "\n",
    "   * **Actor**: The Policy Network\n",
    "\n",
    "     * The **actor** is responsible for selecting actions based on the current state.\n",
    "     * It parameterizes the **policy** $\\pi_\\theta(a|s)$, which is a probability distribution over actions given the state $s$.\n",
    "     * During training, the actor is updated to maximize expected return by following the **policy gradient**, typically using advantage-weighted updates:\n",
    "     $$\n",
    "     \\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot \\hat{A}(s, a) \\right]\n",
    "     $$\n",
    "   \n",
    "   * **Critic**: The Value Network\n",
    "\n",
    "     * The **critic** provides a **baseline estimate** of how good a state is, helping to reduce the variance of the policy gradient.\n",
    "     * It learns the **state value function** $V^\\pi(s)$, which estimates the expected return from state $s$ under the current policy $\\pi$.\n",
    "     * The critic is typically trained by minimizing the squared error between the predicted value and empirical return (or bootstrapped target):\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{critic}} = \\left( V_\\phi(s_t) - R_t \\right)^2\n",
    "     $$\n",
    "\n",
    "\n",
    "* **Advantage Estimation with State-Dependent Baseline (GAE)**:\n",
    "\n",
    "  * PPO reduces variance in the policy gradient by using a **state-dependent baseline**, typically the value function $V(s)$, instead of relying solely on raw returns $R_t$.\n",
    "  * This leads to the **advantage function**:\n",
    "    $$\n",
    "    \\hat{A}_t = R_t - V(s_t)\n",
    "    $$\n",
    "    which measures how much better or worse an action performed compared to the expected value of the state.\n",
    "  * To further improve this estimate, PPO employs **Generalized Advantage Estimation (GAE)**, which blends multi-step bootstrapping and Monte Carlo returns to strike a balance between bias and variance. GAE defines the advantage recursively via the **TD residual** $\\delta_t$:\n",
    "    $$\n",
    "    \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "    $$\n",
    "    and constructs the advantage as an exponentially weighted sum:\n",
    "    $$\n",
    "    \\hat{A}_t^{GAE(\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}\n",
    "    $$\n",
    "  * Here, $\\lambda \\in [0, 1]$ controls the trade-off: lower values reduce variance but introduce more bias, while higher values favor accuracy but increase variance. This technique leads to smoother, more stable advantage estimates and more efficient learning.\n",
    "\n",
    "\n",
    "* **Clipped Surrogate Objective**:\n",
    "\n",
    "  * PPO avoids the complexity of trust region methods (like TRPO) by using a **clipping mechanism** to constrain how much the new policy $\\pi_\\theta$ is allowed to deviate from the old policy $\\pi_{\\theta_{\\text{old}}}$ during each update.\n",
    "  * The key quantity is the **probability ratio**:\n",
    "    $$\n",
    "    r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}\n",
    "    $$\n",
    "    which measures how much more (or less) likely the new policy is to take action $a_t$ at state $s_t$ compared to the old policy.\n",
    "  * The **clipped surrogate objective** then takes the minimum between the unclipped and clipped policy objective:\n",
    "    $$\n",
    "    L^{CLIP}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\n",
    "    $$\n",
    "    where $\\hat{A}_t$ is the estimated advantage and $\\epsilon$ is a small positive hyperparameter (typically $\\epsilon = 0.1 \\sim 0.3$).\n",
    "  * This **clip operation prevents the policy from changing too much in a single update step**, which protects against large, destabilizing policy updates that could hurt performance.\n",
    "    * If $r_t(\\theta) > 1 + \\epsilon$: the update would excessively increase the probability of a \"good\" action ‚Üí clip it down.\n",
    "    * If $r_t(\\theta) < 1 - \\epsilon$: the update would excessively decrease the probability of a \"bad\" action ‚Üí clip it up.\n",
    "  * Unlike TRPO, which enforces a hard KL-divergence constraint using second-order optimization, PPO uses this **first-order, simple-to-implement clip mechanism** to achieve similar \"proximal\" updates while remaining computationally efficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Tips for Practice** Tips for practical implementation (consid)\n",
    "\n",
    "Although PPO is **fairly robust** to the value of hyperparameters, **implementation details still matters a lot**. In fact, the paper *‚ÄúImplementation Matters in Deep RL: A Case Study on PPO and TRPO‚Äù* (Engstrom et al., 2020) identifies **9 critical tricks** that have a **significant impact on PPO's performance**. These implementation techniques ‚Äî such as advantage normalization, observation normalization, proper reward scaling, and value clipping ‚Äî can be considered when building PPO from scratch or tuning an existing implementation, in order to (at least partially) achieve more stable training and better empirical performance.\n",
    "\n",
    "* Engstrom et al. (2020): [Implementation Matters in Deep RL: A Case Study on PPO and TRPO](https://arxiv.org/abs/2005.12729)\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Hyperparameters in PPO**\n",
    "\n",
    "PPO involves several hyperparameters that significantly affect training performance, stability, and sample efficiency. Choosing appropriate values for these parameters is crucial for successful learning. Below, we list some commonly used hyperparameters in PPO, along with their **typical value ranges** and brief notes on **how they impact the algorithm‚Äôs behavior and performance**.\n",
    "\n",
    "\n",
    "| Hyperparameter                        | Description                        | Typical Value(s) | Notes on Performance Impact                         |\n",
    "| ------------------------------------- | ---------------------------------- | ---------------- | --------------------------------------------------- |\n",
    "| `learning_rate_p` / `learning_rate_v` | Learning rate $\\alpha$ for actor and critic | `1e-4 ~ 1e-3`  | Too high ‚Üí divergence; too low ‚Üí slow learning      |\n",
    "| `gae_lambda`                          | GAE smoothing parameter    $\\lambda$        | `0.95 ~ 0.98`           | Closer to 1 ‚Üí less bias, more variance              |\n",
    "| `clip_coef`                           | Clipping range $\\epsilon$          | `0.1 ~ 0.3`      | Smaller ‚Üí more conservative updates                 |\n",
    "| `ent_coef`                            | Entropy bonus weight (exploration) | `0.01 ~ 0.05`    | Higher ‚Üí more exploration, slower convergence       |\n",
    "| `vf_coef`                             | Value function loss weight         | `0.5 ~ 1.0`            | Balance actor/critic loss contributions             |\n",
    "| `max_grad_norm`                       | Gradient clipping norm             | `0.5 ~ 1.0`      | Helps stabilize training                            |\n",
    "| `n_steps`                             | Rollout length                     | `128 ~ 2048`     | Longer ‚Üí better estimates, but more memory          |\n",
    "| `batch_size`                          | Training batch size                | `32 ~ 256`       | Affects convergence speed & stability               |\n",
    "| `n_epochs`                            | Training epochs per rollout        | `4 ~ 10`         | More epochs ‚Üí better fitting, risk of overfitting   |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Based on the hyperparameter configuration shown in the block below, we instantiate the PPO controller and demonstrate both the training process and the controller‚Äôs performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0444de",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "learning_rate_p = 1e-3    # learning rate for policy\n",
    "learning_rate_v = 3e-4    # learning rate for value function\n",
    "gamma = 0.99              # discount factor\n",
    "gae_lambda = 0.95         # lambda for GAE\n",
    "clip_coef = 0.1           # clipping coefficient for PPO\n",
    "ent_coef = 0.01           # entropy coefficient\n",
    "vf_coef = 0.5             # value function coefficient\n",
    "max_grad_norm = 1.5       # gradient clipping\n",
    "n_steps = 256             # size of collected data in each rollout\n",
    "batch_size = 32           # size of mini-batch\n",
    "n_epochs = 10             # number of epochs to update the policy\n",
    "max_iterations = 1300     # maximum number of iterations for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e187525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class 'Env'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_c(env=env, dynamics=dynamics, dt=1/freq)\n",
    "\n",
    "# Instantiate the PPO controller class\n",
    "controller_ppo = PPOController(mdp, freq, max_iterations=max_iterations, seed=seed,\n",
    "                               learning_rate_p=learning_rate_p, learning_rate_v=learning_rate_v,\n",
    "                               gamma=gamma, gae_lambda=gae_lambda, clip_coef=clip_coef,\n",
    "                               ent_coef=ent_coef, vf_coef=vf_coef, max_grad_norm=max_grad_norm,\n",
    "                               n_steps=n_steps, batch_size=batch_size, n_epochs=n_epochs,\n",
    "                               name=\"PPO\")\n",
    "controller_ppo.load(\"checkpoints/ppo.pt\")  # annotate it by the first run, enable to load the stored model\n",
    "#controller_ppo.setup()  # annotate it when using the stored model\n",
    "#controller_ppo.save(\"checkpoints/ppo.pt\")  # annotate it when using the stored model\n",
    "controller_ppo.plot_training_curve()\n",
    "controller_ppo.plot_policy_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21893cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_ppo = Simulator(dynamics, controller_ppo, env, 1/freq, t_terminal)\n",
    "simulator_ppo.run_simulation()\n",
    "\n",
    "# Also setup a NMPC controller for reference\n",
    "env_mpc = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs_mpc, state_ubs=state_ubs_mpc, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_mpc = Dynamics(env_mpc)\n",
    "controller_mpc = MPCController(env_mpc, dynamics_mpc, Q, R, Qf, freq_mpc, N, name=\"NMPC\")\n",
    "simulator_mpc = Simulator(dynamics_mpc, controller_mpc, env_mpc, 1/freq_mpc, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ppo = Visualizer(simulator_ppo)\n",
    "visualizer_ppo.display_contrast_plots(\"Simulation of PPO Controller on Hilly Terrain\", simulator_mpc, if_gray=True)\n",
    "visualizer_ppo.display_contrast_animation_same(simulator_mpc, if_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d5849",
   "metadata": {},
   "source": [
    "#### **Result Analysis:**\n",
    "\n",
    "The above plots show a simulation comparison between a **PPO-trained controller** and a traditional **nonlinear model predictive controller (NMPC)** on a hilly terrain scenario. We observe that **PPO achieves performance comparable to the model-based NMPC**, both in terms of position tracking and constraint satisfaction. Despite minor differences in velocity overshoot and control effort, **PPO successfully reaches the target with stable behavior**.\n",
    "\n",
    "Notably, the small performance discrepancy stems from **different cost function designs**:\n",
    "\n",
    "* The **NMPC cost** is explicitly crafted to **balance stabilization and control effort minimization**, resulting in smoother and more energy-efficient control inputs.\n",
    "* In contrast, the **DRL cost function (used to train PPO)** is often designed to **encourage time-optimality**, pushing the agent to reach the goal as quickly as possible.\n",
    "\n",
    "As a result, the PPO controller adopts a **bang-bang-like control policy**, applying **larger input magnitudes** to accelerate faster, which can be seen in the third subplot. While this leads to higher input usage, it aligns with the learned time-optimal objective and demonstrates the flexibility of PPO in learning aggressive, goal-driven behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e130f",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: hyperparameters in the training step**\n",
    "\n",
    "To better understand how hyperparameters affect training dynamics, try modifying the values of the parameters listed above and observe their impact on the PPO controller's learning behavior. Some hyperparameters that have the **strongest influence** on performance include:\n",
    "\n",
    "* **`learning_rate_p / learning_rate_v`** ‚Äî affects convergence speed and stability; too high may cause divergence.\n",
    "* **`clip_coef`** ‚Äî controls how aggressively the policy is allowed to change; smaller values lead to more conservative updates.\n",
    "* **`ent_coef`** ‚Äî governs the balance between exploration and exploitation; higher values promote exploration.\n",
    "* **`n_steps`** ‚Äî determines how many environment steps are collected per update; longer rollouts can improve value estimates but increase memory use.\n",
    "\n",
    "We encourage you to **experiment** with these values and compare:\n",
    "\n",
    "* How fast the agent learns,\n",
    "* How stable the training curves are,\n",
    "* Whether the final policy performs well across multiple runs.\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c668d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Wrap-up: PPO and What‚Äôs Next**\n",
    "\n",
    "In this chapter, we introduced the **core ideas behind Proximal Policy Optimization (PPO)** ‚Äî one of the most widely used deep reinforcement learning algorithms today. While time constraints prevented us from delving into the full theoretical background or complete implementation, we focused on **how PPO works**, and showcased its **impressive performance**, especially in scenarios involving **continuous state and action spaces**.\n",
    "\n",
    "PPO represents just one example of learning-based control. If you are curious to **explore the underlying theory more deeply**, understand **advanced implementation techniques**, or gain **hands-on experience applying learning algorithms to real-world systems**, we highly encourage you to consider the following courses:\n",
    "\n",
    "* **Advanced Robot Learning and Decision-Making** (SoSe/WiSe): Deepening understanding through hands-on implementation and diving into state-of-the-art methods.\n",
    "* **Autonomous Drone Racing Project Course** (SoSe/WiSe): Apply these algorithms in high-speed, real-time robotics settings.\n",
    "* **Seminar on Semantics for Robot Perception and Decision-Making** (SoSe/WiSe): Building foundational research skills in preparation for thesis and beyond.\n",
    "\n",
    "These courses offer not just theoretical depth, but also the **opportunity to design and implement controllers and modern learning algorithms in practice**. We believe you‚Äôll gain valuable insights and skills ‚Äî and have a lot of fun along the way.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
