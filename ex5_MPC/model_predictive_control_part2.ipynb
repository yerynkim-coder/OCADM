{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d842ea56",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 5.2: MPC for tracking task and Robuct MPC**\n",
    "\n",
    "\n",
    "\n",
    "In this chapter, we explore and analyse practical implementations of Model Predictive Control (MPC) for reference tracking, and its extension to Robust MPC (RMPC) under uncertainty.\n",
    "\n",
    "In the first part, we implement a basic tracking MPC, and then simulate and visualize its performance. Then we compare the tracking and stabilization MPC to highlight their structural differences and draw some key insights for hyperparemeter tuning.\n",
    "\n",
    "In the second part, we extend the framework to Robust MPC: we first show how to implement a RMPC class, then visualize how we compute the robust invariant set in the phase portrait, and finally run full simulations to evaluate RMPC performance under uncertainty.\n",
    "\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 5.2 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 1 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">MPC for Reference Tracking</td>\n",
    "    <td>Example 1: Tracking MPC Implementation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 2: Simulation and Visualization</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 3: MPC for Stabilization vs. MPC for Tracking</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">Robust MPC (RMPC)</td>\n",
    "    <td>Example 1: RMPC Implementation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 2: Robust Invariant Set and Phase Portrait for RMPC</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 3: Simulation and Visualization</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "First, we need to set up our Python environment and specify some scenarios as a testbed for the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *\n",
    "\n",
    "import numpy as np\n",
    "import casadi as ca\n",
    "\n",
    "import pytope as pt\n",
    "from scipy.spatial import ConvexHull, QhullError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2 or 3 or 4\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Define the constraints for input (potentially used)\n",
    "input_lbs = -1.0\n",
    "input_ubs = 1.0\n",
    "\n",
    "# Case 1: unconstrained case\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "dynamics = Dynamics(env)\n",
    "\n",
    "# Case 2: constrained case\n",
    "env_constr = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_constr = Dynamics(env_constr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79814da",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part (a): MPC for Reference Tracking**\n",
    "\n",
    "So far we have limited our discussion on MPC to the case of stabilization. However, in many applications, we want our system to track a given trajectory instead of stabilizing the system at the origin. In the following, we will briefly look at the case of tracking a reference trajectory using MPC. \n",
    "\n",
    "Let the trajectory be defined as $\\boldsymbol{r}_k \\in \\mathbb{R}^s, \\forall k \\in \\{ 0, \\dots, T + N \\}$ with $s \\leq n$ and $T > 0$ and let the output of the system be given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y_k} = \\boldsymbol{C} \\boldsymbol{x_k} \\in \\mathbb{R}^s \\,,\n",
    "$$\n",
    "\n",
    "where $C \\in \\mathbb{R}^{s \\times n}$ maps the full state vector $\\boldsymbol{x_k}$ to the output $\\boldsymbol{y_k}$, which only consists of the elements of the state vector that we want to track. \n",
    "Then the open-loop optimization for the tracking MPC with quadratic tracking objective is:\n",
    "\n",
    "$$\n",
    "J_k^*(\\boldsymbol{x_k}) = \\min_{u_{k|k}, \\ldots, u_{k+N-1|k}} \\sum_{i=0}^{N-1} \\boxed{ \\left( \\boldsymbol{y_{k+i|k}} - \\boldsymbol{r_{k+i|k}} \\right)^T \\boldsymbol{Q} \\left( \\boldsymbol{y_{k+i|k}} - \\boldsymbol{r_{k+i|k}} \\right) } + u_{k+i|k} ^T \\boldsymbol{R} u_{k+i|k}\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+i+1|k}} = \\boldsymbol{f} \\left( \\boldsymbol{x_{k+i|k}}, \\boldsymbol{u_{k+i|k}} \\right), \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+i|k}} \\in \\mathcal{X}, \\quad \\forall i \\in \\{0, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_{k+i|k} \\in \\mathcal{U}, \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+N|k}} \\in \\mathcal{X}_f, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\boldsymbol{y_{k+i|k}} = \\boldsymbol{C} \\boldsymbol{x_{k+i|k}}, \\quad \\forall i \\in \\{0, \\ldots, N\\},}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k|k}} = \\boldsymbol{\\overline{x}_{k}}\n",
    "$$\n",
    "\n",
    "where $Q$ and $R$ are positive semidefinite and positive definite weighting matrices, respectively. Using an MPC has the advantage that we can apply control inputs to the system that anticipate changes in the reference. Furthermore, MPC allows us to enforce state and input constraints during the tracking task. Using a similar objective function we can also track a control input trajectory while satisfying state and input constraints. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84460",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 1: Tracking MPC Implementation**\n",
    "\n",
    "Based on the tracking MPC formulation introduced earlier, we now present the corresponding controller implementation. Compared to the MPC for stabilization tasks, the tracking MPC shares the same structural formulation of the optimization problem. As a result, the `setup` function does not require any modification. In the following, we provide only the updated control loop to reflect the specific problem setting of tracking MPC. In each cycle, you need to:\n",
    "\n",
    "1\\) Assign values for the initial state (`lbx` and `ubx` for $i = 0$) via the interface `set()`;  \n",
    "\n",
    "2\\) Assign values for the tracking reference (`yref` for $i \\in \\{ 1, ... , N \\}$) via the interface `set()`;  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*Note that: the reference state here is a time-varying array, you need to check the value from variable `self.traj_ref`.*\n",
    "\n",
    "3\\) Solve OCP problem by calling `solve()`;  \n",
    "\n",
    "4\\) Get the first input command through `get()`;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594773b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_action_external(self, current_state: np.ndarray, current_time) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve the MPC problem and compute the optimal control action.\n",
    "\n",
    "    Args:\n",
    "    - current_state: The current state of the system.\n",
    "    - current_time: The current time (not used in this time-invariant case).\n",
    "\n",
    "    Returns:\n",
    "    - Optimal control action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Update initial state in the solver\n",
    "    self.solver.set(0, \"lbx\", current_state)\n",
    "    self.solver.set(0, \"ubx\", current_state)\n",
    "\n",
    "    # Update reference trajectory for all prediction steps\n",
    "    input_ref = np.zeros(self.dim_inputs)\n",
    "    ref_length = self.traj_ref.shape[0]\n",
    "\n",
    "    for i in range(self.N):\n",
    "        index = min(current_time + i, ref_length - 1)\n",
    "        state_ref = self.traj_ref[index, :self.dim_states]\n",
    "        self.solver.set(i, \"yref\", np.concatenate((state_ref, input_ref)))\n",
    "    index = min(current_time + self.N, ref_length - 1)\n",
    "    terminal_state_ref = self.traj_ref[index, :self.dim_states]\n",
    "    self.solver.set(self.N, \"yref\", terminal_state_ref)\n",
    "\n",
    "    # Solve the MPC problem\n",
    "    status = self.solver.solve()\n",
    "    #if status != 0:\n",
    "    #    raise ValueError(f\"Acados solver failed with status {status}\")\n",
    "\n",
    "    # Extract the first control action\n",
    "    u_optimal = self.solver.get(0, \"u\")\n",
    "\n",
    "    # Extract the predictions\n",
    "    x_pred = np.zeros((self.N + 1, self.dim_states))\n",
    "    u_pred = np.zeros((self.N, self.dim_inputs))\n",
    "    for i in range(self.N + 1):\n",
    "        x_pred[i, :] = self.solver.get(i, \"x\")\n",
    "        if i < self.N:\n",
    "            u_pred[i, :] = self.solver.get(i, \"u\")\n",
    "\n",
    "    if self.verbose:\n",
    "        print(f\"Optimal control action: {u_optimal}\")\n",
    "        print(f\"x_pred: {x_pred}\")\n",
    "        print(f\"u_pred: {u_pred}\")\n",
    "\n",
    "    return u_optimal, x_pred, u_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d7635",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 2: Simulation and Visualization**  \n",
    "\n",
    "1\\) Subtitute the function `compute_action` in predefined class `TrackingMPCController` with our external control loop `compute_action_external`;\n",
    "\n",
    "2\\) Specify the arguments as follows:  \n",
    "\n",
    "- Parameters:  \n",
    "\n",
    "    i) weight for state $\\bm{Q} = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\bm{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "    iv) horizon $N = 20$\n",
    "\n",
    "3\\) Instantiate the controller `LQRController` and run the simulation to get generate a reference trajectory `state_traj_lqr`;\n",
    "\n",
    "4\\) Use the specified hyperparameetrs and the reference trajectory from LQR to instantiate the controller class `TrackingMPCController`;\n",
    "\n",
    "5\\) Instantiate the class `Simulator` and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "6\\) Instantiate the class `Visualizer`, call function `display_final_results()` and `display_animation()` to show the simulations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackingMPCController.compute_action = compute_action_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47846a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "N = 20\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 8 # time length of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LQR controller\n",
    "controller_lqr = LQRController(env_constr, dynamics_constr, Q, R, freq)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_lqr = Simulator(dynamics_constr, controller_lqr, env_constr, 1/freq, t_terminal)\n",
    "simulator_lqr.run_simulation()\n",
    "state_traj_lqr, input_traj_lqr = simulator_lqr.get_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13be287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MPC controller\n",
    "controller_mpc = TrackingMPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N, traj_ref=state_traj_lqr, name='tracking_MPC', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_plots(\"Example of Tracking MPC with LQR Reference Trajectory\")\n",
    "visualizer_mpc.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dee27f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 3: MPC for Stabilization vs. MPC for Tracking**\n",
    "\n",
    "After implementing the tracking MPC controller, we now return to one of the questions raised at the very beginning: Which hyperparameters affect MPC performance, and in what way? In Part 1, we have already discussed this question in the context of MPC for stabilization. The most influential hyperparameters include:\n",
    "\n",
    "- The relation between state cost $\\boldsymbol{Q}$ and input cost $\\boldsymbol{R}$: A larger $\\boldsymbol{Q}/\\boldsymbol{R}$ ratio typically leads to more aggressive tracking behavior, prioritizing state accuracy over input efficiency. Conversely, higher input penalties result in smoother but slower responses.\n",
    "\n",
    "- The horizon length $N$: Increasing $N$ allows the controller to better foresee long-term consequences, often improving performance at the cost of computational burden. A short horizon can lead to shortsighted behavior and suboptimal transients.\n",
    "\n",
    "- Whether we introduce the special terminal cost design $g_N(\\boldsymbol{x}_{k+N|k})$: An appropriately designed terminal cost improves closed-loop behavior and convergence properties, especially in stabilization problems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d288d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MPC controller (reference)\n",
    "controller_mpc_ref = LinearMPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N, name='MPC_N=20_Q=1(baseline)', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_ref = Simulator(dynamics_constr, controller_mpc_ref, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc_ref.run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1: larger Q value\n",
    "Q_larger = np.diag([100, 100])\n",
    "Qf = Q\n",
    "\n",
    "# Define the MPC controller\n",
    "controller_mpc = LinearMPCController(env_constr, dynamics_constr, Q_larger, R, Qf, freq, N, name='MPC_N=20_Q=100', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_contrast_plots(\"Impact of tuning matrix Q (stabilization task)\", simulator_mpc_ref)\n",
    "visualizer_mpc.display_contrast_animation_same(simulator_mpc_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2: larger N \n",
    "N_test = 60\n",
    "\n",
    "# Define the MPC controller\n",
    "controller_mpc = LinearMPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N_test, name='MPC_N=60_Q=1', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_contrast_plots(\"Impact of raising prediction horizon N (stabilization task)\", simulator_mpc_ref)\n",
    "visualizer_mpc.display_contrast_animation_same(simulator_mpc_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 3: use DARE to compute the infinite LQR weight matrix as terminal cost in MPC\n",
    "\n",
    "# Linearize dynamics at equilibrium and solve DARE to compute the infinite LQR weight matrix\n",
    "A_d, B_d = dynamics.get_linearized_AB_discrete(np.array([0.0, 0.0]), 0.0, 1/freq)\n",
    "P = scipy.linalg.solve_discrete_are(A_d, B_d, Q, R)\n",
    "\n",
    "# Define the MPC controller\n",
    "controller_mpc = LinearMPCController(env_constr, dynamics_constr, Q, R, P, freq, N, name='MPC_with_terminal_cost', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_contrast_plots(\"Impact of introducing infinite LQR cost as terminal cost (stabilization task)\", simulator_mpc_ref)\n",
    "visualizer_mpc.display_contrast_animation_same(simulator_mpc_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6b7d4",
   "metadata": {},
   "source": [
    "From the simulation results of Example 2 and Example 3, we observe that the resulting trajectories are quite similar. Is there a deeper connection between improving MPC performance by extending the prediction horizon $N$ and by introducing a terminal cost $g_N(\\boldsymbol{x}_{k+N|k})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d89cc",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üí° Take-away 1: Key Insight on the Role of Terminal Cost $g_N(\\boldsymbol{x}_{k+N|k})$ in MPC**\n",
    "\n",
    "**Adding a terminal cost can be understood as a structured way of extending the prediction horizon without increasing the number of optimization variables**. While increasing the horizon allows the controller to explicitly consider longer-term consequences, introducing a terminal cost implicitly achieves a similar effect by assigning a cost-to-go approximation to the terminal state. In other words, $g_N(\\boldsymbol{x}_{k+N|k})$ serves as a surrogate for the cost that would be accumulated beyond the finite horizon.  \n",
    "\n",
    "Thus, terminal cost acts as a low-computation alternative to horizon extension, offering similar benefits in convergence speed and policy aggressiveness‚Äîprovided that the terminal cost is well designed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f56fe0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The previous section demonstrated how modifying three key hyperparameters affected controller performance in the stabilization task. As we can observe, compared to tuning the weights $\\boldsymbol{Q}$ and $\\boldsymbol{R}$, increasing the prediction horizon or adding terminal components tends to have a more pronounced impact on the overall system performance. More specifically, extending the horizon improves long-term behavior but comes at the cost of increased computational complexity. In contrast, adding terminal components typically incurs negligible additional computation time, but it requires a reasonable estimate of the terminal cost function.\n",
    "The previous section demonstrated how modifying three key hyperparameters affected controller performance in the stabilization task. As we can observe, compared to tuning the weights $\\boldsymbol{Q}$ and $\\boldsymbol{R}$, increasing the prediction horizon or adding terminal components tends to have a more pronounced impact on the overall system performance. More specifically, extending the horizon improves long-term behavior but comes at the cost of increased computational complexity. In contrast, adding terminal components typically incurs negligible additional computation time, but it requires a reasonable estimate of the terminal cost function.\n",
    "\n",
    "We conclude these properties as the tuning principle follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e3768",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üí° Take-away 2: Principles for Tuning MPC in Stabilization Tasks**\n",
    "\n",
    "If you aim to improve the performance of your MPC controller in a stabilization task, consider the following principles:\n",
    "\n",
    "- **If you can afford increased computational load**, extending the prediction horizon $N$ is often the most effective way to enhance long-term planning and convergence behavior.\n",
    "\n",
    "- **OR if you have a reasonable approximation of the cost-to-go or insight into the terminal state distribution**, adding a well-designed terminal cost can offer similar benefits to increasing the horizon, often with much lower computational cost.\n",
    "\n",
    "- **If neither of the above is feasible**, tuning the cost weights $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ can still improve controller behavior, though the resulting performance gains are typically smaller and more nuanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d83839",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### **MPC for Reference Tracking**\n",
    "\n",
    "The previous section demonstrated how modifying three key hyperparameters affected controller performance in the stabilization task. As we can observe, compared to tuning the weights $\\boldsymbol{Q}$ and $\\boldsymbol{R}$, increasing the prediction horizon or adding terminal components tends to have a more pronounced impact on the overall system performance. More specifically, extending the horizon improves long-term behavior but comes at the cost of increased computational complexity. In contrast, adding terminal components typically incurs negligible additional computation time, but it requires a reasonable estimate of the terminal cost function.\n",
    "However, in the context of **tracking task**, the influence of these hyperparameters differs, which will be illustrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2048ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LQR controller\n",
    "controller_lqr = LQRController(env_constr, dynamics_constr, Q, R, freq)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_lqr = Simulator(dynamics_constr, controller_lqr, env_constr, 1/freq, t_terminal)\n",
    "simulator_lqr.run_simulation()\n",
    "state_traj_lqr, input_traj_lqr = simulator_lqr.get_trajectories()\n",
    "\n",
    "\n",
    "\n",
    "# Define the MPC controller\n",
    "controller_mpc_ref = TrackingMPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N, traj_ref=state_traj_lqr, name='tracking_MPC_baseline', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_ref = Simulator(dynamics_constr, controller_mpc_ref, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc_ref.run_simulation()\n",
    "\n",
    "\n",
    "\n",
    "# Linearize dynamics at equilibrium and solve DARE to compute the infinite LQR weight matrix\n",
    "A_d, B_d = dynamics.get_linearized_AB_discrete(np.array([0.0, 0.0]), 0.0, 1/freq)\n",
    "P = scipy.linalg.solve_discrete_are(A_d, B_d, Q, R)\n",
    "\n",
    "# Define the MPC controller\n",
    "controller_mpc_terminal = TrackingMPCController(env_constr, dynamics_constr, Q, R, P, freq, N, traj_ref=state_traj_lqr, name='tracking_MPC_with_terminal_cost', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_terminal = Simulator(dynamics_constr, controller_mpc_terminal, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc_terminal.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_terminal = Visualizer(simulator_mpc_terminal)\n",
    "visualizer_mpc_terminal.display_contrast_plots(\"Impact of introducing infinite LQR cost as terminal cost (tracking task)\", simulator_mpc_ref)\n",
    "visualizer_mpc_terminal.display_contrast_animation_same(simulator_mpc_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242e380",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Try to tune $\\boldsymbol{Q}$ or raise the horizon $N$**\n",
    "\n",
    "Here we only show the impact of introducing terminal cost, you may also want to try the impact of varying the value of $\\boldsymbol{Q}$ or horizon $N$, what will happen then?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e5e23",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "From the simulation results, we observe that increasing the prediction horizon $N$ or adding a terminal cost has only a marginal effect on the performance of tracking MPC.\n",
    "\n",
    "This is because in the tracking MPC cost function, the state tracking error is relatively evenly distributed across the prediction horizon. This implies that whether or not a larger horizon is used to capture more future states, the optimizer is already receiving consistent guidance from the reference trajectory throughout the horizon. As a result, the controller does not benefit significantly from longer lookahead or terminal extrapolation, unlike in stabilization tasks where future behavior must be inferred more structurally through horizon length and terminal shaping.\n",
    "\n",
    "**In other words, if the reference trajectory is feasible, it already embeds information about the system's underlying dynamics‚Äîspecifically, in the way the state evolves over time.** In such cases, the controller does not need to rely entirely on a long prediction horizon to infer the desired system behavior. **The trajectory itself provides a sufficient guidence that uniquely defining not only the steady-state, but also the transient-state,** thereby reducing the marginal benefit of increasing the horizon length purely for predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5e203",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üí° Take-away 3: Tuning MPC in Tracking Tasks**\n",
    "\n",
    "**When tuning a tracking MPC controller for a feasible reference trajectory, the impact of hyperparameter choices tends to be significantly smaller than in stabilization tasks.** This is because the reference trajectory plays a dominant role in guiding the state evolution during optimization, thereby diminishing the influence of cost weights, horizon length, and terminal cost design.\n",
    "\n",
    "**Although this method demands (Tracking MPC) some offline computation for computing the reference trajectory, its major advantage lies in the fact that tuning is considerably simpler and less sensitive to task-specific configurations, making it the recommended choice in many applications.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0aa93e",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part (b): Robust MPC**\n",
    "\n",
    "So far, we have considered MPC formulations under the assumption that the system dynamics are perfectly known and that there are no disturbances affecting the system. However, in many real-world applications, such assumptions rarely hold: model uncertainties, external disturbances, and unmodeled dynamics often degrade controller performance or even cause constraint violations. To address these challenges, we now turn to Robust Model Predictive Control (Robust MPC) ‚Äî a class of methods designed to ensure constraint satisfaction and stability even in the presence of bounded disturbances or model-mismatches.\n",
    "\n",
    "Robust MPC encompasses a variety of formulations, each differing in how they handle uncertainty and guarantee robustness. These include min-max MPC, scenario-based MPC, and tube-based MPC, among others. In this section, we focus on the tube-based approach, which is conceptually simple yet powerful enough to handle bounded additive disturbances. It works by planning a nominal trajectory and ensuring that the true system state remains within a bounded tube around it, using a combination of feedback and constraint tightening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514f66e",
   "metadata": {},
   "source": [
    "##### **Derivation of Error Dynamics:**\n",
    "\n",
    "Given a linear time-invariant system with bounded additive disturbances:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{A}\\boldsymbol{x}_k + \\boldsymbol{B}\\boldsymbol{u}_k + \\boldsymbol{w}_k,\n",
    "$$\n",
    "\n",
    "with $\\boldsymbol{w}_k \\in \\mathcal{D}$, where $\\mathcal{D}$ is compact and contains the origin.  We can construct an associated linear nominal model accordingly:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\overline{x}}_{k+1} = \\boldsymbol{A}\\boldsymbol{\\overline{x}}_k + \\boldsymbol{B}\\boldsymbol{\\overline{u}}_k,\n",
    "$$\n",
    "\n",
    "Then the error between the true system state and the nominal system state is $\\boldsymbol{e}_k = \\boldsymbol{x}_k - \\boldsymbol{\\overline{x}}_k$. Using the pre-stabilization controller $u_k = \\overline{u}_k + \\boldsymbol{K} ( \\boldsymbol{x}_k - \\boldsymbol{\\overline{x}}_k )$, the error dynamics can be derived as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{e}_{k+1} = \\underbrace{(\\boldsymbol{A} + \\boldsymbol{B} \\boldsymbol{K})}_{\\boldsymbol{A}_{\\text{cl}}} \\boldsymbol{e}_{k} + \\boldsymbol{w}_{k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f6599",
   "metadata": {},
   "source": [
    "##### **Bounds for the Error: Robust Invariant Set $\\Omega_{\\text{tube}}$**\n",
    "\n",
    "While $\\boldsymbol{A}_{\\text{cl}}$ being Schur stable (eigenvalues have norm less than $1$), this yields bounds on all future errors and will converge to an error set that we denote $\\Omega_{\\text{tube}}$. The error set $\\Omega_{\\text{tube}}$ can be naively (there exist more efficient computations) determined using recursion. With $\\boldsymbol{e}_0 = \\boldsymbol{0}$ this yields $\\boldsymbol{e}_1 = \\boldsymbol{w}_0 \\in \\mathcal{D}$. Therefore, the error set at the first time step is $\\Omega_1 = \\mathcal{D}$. Then the error set at the next time step can be computed by applying the error dynamics $\\boldsymbol{A}_{\\text{cl}}$ to every element of $\\Omega_1$ and then expanding the resulting set of errors with every possible disturbance $\\boldsymbol{w}_1 \\in \\mathcal{D}$. We continue this process until convergence. The recursion is given by the following Algorithm, where $\\oplus$ denotes the Minkowski addition.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm: Computing the tube } \\Omega_{\\text{tube}} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{Initialization: } A_{\\text{cl}},\\ \\mathcal{D} \\\\\n",
    "\\text{01: } \\quad k \\gets 1 \\\\\n",
    "\\text{02: } \\quad \\Omega_1 \\gets \\mathcal{D} \\\\\n",
    "\\text{03: } \\quad \\textbf{while True do} \\\\\n",
    "\\text{04: } \\quad | \\quad \\Omega_{k+1} \\gets A_{\\text{cl}} \\Omega_k \\oplus \\mathcal{D} \\\\\n",
    "\\text{05: } \\quad | \\quad \\textbf{if } \\Omega_{k+1} = \\Omega_k \\textbf{ then} \\\\\n",
    "\\text{06: } \\quad | \\quad | \\quad \\Omega_{\\text{tube}} \\gets \\Omega_k \\\\\n",
    "\\text{07: } \\quad | \\quad | \\quad \\textbf{Stop} \\\\\n",
    "\\text{08: } \\quad | \\quad \\textbf{end if} \\\\\n",
    "\\text{09: } \\quad | \\quad k \\gets k + 1 \\\\\n",
    "\\text{10: } \\quad \\textbf{end while}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0be4b2",
   "metadata": {},
   "source": [
    "##### **Formulation of RMPC Problem:**\n",
    "\n",
    "For the nominal model, tube-based MPC solves the following constrained optimization problem at every time step k to obtain the optimal sequence $\\{\\overline{u}_{k|k}, \\ldots, \\overline{u}_{k+N-1|k}\\}$:\n",
    "\n",
    "$$\n",
    "J_k^*(\\boldsymbol{\\overline{x}_k}) = \\min_{\\overline{u}_{k|k}, \\ldots, \\overline{u}_{k+N-1|k}} \n",
    "g_N\\left( \\boldsymbol{\\overline{x}_{k+N|k}} \\right) + \\sum_{i=0}^{N-1} g_i\\left( \\boldsymbol{\\overline{x}_{k+i|k}}, \\boldsymbol{\\overline{u}_{k+i|k}} \\right)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\overline{x}_{k+i+1|k}} = \\boldsymbol{A} \\boldsymbol{\\overline{x}_{k+i|k}} + \\boldsymbol{B} \\boldsymbol{\\overline{u}_{k+i|k}}, \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\overline{x}_{k+i|k}} \\in \\mathcal{X} \\ominus \\mathcal{\\Omega}_{\\text{tube}}, \\quad \\forall i \\in \\{0, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overline{u}_{k+i|k} \\in \\mathcal{U} \\ominus \\boldsymbol{K} \\mathcal{\\Omega}_{\\text{tube}}, \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\overline{x}_{k+N|k}} \\in \\mathcal{X}_{f,\\text{robust}}, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_k} \\in \\boldsymbol{\\overline{x}_{k|k}} \\oplus \\mathcal{\\Omega}_{\\text{tube}},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\overline{x}_{k+i|k}}$ is the open-loop nominal state at time step $k + i$, and $\\mathcal{X} \\ominus \\mathcal{\\Omega}_{tube} = \\{\\boldsymbol{x} \\in R^n: \\boldsymbol{x} + \\boldsymbol{w} \\in \\mathcal{X}, \\boldsymbol{w} \\in \\mathcal{\\Omega}_{\\text{tube}} \\}$ and $\\mathcal{U} \\ominus \\boldsymbol{K} \\mathcal{\\Omega}_{\\text{tube}} = \\{ u \\in R^m : u + \\boldsymbol{K} \\boldsymbol{w} \\in \\mathcal{U}, \\boldsymbol{w} ‚àà \\mathcal{\\Omega}_{\\text{tube}} \\}$ are the state and input constraints tightened using the bounded tube ‚Ñ¶tube with $\\ominus$ denoting the Pontryagin difference. \n",
    "\n",
    "\n",
    "In each step of RMPC, we solve the above optimization problem to obtain the nominal state and input trajectories. Then, by combining these nominal trajectories with a pre-calculated stabilizing feedback control gain, the overall RMPC policy can be expressed as:\n",
    "\n",
    "$$\n",
    "u_k = \\overline{u}_k + \\boldsymbol{K} ( \\boldsymbol{x}_k - \\boldsymbol{\\overline{x}}_k )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b27df",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 1: RMPC Implementation**  \n",
    "\n",
    "In this section, we introduce how to implement an RMPC controller class. As before, we use `Acados` as the underlying solver. However, prior to calling the solver, we need to recursively compute the Robust Invariant Set starting from the disturbance set $\\mathcal{D}$. For this step, we use the `pytopes` package, which provides basic polytope operations such as **affine mapping** and **Minkowski sums**. The implementation will follow three key steps:\n",
    "\n",
    "1\\) Complete the functions `affine_map` and `compute_invariant_tube` to compute the Robust Invariant Set $\\Omega_{\\text{tube}}$;  \n",
    "    Hint: you may use the **affine mapping** and **Minkowski sums** operation from package `pytopes`, please refer to [website](https://github.com/heirung/pytope)\n",
    "\n",
    "2\\) Finish the solver configuration function `setup`, which is almost the same as for the nominal MPC implementation, but additionally requires interfaces for polytopic constraints;  \n",
    "    Hint: here you can directly use the halfspace representation of the polytopes, which can be easily obtained by `pt.Polytope.A` and `pt.Polytope.b`\n",
    "\n",
    "3\\) Finish the control loop function `compute_action`, where two main differences exist:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; i) set the upper limit of the shifted polytope constraint by $\\boldsymbol{b}_{\\text{current}} = \\boldsymbol{A}_0 \\boldsymbol{x}_{\\text{current}} + \\boldsymbol{b}_0$;\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; ii) after computing the nominal state $\\boldsymbol{x}_k$ and input $\\overline{u}_k$ from the solver, apply the stabilizing feedback policy $u_k = \\overline{u}_k + \\boldsymbol{K} ( \\boldsymbol{x}_k - \\boldsymbol{\\overline{x}}_k )$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived class for Tube-based Robust MPC Controller\n",
    "class LinearRMPCController(LQRController):\n",
    "    def __init__(\n",
    "            self, \n",
    "            env: Env, \n",
    "            dynamics: Dynamics, \n",
    "            Q: np.ndarray, \n",
    "            R: np.ndarray, \n",
    "            Qf: np.ndarray, \n",
    "            freq: float, \n",
    "            N: int, \n",
    "            K_feedback: Optional[np.ndarray] = None,  # Feedback gain for tube\n",
    "            disturbance_bounds: Optional[Tuple[np.ndarray, np.ndarray]] = None,  # (lbz, ubz) of D\n",
    "            max_iter: int = 10,  # Max iterations for invariant set computation\n",
    "            name: str = 'RMPC', \n",
    "            type: str = 'RMPC', \n",
    "            verbose: bool = True\n",
    "        ) -> None:\n",
    "\n",
    "        self.Qf = Qf\n",
    "\n",
    "        self.N = N  # Prediction horizon\n",
    "\n",
    "        self.ocp = None\n",
    "        self.solver = None\n",
    "\n",
    "        super().__init__(env, dynamics, Q, R, freq, name, type, verbose)\n",
    "\n",
    "        x0 = np.zeros(self.dynamics.dim_states)\n",
    "        u0 = np.zeros(self.dynamics.dim_inputs)\n",
    "        self.A, self.B = self.dynamics.get_linearized_AB_discrete(x0, u0, self.dt)\n",
    "\n",
    "        # Automatically solve DARE if no K provided\n",
    "        if K_feedback is None:\n",
    "            from scipy.linalg import solve_discrete_are\n",
    "            P = solve_discrete_are(self.A, self.B, Q, R)\n",
    "            self.K_feedback = -np.linalg.inv(R + self.B.T @ P @ self.B) @ self.B.T @ P @ self.A\n",
    "        else:\n",
    "            self.K_feedback = K_feedback\n",
    "\n",
    "        # Compute Omega_tube from disturbance box D \n",
    "        if disturbance_bounds is not None:\n",
    "\n",
    "            disturbance_lbs = disturbance_bounds[0]\n",
    "            disturbance_ubs = disturbance_bounds[1]\n",
    "\n",
    "        elif self.env.disturbance_lbs is not None and self.env.disturbance_ubs is not None:\n",
    "\n",
    "            disturbance_lbs = self.env.disturbance_lbs\n",
    "            disturbance_ubs = self.env.disturbance_ubs\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No bounds of additive disturbances provided, can not initialize RMPC\")\n",
    "        \n",
    "        disturbance_lbs /= freq\n",
    "        disturbance_ubs /= freq\n",
    "        \n",
    "        self.Omega_tube = self.compute_invariant_tube(self.A + self.B @ self.K_feedback, disturbance_lbs, disturbance_ubs, max_iter=max_iter)\n",
    "\n",
    "        # Create polytope for tighten state constraints\n",
    "        dim = len(self.env.state_lbs)\n",
    "        H_box = np.vstack([np.eye(dim), -np.eye(dim)])\n",
    "        h_box = np.hstack([self.env.state_ubs, -self.env.state_lbs])\n",
    "        self.X = pt.Polytope(H_box, h_box)\n",
    "        self.X_tighten = self.X - self.Omega_tube\n",
    "\n",
    "        # Create polytope for tighten input constraints\n",
    "        u_lbs_tighten = self.env.input_lbs + np.max(self.affine_map(self.Omega_tube, self.K_feedback).V, axis=0)\n",
    "        u_ubs_tighten = self.env.input_ubs + np.min(self.affine_map(self.Omega_tube, self.K_feedback).V, axis=0)\n",
    "        self.U_tighten = np.array([u_lbs_tighten, u_ubs_tighten])\n",
    "\n",
    "        self.tube_bounds_x = self.estimate_bounds_from_polytope(self.Omega_tube)\n",
    "        self.tube_bounds_u = np.abs(self.K_feedback @ self.tube_bounds_x)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Tighten state set X-Œ©: {self.X_tighten.V}\")\n",
    "            print(f\"Tube size x: {self.tube_bounds_x}\")\n",
    "            print(f\"Tube size u: {self.tube_bounds_u}\")\n",
    "\n",
    "        self.setup()\n",
    "    \n",
    "    def affine_map(self, poly: pt.Polytope, A: np.ndarray) -> pt.Polytope:\n",
    "        \"\"\"Compute the affine image of a polytope under x ‚Ü¶ A x\"\"\"\n",
    "\n",
    "        assert poly.V is not None, \"No poly.V in Polytope! Can not apply affine map.\"\n",
    "\n",
    "        V = poly.V  # vertices\n",
    "        V_new = (A @ V.T).T\n",
    "\n",
    "        return pt.Polytope(V_new)\n",
    "        \n",
    "    def compute_invariant_tube(self, A_cl, lbz, ubz, tol=1e-4, max_iter=10) -> pt.Polytope:\n",
    "        \"\"\"\n",
    "        Compute the robust positive invariant set Omega_tube using Minkowski recursion.\n",
    "\n",
    "        This implementation uses the `polytope` library's Minkowski sum and affine map.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Define initial disturbance set D (box)\n",
    "        dim = len(lbz)\n",
    "\n",
    "        # H-representation: H x ‚â§ h\n",
    "        H_box = np.vstack([np.eye(dim), -np.eye(dim)])\n",
    "        h_box = np.hstack([ubz, -lbz])\n",
    "\n",
    "        # Create polytope with both H-rep and V-rep\n",
    "        D = pt.Polytope(H_box, h_box)\n",
    "\n",
    "        # Step 2: Initialize Omega := D\n",
    "        Omega = D\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            # Step 3: Apply affine map A_cl to Omega: A_cl * Omega\n",
    "            A_Omega = self.affine_map(Omega, A_cl)\n",
    "\n",
    "            # Step 4: Minkowski sum: Omega_next = A_Omega ‚äï D\n",
    "            Omega_next = A_Omega + D\n",
    "\n",
    "            # Step 5: Check convergence via bounding box approximation\n",
    "            bounds_old = self.estimate_bounds_from_polytope(Omega)\n",
    "            bounds_new = self.estimate_bounds_from_polytope(Omega_next)\n",
    "\n",
    "            if np.allclose(bounds_old, bounds_new, atol=tol):\n",
    "                return Omega_next  # Return as vertices\n",
    "\n",
    "            Omega = Omega_next\n",
    "\n",
    "        return Omega  # Max iteration reached, return current estimate\n",
    "\n",
    "    def estimate_bounds_from_polytope(self, poly: pt.Polytope):\n",
    "        \"\"\"Estimate box bounds from polytope vertices (axis-aligned).\"\"\"\n",
    "        vertices = poly.V\n",
    "        return np.max(np.abs(vertices), axis=0)\n",
    "\n",
    "    def setup(self) -> None:\n",
    "\n",
    "        ## Model\n",
    "        # Set up Acados model\n",
    "        model = AcadosModel()\n",
    "        model.name = self.name\n",
    "\n",
    "        # Define model: x_dot = f(x, u)\n",
    "        model.x = self.dynamics.states\n",
    "        model.u = self.dynamics.inputs\n",
    "        model.f_expl_expr = ca.vertcat(self.dynamics.dynamics_function(self.dynamics.states, self.dynamics.inputs))\n",
    "        model.f_impl_expr = None # no needed, we already have the explicit model\n",
    "\n",
    "        ## Optimal control problem\n",
    "        # Set up Acados OCP\n",
    "        ocp = AcadosOcp()\n",
    "        ocp.model = model # link to the model (class: AcadosModel)\n",
    "        ocp.dims.N = self.N  # prediction horizon\n",
    "        ocp.solver_options.tf = self.N * self.dt  # total prediction time\n",
    "        ocp.solver_options.qp_solver = \"FULL_CONDENSING_HPIPM\" # Partially condensing interior-point method\n",
    "        ocp.solver_options.integrator_type = \"ERK\" # explicit Runge-Kutta\n",
    "        ocp.solver_options.nlp_solver_type = \"SQP\" # sequential quadratic programming\n",
    "\n",
    "        # Set up cost function\n",
    "        ocp.cost.cost_type = \"LINEAR_LS\"\n",
    "        ocp.cost.cost_type_e = \"LINEAR_LS\"\n",
    "        ocp.cost.W = np.block([\n",
    "            [self.Q, np.zeros((self.dim_states, self.dim_inputs))],\n",
    "            [np.zeros((self.dim_inputs, self.dim_states)), self.R],\n",
    "        ])\n",
    "        ocp.cost.W_e = self.Qf\n",
    "\n",
    "        # Set up mapping from QP to OCP\n",
    "        # Define output matrix for non-terminal state\n",
    "        ocp.cost.Vx = np.block([\n",
    "            [np.eye(self.dim_states)],\n",
    "            [np.zeros((self.dim_inputs, self.dim_states))]\n",
    "        ])\n",
    "        # Define breakthrough matrix for non-terminal state\n",
    "        ocp.cost.Vu = np.block([\n",
    "            [np.zeros((self.dim_states, self.dim_inputs))],\n",
    "            [np.eye(self.dim_inputs)]\n",
    "        ])\n",
    "        # Define output matrix for terminal state\n",
    "        ocp.cost.Vx_e = np.eye(self.dim_states)\n",
    "\n",
    "        # Initialize reference of task (stabilization)\n",
    "        ocp.cost.yref = np.concatenate((self.target_state, np.zeros(self.dim_inputs)))\n",
    "        ocp.cost.yref_e = self.target_state\n",
    "\n",
    "        # Input constraints\n",
    "        ocp.constraints.idxbu = np.arange(self.dim_inputs)\n",
    "\n",
    "        if self.env.input_lbs is None:\n",
    "            ocp.constraints.lbu = np.full(self.dim_inputs, -1e6)\n",
    "        else:\n",
    "            ocp.constraints.lbu = self.U_tighten[0]\n",
    "\n",
    "        if self.env.input_ubs is None:\n",
    "            ocp.constraints.ubu = np.full(self.dim_inputs, 1e6)\n",
    "        else:\n",
    "            ocp.constraints.ubu = self.U_tighten[1]\n",
    "\n",
    "        # Expand initial state constraints (not here, do online)\n",
    "        # Add Omega constraints on initial state x0: A x0 <= b\n",
    "        ocp.dims.nh_0 = self.Omega_tube.A.shape[0]\n",
    "        ocp.model.con_h_expr_0 = ca.mtimes(self.Omega_tube.A, ocp.model.x)\n",
    "        ocp.constraints.lh_0 = -1e6 * np.ones(self.Omega_tube.A.shape[0])\n",
    "        ocp.constraints.uh_0 = 1e6 * np.ones(self.Omega_tube.A.shape[0])  # placeholder\n",
    "\n",
    "        # Expand tighten state constraints \n",
    "        ocp.dims.nh = self.X_tighten.A.shape[0]\n",
    "        ocp.dims.nh_e = self.X_tighten.A.shape[0]\n",
    "        ocp.model.con_h_expr = ca.mtimes(self.X_tighten.A, ocp.model.x)\n",
    "        ocp.model.con_h_expr_e = ca.mtimes(self.X_tighten.A, ocp.model.x)\n",
    "        ocp.constraints.lh = -1e6 * np.ones(self.X_tighten.A.shape[0])\n",
    "        ocp.constraints.lh_e = -1e6 * np.ones(self.X_tighten.A.shape[0])\n",
    "        ocp.constraints.uh = self.X_tighten.b.flatten()\n",
    "        ocp.constraints.uh_e = self.X_tighten.b.flatten()\n",
    "\n",
    "        # Recreate solver with tightened constraints\n",
    "        self.ocp = ocp\n",
    "        self.solver = AcadosOcpSolver(self.ocp, json_file=f\"{self.name}.json\", generate=True)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Tube-based MPC setup with constraint tightening completed.\")\n",
    "\n",
    "    @check_input_constraints\n",
    "    def compute_action(self, current_state: np.ndarray, current_time) -> np.ndarray:\n",
    "\n",
    "        # Set upper limit of convex set equality constraint on target step to be 0\n",
    "        lh_dynamic = self.Omega_tube.A @ current_state + self.Omega_tube.b.flatten()\n",
    "        self.solver.constraints_set(0, \"uh\", lh_dynamic)\n",
    "\n",
    "        status = self.solver.solve()\n",
    "\n",
    "        x_nominal = self.solver.get(0, \"x\")\n",
    "        u_nominal = self.solver.get(0, \"u\")\n",
    "\n",
    "        # Apply tube feedback control\n",
    "        u_real = u_nominal + self.K_feedback @ (current_state - x_nominal)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Current state:\", current_state)\n",
    "            print(\"Nominal state:\", x_nominal)\n",
    "            print(\"Nominal input:\", u_nominal)\n",
    "            print(\"Tube-corrected input:\", u_real)\n",
    "\n",
    "        # Also return nominal predictions\n",
    "        x_pred = np.zeros((self.N + 1, self.dim_states))\n",
    "        u_pred = np.zeros((self.N, self.dim_inputs))\n",
    "        for i in range(self.N + 1):\n",
    "            x_pred[i, :] = self.solver.get(i, \"x\")\n",
    "            if i < self.N:\n",
    "                u_pred[i, :] = self.solver.get(i, \"u\")\n",
    "\n",
    "        return u_real, x_pred, u_pred, u_nominal\n",
    "\n",
    "    def plot_robust_invariant_set(self, lbz, ubz, tol=1e-4, max_iter=10):\n",
    "        \n",
    "        \"\"\"\n",
    "        Plot the 2D invariant set (Œ©). Assumes 2D state space.\n",
    "        \"\"\"\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(6, 6))\n",
    "\n",
    "        def plot_polytope(Omega: pt.Polytope, label=None):\n",
    "            Omega_vertices = Omega.V\n",
    "            assert Omega_vertices.shape[1] == 2, \"Only 2D invariant sets are supported.\"\n",
    "\n",
    "            # Convex hull of the Omega polytope\n",
    "            hull = ConvexHull(Omega_vertices)\n",
    "            hull_pts = Omega_vertices[hull.vertices]\n",
    "            hull_pts = np.vstack([hull_pts, hull_pts[0]])\n",
    "\n",
    "            plt.fill(hull_pts[:, 0], hull_pts[:, 1], color='red', alpha=0.1, label=label)\n",
    "            plt.plot(hull_pts[:, 0], hull_pts[:, 1], color='red', linewidth=2)\n",
    "\n",
    "        # Step 1: Define initial disturbance set D (box)\n",
    "        dim = len(ubz)\n",
    "\n",
    "        # H-representation: H x ‚â§ h\n",
    "        H_box = np.vstack([np.eye(dim), -np.eye(dim)])\n",
    "        h_box = np.hstack([ubz, -lbz])\n",
    "\n",
    "        # Create polytope with both H-rep and V-rep\n",
    "        D = pt.Polytope(H_box, h_box)\n",
    "\n",
    "        # Step 2: Initialize Omega := D\n",
    "        Omega = D\n",
    "        plot_polytope(Omega, label = \"Œ©\")\n",
    "\n",
    "        A_cl = self.A + self.B @ self.K_feedback\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            # Step 3: Apply affine map A_cl to Omega: A_cl * Omega\n",
    "            A_Omega = self.affine_map(Omega, A_cl)\n",
    "\n",
    "            # Step 4: Minkowski sum: Omega_next = A_Omega ‚äï D\n",
    "            Omega_next = A_Omega + D\n",
    "\n",
    "            # Step 5: Check convergence via bounding box approximation\n",
    "            bounds_old = self.estimate_bounds_from_polytope(Omega)\n",
    "            bounds_new = self.estimate_bounds_from_polytope(Omega_next)\n",
    "\n",
    "            if np.allclose(bounds_old, bounds_new, atol=tol):\n",
    "                return Omega_next  # Return as vertices\n",
    "\n",
    "            Omega = Omega_next\n",
    "            plot_polytope(Omega)\n",
    "\n",
    "        plt.title(\"Robust Invariant Set Œ©\")\n",
    "        plt.xlabel(\"Position p\")\n",
    "        plt.ylabel(\"Velocity v\")\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b829d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 2: Robust Invariant Set and Phase Portrait for RMPC**  \n",
    "\n",
    "One of the key features that distinguishes Robust MPC (RMPC) from standard MPC is its explicit consideration of worst-case disturbances in the control design. To ensure robust constraint satisfaction under all allowable disturbances, RMPC requires the computation of a **Robust Invariant Set (RIS)** ‚Äî a set of states from which the system can be kept within constraints, regardless of disturbance realizations.\n",
    "\n",
    "In this section, we present how the RIS can be recursively constructed from the original disturbance set using set operations. This forms a fundamental step unique to RMPC, as it quantifies the margin of robustness and is later used to tighten constraints and guarantee recursive feasibility.\n",
    "\n",
    "Finally, we visualize the performance of RMPC in the phase portrait, comparing the actual trajectory with the predicted nominal states and illustrating how the system safely remains within the robust tube defined by the RIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371219d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Define the physical boundary condition\n",
    "state_lbs = np.array([-2.0, -4.0])\n",
    "state_ubs = np.array([2.0, 4.0])\n",
    "input_lbs = -8.0\n",
    "input_ubs = 8.0\n",
    "\n",
    "# Note: the value here in upper and lower bounds are defined for continuous time, \n",
    "#       but the dynamics is discrete, so it will be divided by the frequency in simulator automatically\n",
    "disturbance_lbs = np.array([-0.05, -0.1])\n",
    "disturbance_ubs = np.array([0.05, 0.1])\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs,\n",
    "          disturbance_lbs=disturbance_lbs, disturbance_ubs=disturbance_ubs)\n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics = Dynamics(env)\n",
    "\n",
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 10 # controll frequency\n",
    "t_terminal = 8 # time length of simulation\n",
    "\n",
    "N = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed79abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RMPC controller\n",
    "controller_rmpc = LinearRMPCController(env, dynamics, Q, R, Qf, freq, N, max_iter=10, name='LinearRMPC_example', verbose=False)\n",
    "controller_rmpc.plot_robust_invariant_set(disturbance_lbs, disturbance_ubs)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_rmpc = Simulator(dynamics, controller_rmpc, env, 1/freq, t_terminal)\n",
    "simulator_rmpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_rmpc = Visualizer(simulator_rmpc)\n",
    "visualizer_rmpc.display_phase_portrait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47584f",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "The first figure illustrates the iterative computation of the robust invariant set $\\Omega_{\\text{tube}}$, starting from the initial disturbance set $\\mathcal{D}$. Each contour represents one iteration of the recursion $\\Omega_{k+1} = \\boldsymbol{A}_{\\text{cl}} \\Omega_k \\oplus \\mathcal{D}$, gradually expanding until convergence. The final set captures all possible closed-loop state deviations caused by bounded disturbances. This invariant set serves as the cross-sectional shape of the robust tube used in RMPC for constraint tightening.\n",
    "\n",
    "In the second figure, we visualize the performance of the RMPC controller in the phase portrait, showing the real system trajectory (blue) alongside the nominal trajectory (black dashed). The red tube, centered around the nominal trajectory, represents the translated invariant set $\\Omega_{\\text{tube}}$ at each time step. It is evident that the real trajectory remains entirely within the robust tube, confirming that RMPC successfully maintains constraint satisfaction under additive disturbances and ensures robust stability throughout the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca29290",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 3: Simulation and Visualization**  \n",
    "\n",
    "Here, we instantiate the previously implemented RMPC class and apply it to a concrete control problem to evaluate its performance.\n",
    "\n",
    "Suppose the original discrete-time dynamics are given by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_{k} + \\boldsymbol{B} \\boldsymbol{u}_{k},\n",
    "$$\n",
    "\n",
    "We now consider the presence of an additive disturbance $\\boldsymbol{w}_k \\in \\mathcal{D}$, leading to the following disturbed dynamics:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_{k} + \\boldsymbol{B} \\boldsymbol{u}_{k} + \\boxed{ \\boldsymbol{w}_{k} },\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{w}_k$ is a random disturbance uniformly distributed within the bounded set $\\mathcal{D}=[\\boldsymbol{\\underline{w}}, \\boldsymbol{\\overline{w}}]$.\n",
    "\n",
    "We us the following parameter set as testbed:  \n",
    "\n",
    "   - case: 1 (linear case)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "   - state constraints: $ \\mathcal{X}_1 = [-2.0, 2.0]$, $ \\mathcal{X}_2 = [-4.0, 4.0]$\n",
    "\n",
    "   - input constraints: $ \\mathcal{U} = [-8.0, 8.0]$\n",
    "\n",
    "   - disturbances bounds: $ \\mathcal{D}_1 = [-0.4, 0.4]$, $ \\mathcal{D}_2 = [-0.8, 0.8]$\n",
    "\n",
    "   - weight in cost: $\\bm{Q} = \\text{diag}([1, 1])$, $\\bm{R} = [0.1]$\n",
    "\n",
    "   - horizon $N = 30$\n",
    "\n",
    "   - control frequency $f = 10$\n",
    "\n",
    "We apply both the previously implemented linear MPC and robust MPC controllers to this scenario, run simulations, and observe the resulting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Define the physical boundary condition\n",
    "state_lbs = np.array([-2.0, -4.0])\n",
    "state_ubs = np.array([2.0, 4.0])\n",
    "input_lbs = -8.0\n",
    "input_ubs = 8.0\n",
    "\n",
    "# Note: the value here in upper and lower bounds are defined for continuous time, \n",
    "#       but the dynamics is discrete, so it will be divided by the frequency in simulator automatically\n",
    "disturbance_lbs = np.array([-0.4, -0.8])\n",
    "disturbance_ubs = np.array([0.4, 0.8])\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "# Arguments (without constraints): \n",
    "#   1) case: $n \\in [1, 2, 3, 4]$, type: int\n",
    "#   2) initial state: x_0 = [p_0, v_0], type: np.array\n",
    "#   3) terminal state: x_T = [p_T, v_T], type: np.array\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs,\n",
    "          disturbance_lbs=disturbance_lbs, disturbance_ubs=disturbance_ubs)\n",
    "#env.test_env() #  shape of slope (left side) and theta curve (right side) \n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "# Arguments: \n",
    "#   1) an object of class `Env`, type: Env  \n",
    "dynamics = Dynamics(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "N = 30\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 10 # controll frequency\n",
    "t_terminal = 8 # time length of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ee299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LMPC\n",
    "# Define the MPC controller\n",
    "controller_mpc = MPCController(env, dynamics, Q, R, Qf, freq, N, name='MPC_baseline', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics, controller_mpc, env, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "\n",
    "## RMPC\n",
    "# Define the RMPC controller\n",
    "controller_rmpc = LinearRMPCController(env, dynamics, Q, R, Qf, freq, N, max_iter=100, name='RMPC', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_rmpc = Simulator(dynamics, controller_rmpc, env, 1/freq, t_terminal)\n",
    "simulator_rmpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_rmpc = Visualizer(simulator_rmpc)\n",
    "visualizer_rmpc.display_contrast_plots(\"Comparison between the linear MPC and robust MPC under disturbanced scenario\", simulator_mpc)\n",
    "visualizer_rmpc.display_contrast_animation_same(simulator_mpc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba395e88",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "The figure above compares the performance of a standard linear MPC controller and a RMPC controller under a disturbed scenario. Although both controllers successfully guide the system toward the target, key differences emerge in how each handles uncertainty.\n",
    "\n",
    "In the position and velocity plots, we observe that the RMPC trajectory (blue) is generally more conservative but smoother compared to the baseline MPC (red dashed). This is a direct consequence of RMPC‚Äôs tube-based formulation, which anticipates worst-case disturbances and proactively tightens constraints using the robust invariant set (RIS).\n",
    "\n",
    "On the input plot, RMPC tends to inject slightly more conservative control early on to preempt deviation, whereas linear MPC only reacts to observed state errors. Despite similar nominal trajectories, the RMPC ensures that the real trajectory stays within a guaranteed safety tube ‚Äî something the baseline controller cannot ensure without explicit disturbance modeling.\n",
    "\n",
    "This contrast also highlights a key insight: **robustness is not simply about reacting to disturbance ‚Äî it is about proactively embedding worst-case awareness into the optimization structure itself.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2057d",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Improve the Robustness of Normal MPC**\n",
    "\n",
    "Could we make a linear MPC controller behave more robustly without redesigning it as RMPC? Try it out yourself!\n",
    "\n",
    "*Hint: think about the three key hyperparameters we introduced in the first part that affecting the MPC's performance most. Which one we should choose?*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
