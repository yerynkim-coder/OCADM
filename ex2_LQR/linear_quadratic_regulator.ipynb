{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5be840",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 2: Linear Quadratic Regulator**\n",
    "\n",
    "\n",
    "In this chapter, we will provide the implementation of the LQR controller and demonstrate how it can be applied to a specific task (stabilization task). We begin by formulating the finite-horizon discrete-time LQR, where the optimal control policy is computed via backward dynamic programming over a fixed time window. This formulation provides time-varying feedback gains and serves as a natural starting point for understanding the structure of optimal control.\n",
    "\n",
    "We then move to the infinite-horizon LQR formulation, which yields a time-invariant feedback gain by solving the algebraic Riccati equation (DARE). This approach is more computationally efficient and widely used in practice when the control objective is to stabilize the system over an indefinite time horizon.\n",
    "\n",
    "Finally, we conclude by discussing several limitations of the LQR framework, including its reliance on linearized dynamics, quadratic cost assumptions, and not compatible with constraints. These limitations motivate the exploration of more advanced control methods, such as Model Predictive Control (MPC) and reinforcement learning-based approaches.\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 2 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 1 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Finite-horizon LQR</td>\n",
    "    <td>implement the finite-horizon LQR controller using bellman recursion</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>simulation and visualization</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Infinite-horizon LQR</td>\n",
    "    <td>implement the infinite-horizon LQR controller using bellman equation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>simulation and visualization</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 3 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"1\">Influence of hyperparameters in LQR</td>\n",
    "    <td>analyze the impact of adjusting the weight matrices on system behavior.</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Limitations of LQR</td>\n",
    "    <td>Limitation 1: linear dynamics and quadratic cost</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Limitation 2: not accounting for constraints</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "First, we need to set up our Python environment and import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1e8fa-a5c2-4219-a5d0-d6cc0ade876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *\n",
    "\n",
    "import numpy as np\n",
    "import casadi as ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554d557",
   "metadata": {},
   "source": [
    "### **Problem setup:**\n",
    "\n",
    "- Task: start from given initial position $p_0$, to reach a given terget position $p_T$ (Stabilization)\n",
    "\n",
    "- Slope profile (height $h$ with reference to horizontal displacement $p$):  \n",
    "   - case 1: zero slope (linear case), $h(p) = c$\n",
    "   - case 2: constant slope (linear case), $h(p) = \\frac{\\pi}{18} \\cdot p$\n",
    "   - case 3: varying slope for small disturbances (nonlinear case), $h(p) = k \\cdot \\cos(18 p)$\n",
    "   - case 4: varying slope for under actuated case (nonlinear case), $h(p) = \\begin{cases} k \\cdot \\sin(3 p), & p \\in [- \\frac{\\pi}{2}, \\frac{\\pi}{6}] \\\\ k, & p \\in (-\\infty, -\\frac{\\pi}{2}) \\cup (\\frac{\\pi}{6}, \\infty) \\end{cases}$\n",
    "\n",
    "- System dynmaics of 1d mountain car model (in State space representation): \n",
    "   - state vector $\\boldsymbol{x} = [p, v]^T$\n",
    "   - input vector $u$\n",
    "   - system dynamics:\n",
    "   \\begin{align*}\n",
    "     \\begin{bmatrix} \\dot{p} \\\\ \\dot{v} \\end{bmatrix} = \\begin{bmatrix} v \\\\ - g \\sin(\\theta) \\cos(\\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\cos(\\theta)  \\end{bmatrix} u\n",
    "   \\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cad49",
   "metadata": {},
   "source": [
    "### **Preparation: define the mountain car environment and the system dynamics**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi symbolic system, inclusive defining the profile over a slope $h(p)$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establish the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "**Step 1: specify the arguments for class `Env` and instantiate the class**\n",
    "\n",
    "- To start with the simpler case (also more compatible with LQR), we will initially focus on a linear system in an unconstrained scenario\n",
    "\n",
    "- Parameters in the task:  \n",
    "   - case: 1 (linear case)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "**Step 2: call function `test_env()` to plot the mountain profile $h(p)$ and curve of inclination angle $\\theta(p)$**\n",
    "\n",
    "**Step 3: specify the arguments for class `Dynmaics` and instantiate the class**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d782c95-de15-49ec-a999-37ee410d8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "# Arguments (without constraints): \n",
    "#   1) case: $n \\in [1, 2, 3, 4]$, type: int\n",
    "#   2) initial state: x_0 = [p_0, v_0], type: np.array\n",
    "#   3) terminal state: x_T = [p_T, v_T], type: np.array\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env.test_env() #  shape of slope (left side) and theta curve (right side) \n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "# Arguments: \n",
    "#   1) an object of class `Env`, type: Env  \n",
    "dynamics = Dynamics(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea619587",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Part (a): finite-horizon discrete-time LQR controller**\n",
    "\n",
    "In this section, we will provide the implementation of the finite horizon LQR controller and demonstrate how it can be applied to a specific task (stabilization task). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b99a5",
   "metadata": {},
   "source": [
    "**Problem formulation:**\n",
    "\n",
    "For a linear system:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+1}} = \\boldsymbol{A} \\boldsymbol{x_k} + \\boldsymbol{B} \\boldsymbol{u_k}\n",
    "$$\n",
    "\n",
    "The finite-horizon quadratic cost function is given by:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{x_0}) = \\boldsymbol{x}_N^T \\boldsymbol{Q}_N \\boldsymbol{x}_N + \\sum_{k=0}^{N-1} \\left(\\boldsymbol{x}_k^T \\boldsymbol{Q}_k \\boldsymbol{x}_k + \\boldsymbol{u}_k^T \\boldsymbol{R}_k \\boldsymbol{u}_k\\right)\n",
    "$$\n",
    "\n",
    "*Note that: in this formulation, the weighting matrices $\\boldsymbol{Q}_k$ and $\\boldsymbol{R}_k$ are defined as time-varying. This is intended to maintain generality in the problem setup.\n",
    "In practical applications, however, it is common to assume time-invariant weights, that is, $\\boldsymbol{Q}_0=...=\\boldsymbol{Q}_{N}=\\boldsymbol{Q}$ and $\\boldsymbol{R}_0=...=\\boldsymbol{R}_{N-1}=\\boldsymbol{R}$, which can also be regarded as a special case of this general formulation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340ee82",
   "metadata": {},
   "source": [
    "**Problem solving:**\n",
    "\n",
    "We solve this problem using **dynamic programming**, starting from the terminal time step $ k = N $ and working backwards to $ k = 0 $.\n",
    "\n",
    "We start with the value function at time $N$:\n",
    "\n",
    "$$\n",
    "V_N(\\boldsymbol{x}_N) = \\boldsymbol{x}_N^T \\boldsymbol{Q}_N \\boldsymbol{x}_N\n",
    "$$\n",
    "\n",
    "with terminal condition:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_N = \\boldsymbol{Q}_N\n",
    "$$\n",
    "\n",
    "The **Bellman recursion** for backward value function update is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_k(\\boldsymbol{x}_k) \n",
    "&= \\min_{\\boldsymbol{u}_k} \\left[ \\boldsymbol{x}_k^T \\boldsymbol{Q}_k \\boldsymbol{x}_k + \\boldsymbol{u}_k^T \\boldsymbol{R}_k \\boldsymbol{u}_k + V_{k+1}(\\boldsymbol{x}_{k+1}) \\right] \\\\\n",
    "&= \\min_{\\boldsymbol{u}_k} \\left[ \\boldsymbol{x}_k^T \\boldsymbol{Q}_k \\boldsymbol{x}_k + \\boldsymbol{u}_k^T \\boldsymbol{R}_k \\boldsymbol{u}_k + (\\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k)^T \\boldsymbol{S}_{k+1} (\\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Expanding the above and completing the square yields the optimal control law:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}_k^* = \\boldsymbol{K}_k \\boldsymbol{x}_k \n",
    "$$\n",
    "\n",
    "with time-varying gain matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{K}_k = -(\\boldsymbol{R}_k + \\boldsymbol{B}^T \\boldsymbol{S}_{k+1} \\boldsymbol{B})^{-1} \\boldsymbol{B}^T \\boldsymbol{S}_{k+1} \\boldsymbol{A}\n",
    "$$\n",
    "\n",
    "The Riccati matrix update, also known as **discrete-time algebraic Riccati equation (DARE)**, is:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_k = \\boldsymbol{Q}_k + \\boldsymbol{A}^T \\boldsymbol{S}_{k+1} \\boldsymbol{A} - \\boldsymbol{A}^T \\boldsymbol{S}_{k+1} \\boldsymbol{B} (\\boldsymbol{R}_k + \\boldsymbol{B}^T \\boldsymbol{S}_{k+1} \\boldsymbol{B})^{-1} \\boldsymbol{B}^T \\boldsymbol{S}_{k+1} \\boldsymbol{A}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c4ff2",
   "metadata": {},
   "source": [
    "**Step 1: implement the finite horizon LQR controller**  \n",
    "\n",
    "In this step, we will implement the setup function `setup_external()`, which will be automatically called in the constructor of class `FiniteLQRController`. The implementation will follow three key steps:\n",
    "\n",
    "1\\) Obtain the system matrices: Retrieve the $\\boldsymbol{A}$ and $\\boldsymbol{B}$ matrices from the discrete-time system dynamics;  \n",
    "    Hint: you may use the method `get_linearized_AB_discrete()` from class `Dynamics` to get the discretized system matrices\n",
    "\n",
    "2\\) Calculate the feedback gain $\\boldsymbol{K}$: Use the computed Hessian $\\boldsymbol{S}$ to determine the LQR feedback gain $\\boldsymbol{K}$;  \n",
    "    Hint: you may use the method `numpy.linalg.inv()` to solve the inverse matrix\n",
    "\n",
    "3\\) Update the Hessian matrix $\\boldsymbol{S}$ of optimal cost: use riccati recursion to calculate $\\boldsymbol{S}$ for the last step;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f945dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_external(self) -> None:\n",
    "        \n",
    "    # Set up equilibrium state\n",
    "    self.x_eq = self.state_lin\n",
    "\n",
    "    # Solve input at equilibrium\n",
    "    self.u_eq = self.dynamics.get_equilibrium_input(self.x_eq)\n",
    "\n",
    "    # Linearize and discretize the system dynamics at equilibrium\n",
    "    self.A, self.B = self.dynamics.get_linearized_AB_discrete(\n",
    "        current_state=self.x_eq, current_input=self.u_eq, dt=self.dt\n",
    "    )\n",
    "\n",
    "    # Initialize terminal cost\n",
    "    S = self.Q_N.copy()\n",
    "\n",
    "    # Solve Bellman Recursion from backwards to compute gain matrix\n",
    "    for k in reversed(range(self.N)):\n",
    "\n",
    "        K = - np.linalg.inv(self.R + self.B.T @ S @ self.B) @ (self.B.T @ S @ self.A)\n",
    "\n",
    "        self.K_list[k] = K\n",
    "\n",
    "        S = self.Q + self.A.T @ S @ self.A - (self.A.T @ S @ self.B) @ np.linalg.inv(self.R + self.B.T @ S @ self.B) @ (self.B.T @ S @ self.A)\n",
    "\n",
    "    if self.verbose:\n",
    "        print(f\"LQR Gain Matrix K: {self.K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7781a9",
   "metadata": {},
   "source": [
    "**Step 2: Bind the defined setup function to the class \"FiniteLQRController\", and run the simulation to see the performance of controller**  \n",
    "\n",
    "1\\) Bind the defined setup function for DP algorithm `setup_external()` to class `FiniteLQRController`;\n",
    "\n",
    "2\\) Specify the arguments and instantiate the controller class `FiniteLQRController`; \n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "    i) weight for state $\\boldsymbol{Q} = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\boldsymbol{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "3\\) Instantiate the class `Simulator` and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "4\\) Instantiate the class `Visualizer`, call function `display_final_results()` and `display_animation()` to show the simulations;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86587b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the defined LQR algorithm to the corresponding class, will be automatically called by constructor\n",
    "FiniteLQRController.setup = setup_external\n",
    "\n",
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Q_N = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 10 # time length of simulation\n",
    "horizon = freq * t_terminal # number of time steps\n",
    "\n",
    "# Instantiate the finite horizon LQR controller class\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input, type: np.array  \n",
    "#                                         iii) `Q_N`: weight metrix of state, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) name: the name of current coltroller displayed in plots, type: string\n",
    "controller_lqr_f = FiniteLQRController(env, dynamics, Q, R, Q_N, freq, horizon=horizon, name='LQR_finite')\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_f = Simulator(dynamics, controller_lqr_f, env, 1/freq, t_terminal)\n",
    "simulator_lqr_f.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_f = Visualizer(simulator_lqr_f)\n",
    "visualizer_lqr_f.display_plots()\n",
    "visualizer_lqr_f.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993c0d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Part (b): infinite-horizon discrete-time LQR controller**\n",
    "\n",
    "While the finite-horizon LQR offers greater flexibility by incorporating terminal costs and producing time-varying control gains, it suffers from a key limitation: the solution can only be computed via backward recursion. As a result, the entire control policy must be precomputed before execution, making it unsuitable for online adaptation in most practical scenarios. In this section, we will provide the implementation of the LQR controller and demonstrate how it can be applied to a specific task (stabilization task). We will also show the resulting performance and the effects of using the LQR controller in achieving optimal control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5507f9",
   "metadata": {},
   "source": [
    "**Problem formulation:**\n",
    "\n",
    "For a same linear system formulation:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+1}} = \\boldsymbol{A} \\boldsymbol{x_k} + \\boldsymbol{B} \\boldsymbol{u_k}\n",
    "$$\n",
    "\n",
    "As the horizon length $N$ approaches infinity, the problem converges to the infinite-horizon formulation with time-invariant optimal policy. Then the infinite-horizon quadratic cost function is given by:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{x_0}) = \\sum_{k=0}^{\\infty} \\left(\\boldsymbol{x}_k^T \\boldsymbol{Q} \\boldsymbol{x}_k + \\boldsymbol{u}_k^T \\boldsymbol{R} \\boldsymbol{u}_k\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82447071",
   "metadata": {},
   "source": [
    "**Explicit Solution:**\n",
    "\n",
    "The solution to this problem is obtained by solving the **Discrete Algebraic Riccati Equation (DARE):**\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \\boldsymbol{Q} + \\boldsymbol{A}^T \\boldsymbol{S} \\boldsymbol{A} - \\boldsymbol{A}^T \\boldsymbol{S} \\boldsymbol{B}(\\boldsymbol{R} + \\boldsymbol{B}^T \\boldsymbol{S} \\boldsymbol{B})^{-1} \\boldsymbol{B}^T \\boldsymbol{S} \\boldsymbol{A}\n",
    "$$\n",
    "\n",
    "The optimal feedback control policy is given by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u^*} = \\boldsymbol{K} \\boldsymbol{x_k}, \\quad \\boldsymbol{K} = -\\left( \\boldsymbol{R} + \\boldsymbol{B}^T \\boldsymbol{S} \\boldsymbol{B} \\right)^{-1} \\boldsymbol{B}^T \\boldsymbol{S} \\boldsymbol{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d49cc",
   "metadata": {},
   "source": [
    "**Step 1: implement the infinite horizon LQR controller**  \n",
    "\n",
    "In this step, we will implement the setup function `setup_external()`, which will be automatically called in the constructor of class `LQRController`. The implementation will follow three key steps:\n",
    "\n",
    "1\\) Obtain the system matrices: Retrieve the $\\boldsymbol{A}$ and $\\boldsymbol{B}$ matrices from the discrete-time system dynamics;  \n",
    "    Hint: you may use the method `get_linearized_AB_discrete()` from class `Dynamics` to get the discretized system matrices\n",
    "\n",
    "2\\) Solve for the Hessian matrix $\\boldsymbol{S}$ of optimal cost: Compute the solution to the Discrete Algebraic Riccati Equation (DARE) to obtain $\\boldsymbol{S}$;  \n",
    "    Hint: you may use the method `scipy.linalg.solve_discrete_are()` to solve DARE\n",
    "\n",
    "3\\) Calculate the feedback gain $\\boldsymbol{K}$: Use the computed Hessian $\\boldsymbol{S}$ to determine the LQR feedback gain $\\boldsymbol{K}$;  \n",
    "    Hint: you may use the method `numpy.linalg.inv()` to solve the inverse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_external(self):\n",
    "\n",
    "    # Set up equilibrium state\n",
    "    self.x_eq = self.state_lin # self.state_lin default to be target state\n",
    "\n",
    "    # Solve input at equilibrium\n",
    "    self.u_eq = self.dynamics.get_equilibrium_input(self.x_eq)\n",
    "\n",
    "    # Linearize dynamics at equilibrium\n",
    "    # Hint: use function self.dynamics.get_linearized_AB_discrete\n",
    "    self.A, self.B = self.dynamics.get_linearized_AB_discrete(\n",
    "        current_state=self.x_eq, current_input=self.u_eq, dt=self.dt\n",
    "    )\n",
    "\n",
    "    # Solve DARE to compute gain matrix\n",
    "    # Hint: use function scipy.linalg.solve_discrete_are\n",
    "    S = scipy.linalg.solve_discrete_are(self.A, self.B, self.Q, self.R)\n",
    "    self.K = - np.linalg.inv(self.R + self.B.T @ S @ self.B) @ (self.B.T @ S @ self.A)\n",
    "\n",
    "    # With this LQR gain self.K, the input can be calculate by multiplying the state difference\n",
    "    # u = self.u_eq + self.K @ (current_state - self.self.x_eq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6160",
   "metadata": {},
   "source": [
    "**Step 2: Bind the defined setup function to the class \"LQRController\", and run the simulation to see the performance of controller**  \n",
    "\n",
    "1\\) Bind the defined setup function for DP algorithm `setup_external()` to class `LQRController`;\n",
    "\n",
    "2\\) Specify the arguments and instantiate the controller class `LQRController` as `LQR_Q=10R`; \n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "    i) weight for state $\\boldsymbol{Q} = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\boldsymbol{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "3\\) Instantiate the class `Simulator` and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "4\\) Instantiate the class `Visualizer`, call function `display_final_results()` and `display_animation()` to show the simulations;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f87f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the defined DP algorithm to the corresponding class, will be automatically called by constructor\n",
    "LQRController.setup = setup_external\n",
    "\n",
    "# Define weight matrix in stage and terminal cost\n",
    "Q_0 = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 10 # time length of simulation\n",
    "\n",
    "# Instantiate the LQR controller class\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) name: the name of current coltroller displayed in plots, type: string\n",
    "controller_lqr_0 = LQRController(env, dynamics, Q_0, R, freq, name='LQR_Q=10R')\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_0 = Simulator(dynamics, controller_lqr_0, env, 1/freq, t_terminal)\n",
    "simulator_lqr_0.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_0 = Visualizer(simulator_lqr_0)\n",
    "visualizer_lqr_0.display_plots()\n",
    "visualizer_lqr_0.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c67a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1beff2",
   "metadata": {},
   "source": [
    "### **Part (C): Influence of the Weight Matrix on LQR Performance**\n",
    "\n",
    "In this section, we will analyze how the choice of weight matrices in the cost function affects the performance of the LQR controller. By tuning the weight matrices $\\boldsymbol{Q}$ (which penalize deviations in state) while keeping $\\boldsymbol{R}$ (which penalizes control input) fixed, we can observe how the system behaves under different control strategies.\n",
    "\n",
    "Specifically, we define two LQR controller with different weight matrices: \n",
    "\n",
    "- `LQR_Q=100R`: $ \\quad \\boldsymbol{Q}_1 = \\text{diag}([10, 10]), \\quad \\boldsymbol{R} = [0.1]$  \n",
    "\n",
    "- `LQR_Q=R`: $ \\quad \\boldsymbol{Q}_2 = \\text{diag}([0.1, 0.1]), \\quad \\boldsymbol{R} = [0.1]$  \n",
    "\n",
    "Using these configurations, we simulate the system's response under two LQR controllers and and make a contrast with the controller defined in the last step (`LQR_Q=10R`). By observing the results, we gain insight into how adjusting the weight matrix influences the controller's aggressiveness and efficiency.\n",
    "\n",
    "*Note that: here we only vary the value of $\\boldsymbol{Q}$. The effect of $\\boldsymbol{R}$ is inverse, as the controller's behavior is fundamentally influenced by the relative magnitude of $\\boldsymbol{Q}$ and $\\boldsymbol{R}$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b152dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "#   for reference: Q_0 = np.diag([1, 1])\n",
    "Q_1 = np.diag([10, 10])\n",
    "Q_2 = np.diag([0.1, 0.1])\n",
    "\n",
    "# Instantiate the LQR controller class, the simulator class, and then run the simulation\n",
    "controller_lqr_1 = LQRController(env, dynamics, Q_1, R, freq, name='LQR_Q=100R')\n",
    "simulator_lqr_1 = Simulator(dynamics, controller_lqr_1, env, 1/freq, t_terminal)\n",
    "simulator_lqr_1.run_simulation()\n",
    "\n",
    "controller_lqr_2 = LQRController(env, dynamics, Q_2, R, freq, name='LQR_Q=R')\n",
    "simulator_lqr_2 = Simulator(dynamics, controller_lqr_2, env, 1/freq, t_terminal)\n",
    "simulator_lqr_2.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_0.display_contrast_plots(simulator_lqr_1, simulator_lqr_2)\n",
    "visualizer_lqr_0.display_contrast_animation_same(simulator_lqr_1, simulator_lqr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8944e",
   "metadata": {},
   "source": [
    "#### **Results Analysis: Influence of LQR Weight Matrices on System Performance**\n",
    "\n",
    "The figure above compares the performance of three Linear Quadratic Regulator (LQR) controllers: **LQR_Q_10**, **LQR_Q_1**, and **LQR_Q_01**. Each controller is defined with different weight matrices $\\boldsymbol{Q}$, which penalize the state deviations, while $\\boldsymbol{R}$, which penalizes control effort, remains the same.\n",
    "\n",
    "The comparison clearly demonstrates the impact of the weight matrix $\\boldsymbol{Q}$ on LQR performance:\n",
    "1. **Higher $\\boldsymbol{Q}$** (`LQR_Q=100R`):  \n",
    "   - Prioritizes minimizing state deviations.  \n",
    "   - Results in faster convergence but higher control effort.  \n",
    "\n",
    "2. **Lower $\\boldsymbol{Q}$** (`LQR_Q=R`):  \n",
    "   - Reduces control effort and produces smoother trajectories.  \n",
    "   - Leads to slower convergence and delayed state correction.  \n",
    "\n",
    "3. **Moderate $\\boldsymbol{Q}$** (`LQR_Q=10R`):  \n",
    "   - Balances control effort and tracking performance.  \n",
    "\n",
    "The selection of $\\boldsymbol{Q}$ depends on the system's requirements:  \n",
    "- For tasks requiring fast convergence, a higher $\\boldsymbol{Q}$ is appropriate.  \n",
    "- For energy-constrained systems, a lower $\\boldsymbol{Q}$ ensures smoother and less aggressive control. \n",
    "\n",
    "#### **Main Conclusion:**\n",
    "\n",
    "By tuning the relative magnitude of weight matrices $\\boldsymbol{Q}$ and $\\boldsymbol{R}$, LQR controllers can achieve the desired trade-off between tracking performance and control effort.\n",
    "\n",
    "*Note that: in this example, we use $\\boldsymbol{Q}$ with eigenvalues equal to $\\boldsymbol{R}$ as a moderate baseline. However, this choice does not universally apply, as the magnitude of states and inputs depends on their units. But conversely, in the practice it is always important to carefully scale the states and inputs to similar numerical ranges. Otherwise, large discrepancies in the scales of $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ can result in an ill-conditioned cost Hessian, causing numerical instability and divergence during optimization.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c949",
   "metadata": {},
   "source": [
    "### **Part (D): Limitations of the LQR Controller**\n",
    "\n",
    "At last, we want to emphasize the limitations of LQR: \n",
    "\n",
    "  1) a **quadratic cost function**;\n",
    "\n",
    "  2) a **linear dynamics**;  \n",
    "  \n",
    "  3) and operates under **no constraints**;  \n",
    "\n",
    "While the quadratic cost is easy to be satisfied, we will mostly focus on the last 2 requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fedbb",
   "metadata": {},
   "source": [
    "#### **1. Linear vs. Nonlinear Dynamics**\n",
    "\n",
    "LQR is designed based on linear system dynamics. However, real-world physical systems are often inherently nonlinear. If we directly apply an LQR controller to such nonlinear systems, what would happen? Will the controller still be able to achieve the control objectives, or will it completely fail to work? In this section, we will present simulation results under different scenarios to explore and discuss these questions.\n",
    "\n",
    "It should first be noted that when attempting to apply an LQR control law to a nonlinear system, regardless of whether the controller ultimately works or not, we must first select an operating point around which to linearize the nonlinear dynamics. This requirement is inherent to the structure of the LQR design. The choice of the linearization point is arbitrary in principle, since at any given state we can always compute a corresponding equilibrium input by setting the right-hand side of the Newtonian dynamics to zero. However, in practice, **the selection of the linearization point can significantly affect the performance of the LQR controller when applied to a nonlinear system**, as will be demonstrated in the examples that follow.\n",
    "\n",
    "The testbed was chosen to be case 4. The specific curve is shown in the figure below. It can be seen that it consists of a valley and a flat ground. The dynamics of the valley part is nonlinear (the inclination angle is a function of position $p$), while the flat ground part is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the case\n",
    "case_nonlinear = 4\n",
    "# Instantiate class 'Env' \n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env_nonlinear.show_slope()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b82d6",
   "metadata": {},
   "source": [
    "\n",
    "##### **Example 1: linearize around the flat ground, drive the car from the top to the valley**\n",
    "\n",
    "Let us first consider a relatively simple case: the car starts from flat ground and attempts to stabilize at the bottom of the valley under the action of the LQR control law, which implies that:\n",
    "\n",
    "$$\n",
    "p_{start} = 0.6, \\quad p_{target} = -0.5,\n",
    "$$\n",
    "\n",
    "Considering that the valley itself is an attractive equilibrium point, the terminal condition is easily satisfied, and thus the linearization point can be directly chosen at the flat ground:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\overline{x}} = [1.0, 0.0],\n",
    "$$\n",
    "\n",
    "Please run the following simulation and observe the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20242aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start point and the target\n",
    "initial_position = 0.6\n",
    "target_position = -0.5\n",
    "\n",
    "# Define linearization of the flat space\n",
    "state_lin = np.array([1.0, 0.0])\n",
    "\n",
    "# Instantiate class 'Env' \n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear = LQRController(env_nonlinear, dynamics_nonlinear, Q_1, R, freq, name='LQR_case4')\n",
    "# Refresh the algorithm\n",
    "controller_lqr_nonlinear.set_lin_point(state_lin)\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear = Simulator(dynamics_nonlinear, controller_lqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_nonlinear = Visualizer(simulator_lqr_nonlinear)\n",
    "visualizer_lqr_nonlinear.display_plots()\n",
    "visualizer_lqr_nonlinear.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf61f4b",
   "metadata": {},
   "source": [
    "\n",
    "##### **Example 2: linearize around the flat ground, drive the car from the valley to the top**\n",
    "\n",
    "Let us slightly increase the difficulty of the task: while still choosing the flat ground as the linearization point, we attempt to use the LQR control law to drive the car from the bottom of the valley toward the right hilltop. The parameters can be set according to the following values:\n",
    "\n",
    "$$\n",
    "p_{start} = -0.5, \\quad p_{target} = 0.6, \\quad \\boldsymbol{\\overline{x}} = [1.0, 0.0],\n",
    "$$\n",
    "\n",
    "Please run the following simulation and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87024911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start point and the target\n",
    "initial_position = -0.5\n",
    "target_position = 0.6\n",
    "\n",
    "#Define linearization of the flat space\n",
    "state_lin = np.array([1.0, 0.0])\n",
    "\n",
    "# Instantiate class 'Env' \n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear = LQRController(env_nonlinear, dynamics_nonlinear, Q_1, R, freq, name='LQR_case4')\n",
    "# Refresh the algorithm\n",
    "controller_lqr_nonlinear.set_lin_point(state_lin)\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear = Simulator(dynamics_nonlinear, controller_lqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_nonlinear = Visualizer(simulator_lqr_nonlinear)\n",
    "visualizer_lqr_nonlinear.display_plots()\n",
    "visualizer_lqr_nonlinear.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122c118",
   "metadata": {},
   "source": [
    "\n",
    "##### **Example 3: linearize around a point on the slope, drive the car from the valley to the top**\n",
    "\n",
    "Based on the simulation results of Example 2, we attempt to make some adjustments to the linearization point to observe how it affects the system's response. We move the linearization point to a point on the slope, which implies that:\n",
    "\n",
    "$$\n",
    "p_{start} = -0.5, \\quad p_{target} = 0.6, \\quad \\boldsymbol{\\overline{x}} = [0.0, 0.0],\n",
    "$$\n",
    "\n",
    "Rerun the simulation and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start point and the target\n",
    "initial_position = -0.5\n",
    "target_position = 0.6\n",
    "\n",
    "#Define linearization of the flat space\n",
    "state_lin = np.array([0.0, 0.0])\n",
    "\n",
    "# Instantiate class 'Env' \n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear = LQRController(env_nonlinear, dynamics_nonlinear, Q_1, R, freq, name='LQR_case4')\n",
    "# Refresh the algorithm\n",
    "controller_lqr_nonlinear.set_lin_point(state_lin)\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear = Simulator(dynamics_nonlinear, controller_lqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_nonlinear = Visualizer(simulator_lqr_nonlinear)\n",
    "visualizer_lqr_nonlinear.display_plots()\n",
    "visualizer_lqr_nonlinear.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671287ca",
   "metadata": {},
   "source": [
    "##### **Results Analysis: influence of the linearization point**\n",
    "\n",
    "Summarizing the above three examples, we observe that both the task target and the choice of linearization point affect the performance of applying LQR to a nonlinear system. Specifically:\n",
    "\n",
    "- **Example 1**: When the car moves downhill to the left, the required driving force can be fully provided by the LQR policy, thus allowing leftward movement. Moreover, the target itself is a stable equilibrium point of the nonlinear system, meaning that even in the presence of nonlinear disturbances, the system may exhibit some overshoot but will ultimately stabilize at the target point.  \n",
    "\n",
    "- **Example 2**: Unlike in Example 1, when the car moves uphill along the slope, it becomes stuck halfway due to the insufficient input provided by the LQR policy. This is because the linearization point is chosen on the flat platform, leading the LQR controller to compute a policy that only generates inputs sufficient for horizontal motion. However, in order for the car to move uphill, additional input is required to overcome the gravitational force. As a result, the car is unable to generate enough input and ultimately gets stuck halfway up the slope.  \n",
    "\n",
    "- **Example 3**: Building upon Example 2, we move the linearization point onto the slope, allowing the LQR controller to recognize the need to generate additional input to counteract the gravitational component when computing the policy. As a result, the car is able to climb up the slope. However, a drawback is that the car cannot be precisely stabilized at the target point, and a noticeable steady-state error appears. This can be interpreted as a loss of control accuracy caused by model mismatch.  \n",
    "\n",
    "In summary, although the LQR policy can be applied to nonlinear systems, its performance cannot be guaranteed. Therefore, when making such attempts, it is crucial to **carefully select the linearization point** to improve performance as much as possible and to mitigate the losses caused by model mismatch.\n",
    "\n",
    "\n",
    "*Note: The loss of accuracy in nonlinear systems can be mitigated by applying the **iterative LQR (iLQR)** method (which will be introduced in the next Lecture). The key idea of iLQR is to perform a Taylor expansion of the cost function and dynamics around each operating point, incorporating higher-order derivatives into the optimization process. In fact, LQR can be viewed as a special case of iLQR when the system dynamics are linear.*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c4bba",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **‚ùì Extension Questions: Overshoot when applying LQR to nonlinear cases**\n",
    "\n",
    "When applying LQR to nonlinear systems, it is common to observe that the resulting trajectory exhibits overshoot or even fails to reach the desired goal. In some cases, the system may stabilize at a steady-state far from the target.\n",
    "\n",
    "**Open question 1**: What might cause this steady-state offset when applying an LQR controller to a nonlinear system?\n",
    "\n",
    "*Hint: Think about the **assumptions** behind LQR design. LQR is derived from **linearized dynamics around an operating point**.*\n",
    "\n",
    "**Open question 2**: How can you compute this offset? \n",
    "\n",
    "*Hint: Compute the steady-state of the closed-loop system. In this case you should use the nonlinear dynamics and substitute the LQR policy $\\boldsymbol{u} = \\boldsymbol{K} \\boldsymbol{x}$ for the input. For the steady-state the velocity $v_{\\text{steady}} = 0$. Then compute the position $p_{\\text{steady}}$.*\n",
    "\n",
    "</blockquote> \n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551bb4e7",
   "metadata": {},
   "source": [
    "##### **üîç Question:**\n",
    "\n",
    "For Example 3, is there a way to design the LQR such that the car can move uphill along the slope and precisely stop at the target point without exhibiting any steady-state error?\n",
    "\n",
    "*Hint: think about a time-varying policy*\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055c152",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. Effect of Input Constraints on LQR Performance**\n",
    "\n",
    "LQR is formulated under the assumption of **unconstrained inputs**, meaning it does not inherently support input saturation or upper/lower constraints. However, in real-world systems, control inputs such as force, torque, or voltage often have physical limits. When the LQR controller generates control inputs exceeding these limits, the inputs will be **clipped** to the boundaries. This behavior can cause saturation between the actual system trajectory and the ideal trajectory predicted by the LQR controller.\n",
    "\n",
    "To illustrate this limitation, we design two test scenarios:\n",
    "\n",
    "- **Ideal LQR Control (`LQR_linear`)**:  \n",
    "\n",
    "   - Linear system \n",
    "\n",
    "   - No constraints are applied to the control inputs.\n",
    "\n",
    "- **Clipped LQR Control (`LQR_linear_clipped`)**: \n",
    "\n",
    "   - Linear system \n",
    "\n",
    "   - Input constraints are imposed, which means that the input is limited within an upper and lower bound\n",
    "\n",
    "   - Any control input exceeding these bounds is **clipped**, leading to saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the case\n",
    "case_linear = 1\n",
    "\n",
    "# Instantiate class 'Env' \n",
    "env_linear = Env(case_linear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_linear = Dynamics(env_linear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_linear = LQRController(env_linear, dynamics_linear, Q_0, R, freq, name='LQR_case1')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_linear = Simulator(dynamics_linear, controller_lqr_linear, env_linear, 1/freq, t_terminal)\n",
    "simulator_lqr_linear.run_simulation()\n",
    "\n",
    "\n",
    "\n",
    "# Define a upper bound of input a\n",
    "input_ubs = 0.25\n",
    "input_lbs = -0.25\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "env_linear_clipped = Env(case_linear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]), \n",
    "                           input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_linear_clipped = LQRController(env_linear_clipped, dynamics_linear, Q_0, R, freq, name='LQR_case1_clipped')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_linear_clipped = Simulator(dynamics, controller_lqr_linear_clipped, env_linear_clipped, 1/freq, t_terminal)\n",
    "simulator_lqr_linear_clipped.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_linear_clipped = Visualizer(simulator_lqr_linear_clipped)\n",
    "visualizer_lqr_linear_clipped.display_contrast_plots(simulator_lqr_linear)\n",
    "visualizer_lqr_linear_clipped.display_contrast_animation_same(simulator_lqr_linear)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6d926",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "1. **Input Response**:  \n",
    "   - In the input plot (right panel), the ideal LQR (red dashed line) generates a continuous and reasonable input.  \n",
    "   - However, the clipped LQR (blue solid line) shows clear cut-off behavior, where the input is clipped to the specified upper and lower bounds (gray shaded regions). This results in a flattened profile that deviates from the ideal input.\n",
    "\n",
    "2. **Position Response**:  \n",
    "   - In the position plot (left panel), the ideal trajectory (LQR_0, red dashed line) smoothly reaches the target position.  \n",
    "   - In contrast, the clipped trajectory (LQR_real_traj, blue solid line) shows slower convergence due to input clipping.\n",
    "\n",
    "Despite the suppression of velocity by the input constraints, the car is still able to gradually reach the target position. This is attributed to the fact that, for linear systems, a stable equilibrium point is typically globally attractive; which means that, the system will asymptotically converge to the target state from any initial condition. **However, such a property does not generally hold for a nonlinear system, as demonstrated in the following example.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceba66a",
   "metadata": {},
   "source": [
    "To further illustrate this limitation for the nonlinear systems, we design following two test cases:\n",
    "\n",
    "- **Ideal LQR Control for Nonlinear System (`LQR_nonlinear`)**:  \n",
    "\n",
    "   - Nonlinear system \n",
    "\n",
    "   - Linearization point for LQR is chosen to be $\\overline{x} = [0, 0]$ (on the slope)\n",
    "\n",
    "   - No constraints are applied to the control inputs.\n",
    "\n",
    "- **Clipped LQR Control for Nonlinear System (`LQR_nonlinear_clipped`)**: \n",
    "\n",
    "   - Input constraints are imposed, which means that the input is limited within an upper and lower bound\n",
    "\n",
    "   - Any control input exceeding these bounds is **clipped**, leading to an underactuated system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c32f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the case\n",
    "case_nonlinear = 4\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear = LQRController(env_nonlinear, dynamics_nonlinear, Q_1, R, freq, name='LQR_case4')\n",
    "controller_lqr_nonlinear.set_lin_point(np.array([0.0, 0.0]))\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear = Simulator(dynamics_nonlinear, controller_lqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear.run_simulation()\n",
    "\n",
    "\n",
    "\n",
    "# Define a upper bound of input a\n",
    "input_ubs = 8.0\n",
    "input_lbs = -5.0\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_clipped = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]), \n",
    "                           input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear_clipped = LQRController(env_nonlinear_clipped, dynamics_nonlinear, Q_1, R, freq, name='LQR_case4_clipped')\n",
    "controller_lqr_nonlinear_clipped.set_lin_point(np.array([0.0, 0.0]))\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear_clipped = Simulator(dynamics_nonlinear, controller_lqr_nonlinear_clipped, env_nonlinear_clipped, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear_clipped.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_nonlinear_clipped = Visualizer(simulator_lqr_nonlinear_clipped)\n",
    "visualizer_lqr_nonlinear_clipped.display_contrast_plots(simulator_lqr_nonlinear)\n",
    "visualizer_lqr_nonlinear_clipped.display_contrast_animation(simulator_lqr_nonlinear)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a9843",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "1. **Input Response**:  \n",
    "   - In the input plot (right panel), the ideal LQR (red dashed line) generates a reasonable input that a larger control effort is required when the system is far from the target, whereas the required input decreases as the system approaches the target.\n",
    "   - However, the clipped LQR (blue solid line) shows clear cut-off behavior, where the input is clipped to (and consistently bounded to) the specified upper (gray shaded regions).\n",
    "\n",
    "2. **Position Response**:  \n",
    "   - In the position plot (left panel), the ideal trajectory (LQR_0, red dashed line) smoothly reaches the target position. However, due to system nonlinearity, it fails to stabilize at the target and quickly diverges from the equilibrium point.\n",
    "   - In contrast, the clipped trajectory (LQR_real_traj, blue solid line) fails to reach the target position due to input constraints, which prevent it from generating sufficiently large control inputs to overcome the slope. As a result, it keeps oscillating at the bottom of the valley.\n",
    "\n",
    "This limitation highlights the importance of considering input constraints during controller design. Methods such as **Model Predictive Control (MPC)** can address this issue by explicitly incorporating input limits into the optimization problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
