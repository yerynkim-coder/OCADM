{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c523e14e",
   "metadata": {},
   "source": [
    "### **Overview: The Stochastic Shortest Path Problem**\n",
    "\n",
    "This notebook addresses the **Stochastic Shortest Path (SSP)** problem over a finite-state Markov decision process, where the goal is to compute an optimal policy $ \\pi^\\star $ minimizing the expected number of transitions from each initial state to a designated terminal state.\n",
    "\n",
    "We consider a finite set of nodes $ \\mathcal{X} = \\{1, 2, \\dots, n, t\\} $, where $ t $ is the absorbing (termination) state. The system is controlled by inputs $ u \\in \\mathcal{U} = \\{1, \\dots, m\\} $, and transitions occur according to a set of stochastic matrices $ P(u) \\in \\mathbb{R}^{(n+1) \\times (n+1)} $, with each row summing to 1.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Generate random SSP problem data.\n",
    "2. Solve the SSP problem using three methods:\n",
    "   - Policy Iteration\n",
    "   - Value Iteration\n",
    "   - Linear Programming\n",
    "3. Simulate the SSP process and validate the optimal policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11072fa1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 0: Import Libraries and Set Seed**\n",
    "\n",
    "We begin by importing the required Python libraries for numerical computation, optimization, and visualization.\n",
    "\n",
    "To ensure reproducibility of all random results (e.g., transition matrices, simulations), we also fix the random seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 0: Import Required Libraries and Utilities\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from ssp_utils import (\n",
    "    generate_problem_data,\n",
    "    plot_transition_matrix,\n",
    "    SSPPolicyIteration,\n",
    "    SSPValueIteration,\n",
    "    SSPLinearProgram,\n",
    "    simulate_ssp\n",
    ")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43f980",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 1: Generate Random SSP Problem**\n",
    "\n",
    "We begin by generating random problem data $ P(i, j, u) $, which encodes the probability of transitioning from state $ i $ to state $ j $ under action $ u $. The terminal state is absorbing, meaning once it is reached, no further transitions occur.\n",
    "\n",
    "The structure of the probabilities ensures that transitions to nearby states are more likely than to distant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d265674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 1: Generate Problem Data (P tensor)\n",
    "# =========================================\n",
    "N = 100        # Number of states, NOT including the termination state which is N+1\n",
    "M = 100        # Number of control inputs\n",
    "P = generate_problem_data(N=N, M=M, SCALE_EXPONENT=2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88293d6b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 2: Visualize Transition Matrix**\n",
    "\n",
    "To better understand the structure of our SSP problem, we visualize the transition probability matrix $ P(u) $ for a selected control input $ u $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce565742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 2: Visualize Transition Matrix for u = 1\n",
    "# =========================================\n",
    "\n",
    "# Get problem size\n",
    "n = P.shape[0] - 1  # Number of non-terminal states\n",
    "m = P.shape[2]      # Number of control inputs\n",
    "\n",
    "# Display problem info\n",
    "print(f\"Stochastic Shortest Path Problem\\n>> {n + 1} discrete states\\n>> {m} discrete inputs\")\n",
    "\n",
    "# === Sanity check: All entries in P should be in [0, 1]\n",
    "min_val = np.min(P)\n",
    "max_val = np.max(P)\n",
    "if min_val < 0 or max_val > 1:\n",
    "    raise ValueError(\"Entries in P are not in [0,1].\")\n",
    "\n",
    "# === Sanity check: Terminal state condition\n",
    "# p_tt = 1, p_tx = 0 for any x ≠ t\n",
    "if not (np.all(P[n, 0:n, :] == 0) and np.all(P[n, n, :] == 1)):\n",
    "    raise ValueError(\"Terminal state conditions are not satisfied.\")\n",
    "\n",
    "# Plot the transition matrix under the first control input\n",
    "plot_transition_matrix(P, u=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffccb537",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 3: Solve using Policy Iteration**\n",
    "\n",
    "We now solve the SSP problem using the **Policy Iteration** method. This algorithm iteratively evaluates and improves the policy $ \\pi $ until convergence to the optimal policy $ \\pi^\\star $.\n",
    "\n",
    "The cost function being minimized is the expected number of transitions until absorption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 3: Solve using Policy Iteration\n",
    "# =========================================\n",
    "# Solve the stochastic shortest path (SSP) problem using policy iteration.\n",
    "# Goal: Minimize the expected number of transitions from a given initial node\n",
    "#       to the terminal (absorbing) state.\n",
    "#\n",
    "# Input:\n",
    "#   - P: Transition probability tensor\n",
    "# Output:\n",
    "#   - J_policy: Optimal cost-to-go from each node\n",
    "#   - F_policy: Optimal control action at each node\n",
    "\n",
    "print(\"Solution Method 1: Policy Iteration\")\n",
    "print(\"Solving the stochastic shortest path (SSP) problem using policy iteration...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "policy_solver = SSPPolicyIteration(P)\n",
    "J_policy, F_policy = policy_solver.solve()\n",
    "\n",
    "print(f\"Execution time: {time.time() - start_time:.5f}s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46444051",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 4: Solve using Value Iteration**\n",
    "\n",
    "As an alternative to policy iteration, we solve the SSP problem using **Value Iteration**. This method iteratively updates the value function $ J $ until convergence. The result from Policy Iteration can be used to set a meaningful stopping criterion (tolerance $ \\epsilon $).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef864349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 4: Solve using Value Iteration\n",
    "# =========================================\n",
    "# Solve the stochastic shortest path (SSP) problem using value iteration.\n",
    "# Goal: Minimize the expected number of transitions from a given initial node\n",
    "#       to the terminal (absorbing) state.\n",
    "#\n",
    "# Notes:\n",
    "# - Uses a convergence threshold (epsilon) to determine stopping.\n",
    "# - A good epsilon can be informed by the result from policy iteration (Step 4).\n",
    "#\n",
    "# Input:\n",
    "#   - P: Transition probability tensor\n",
    "# Output:\n",
    "#   - J_value: Optimal cost-to-go from each node\n",
    "#   - F_value: Optimal control action at each node\n",
    "\n",
    "print(\"Solution Method 2: Value Iteration\")\n",
    "print(\"Solving the stochastic shortest path (SSP) problem using value iteration...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "value_solver = SSPValueIteration(P)\n",
    "J_value, F_value = value_solver.solve(epsilon=0.1)\n",
    "\n",
    "print(f\"Execution time: {time.time() - start_time:.5f}s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2caa7a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 5: Solve using Linear Programming**\n",
    "\n",
    "We formulate the SSP problem as a **Linear Program (LP)** to solve for the optimal cost-to-go \\( J^\\star \\). The policy \\( \\pi^\\star \\) is then recovered by selecting the action that minimizes the expected cost at each state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 5: Solve using Linear Programming\n",
    "# =========================================\n",
    "# Solve the stochastic shortest path (SSP) problem using linear programming.\n",
    "# Goal: Minimize the expected number of transitions from a given initial node\n",
    "#       to the terminal (absorbing) state.\n",
    "#\n",
    "# Notes:\n",
    "# - This method reformulates the SSP problem as a linear program.\n",
    "# - The LP minimizes the total expected cost under linear inequality constraints.\n",
    "#\n",
    "# Input:\n",
    "#   - P: Transition probability tensor\n",
    "# Output:\n",
    "#   - J_lp: Optimal cost-to-go from each node\n",
    "#   - F_lp: Optimal control action at each node (post-recovered from J_lp)\n",
    "\n",
    "print(\"Solution Method 3: Linear Programming\")\n",
    "print(\"Solving the stochastic shortest path (SSP) problem using linear programming...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lp_solver = SSPLinearProgram(P)\n",
    "J_lp, F_lp = lp_solver.solve()\n",
    "\n",
    "print(f\"Execution time: {time.time() - start_time:.5f}s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166c72a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 6: Compare Cost-to-Go Estimates from All Methods**\n",
    "\n",
    "We now compare the **optimal cost-to-go functions** $ J^\\star $ obtained using:\n",
    "- Policy Iteration\n",
    "- Value Iteration\n",
    "- Linear Programming\n",
    "\n",
    "For each state $ i \\in \\mathcal{X} \\setminus \\{t\\} $, we plot the expected number of transitions required to reach the terminal state under the respective optimal policy $ \\pi^\\star $.\n",
    "\n",
    "This visualization confirms that all three methods yield consistent optimal costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae922070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 6: Compare Cost-to-Go from All Methods\n",
    "# =========================================\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "states = np.arange(1, n + 1 + 1)  # states 1 to n (MATLAB 1-indexing)\n",
    "\n",
    "plt.plot(states[:-1], J_policy[:n], 'b.', markersize=4, label='Policy Iteration')\n",
    "plt.plot(states[:-1], J_value[:n], 'r.', markersize=4, label='Value Iteration')\n",
    "plt.plot(states[:-1], J_lp[:n], 'ko', markersize=8, markerfacecolor='none', label='Linear Programming')\n",
    "\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Cost-to-Go')\n",
    "plt.title('Cost-to-Go: Value and Policy Iteration, Linear Programming')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c382bc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 7: Visualize Optimal Policy from Policy Iteration**\n",
    "\n",
    "Here we visualize the **optimal control action** $ \\pi^\\star(i) $ at each state $ i \\in \\mathcal{X} \\setminus \\{t\\} $, as computed by the Policy Iteration method.\n",
    "\n",
    "Each bar indicates which control input is selected by the optimal policy $ \\pi^\\star $ at the corresponding state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 7: Plot Optimal Policy from Policy Iteration\n",
    "# =========================================\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(states[:-1], F_policy[:n])\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Optimal Input')\n",
    "plt.title('Optimal Policy from Policy Iteration')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d64495",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Step 8: Simulate SSP and Analyze Performance**\n",
    "\n",
    "To validate the computed policy $ \\pi^\\star $, we simulate the SSP process multiple times starting from a fixed node $ i_0 \\in \\mathcal{X} $.\n",
    "\n",
    "We compute the empirical mean and standard deviation of the total cost (i.e., number of steps until termination), and compare them with the theoretical values obtained from the three solution methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 8: Simulate SSP and Analyze Statistics\n",
    "# =========================================\n",
    "# Simulate the stochastic shortest path (SSP) system using the optimal policy\n",
    "# obtained from Policy Iteration (Method 1). Perform multiple runs from a fixed\n",
    "# starting state and analyze the resulting cost distribution.\n",
    "#\n",
    "# Purpose:\n",
    "# - Validate the policy via Monte Carlo simulations.\n",
    "# - Compare empirical cost statistics with theoretical expectations.\n",
    "#\n",
    "# Inputs:\n",
    "#   - P: Transition probability tensor\n",
    "#   - F_policy: Optimal policy obtained from policy iteration\n",
    "#   - i0: Initial state to start all simulations from (center of grid)\n",
    "#   - SIM_NUM: Number of simulation runs\n",
    "#\n",
    "# Outputs:\n",
    "#   - sim_costs: Vector of total cost in each simulation\n",
    "#   - avgCost: Mean of simulation costs\n",
    "#   - stdCost: Standard deviation of simulation costs\n",
    "\n",
    "# Number of simulations to estimate statistics\n",
    "SIM_NUM = 1000\n",
    "\n",
    "# Start from the center of the wrap-around grid\n",
    "i0 = n // 2\n",
    "\n",
    "print(\"\\nSimulate Problem\")\n",
    "print(f\"Running {SIM_NUM} simulations from node {i0}...\")\n",
    "\n",
    "# Run simulations under policy iteration's optimal policy\n",
    "sim_costs = simulate_ssp(P, i0, SIM_NUM, F_policy)\n",
    "\n",
    "# Compute statistics\n",
    "avgCost = np.mean(sim_costs)\n",
    "stdCost = np.std(sim_costs)\n",
    "\n",
    "# Display comparison between simulation and theoretical expectations\n",
    "print(\"\\nSIMULATION\")\n",
    "print(f\"Starting node {i0}\")\n",
    "print(f\"Simulated average cost           : {avgCost:.4f}\")\n",
    "print(f\"Simulated cost standard deviation: {stdCost:.4f}\")\n",
    "print(f\"Expected minimum cost (policy iteration) : {J_policy[i0]:.4f}\")\n",
    "print(f\"Expected minimum cost (value iteration)  : {J_value[i0]:.4f}\")\n",
    "print(f\"Expected minimum cost (linear programming): {J_lp[i0]:.4f}\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "# Plot simulation results\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, SIM_NUM + 1), sim_costs, 'b.', markersize=2, label='Simulated Cost')\n",
    "\n",
    "# Overlay mean and ±std lines\n",
    "plt.hlines(avgCost, 1, SIM_NUM, colors='k', linestyles='-', label='Mean')\n",
    "plt.hlines([avgCost - stdCost, avgCost + stdCost], 1, SIM_NUM, colors='r', linestyles='--', label='Mean ± Std')\n",
    "\n",
    "# Annotate mean and std\n",
    "annotation_text = f\"mean = {avgCost:.2f}, std dev = {stdCost:.2f}\"\n",
    "plt.text(0.05 * SIM_NUM, max(sim_costs) * 0.9, annotation_text)\n",
    "\n",
    "plt.xlabel('Simulation #')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Simulated Costs')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
