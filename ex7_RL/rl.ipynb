{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 7.1: Model-Based Reinforcement Learning**\n",
    "\n",
    "\n",
    "\n",
    "In this chapter, we introduce reinforcement learning, a powerful and widely used framework for solving constrained optimal control problems in both linear and nonlinear systems.\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 7.1 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 1 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Discretization I</td>\n",
    "    <td>Nearest Neighbor</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Value Iteration</td>\n",
    "    <td>Task 1: Linear Case</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 2: Nonlinear Case</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 3 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Discretization II</td>\n",
    "    <td>Linear Interpolation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Policy Iteration</td>\n",
    "    <td>Task 1: Nonlinear Case</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"1\">Comparison</td>\n",
    "    <td>Model-Based RL vs. NMPC</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "First, we need to set up our Python environment, import relevant packages, and setup some needed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from functools import wraps\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *\n",
    "\n",
    "# import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import casadi as ca\n",
    "\n",
    "\n",
    "def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap'):\n",
    "    '''\n",
    "    Function to offset the \"center\" of a colormap. Useful for\n",
    "    data with a negative min and positive max and you want the\n",
    "    middle of the colormap's dynamic range to be at zero.\n",
    "\n",
    "    From: https://stackoverflow.com/questions/7404116/defining-the-midpoint-of-a-colormap-in-matplotlib\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "      cmap : The matplotlib colormap to be altered\n",
    "      start : Offset from lowest point in the colormap's range.\n",
    "          Defaults to 0.0 (no lower offset). Should be between\n",
    "          0.0 and `midpoint`.\n",
    "      midpoint : The new center of the colormap. Defaults to \n",
    "          0.5 (no shift). Should be between 0.0 and 1.0. In\n",
    "          general, this should be  1 - vmax / (vmax + abs(vmin))\n",
    "          For example if your data range from -15.0 to +5.0 and\n",
    "          you want the center of the colormap at 0.0, `midpoint`\n",
    "          should be set to  1 - 5/(5 + 15)) or 0.75\n",
    "      stop : Offset from highest point in the colormap's range.\n",
    "          Defaults to 1.0 (no upper offset). Should be between\n",
    "          `midpoint` and 1.0.\n",
    "    '''\n",
    "    cdict = {\n",
    "        'red': [],\n",
    "        'green': [],\n",
    "        'blue': [],\n",
    "        'alpha': []\n",
    "    }\n",
    "\n",
    "    # regular index to compute the colors\n",
    "    reg_index = np.linspace(start, stop, 257)\n",
    "\n",
    "    # shifted index to match the data\n",
    "    shift_index = np.hstack([\n",
    "        np.linspace(0.0, midpoint, 128, endpoint=False), \n",
    "        np.linspace(midpoint, 1.0, 129, endpoint=True)\n",
    "    ])\n",
    "\n",
    "    for ri, si in zip(reg_index, shift_index):\n",
    "        r, g, b, a = cmap(ri)\n",
    "\n",
    "        cdict['red'].append((si, r, r))\n",
    "        cdict['green'].append((si, g, g))\n",
    "        cdict['blue'].append((si, b, b))\n",
    "        cdict['alpha'].append((si, a, a))\n",
    "\n",
    "    newcmap = matplotlib.colors.LinearSegmentedColormap(name, cdict)\n",
    "    # plt.register_cmap(cmap=newcmap)\n",
    "    matplotlib.colormaps.register(name=name, cmap=newcmap)\n",
    "\n",
    "    return newcmap\n",
    "\n",
    "\n",
    "orig_cmap = matplotlib.cm.coolwarm\n",
    "midpoint = 1 - 10 / (10 + 1)\n",
    "shifted_cmap = shiftedColorMap(orig_cmap, midpoint=midpoint, name='shifted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Setup:**\n",
    "\n",
    "- Task: starting from given initial position $p_0$, reach a given target position $p_T$ (stabilization)\n",
    "\n",
    "- Slope profile (height $h$ with reference to horizontal displacement $p$):  \n",
    "   - case 1: zero slope (linear case), $h(p) = c$\n",
    "   - case 2: constant slope (linear case), $h(p) = \\frac{\\pi}{18} \\cdot p$\n",
    "   - case 3: varying slope for small disturbances (nonlinear case), $h(p) = k \\cdot \\cos(18 p)$\n",
    "   - case 4: varying slope for under actuated case (nonlinear case), $h(p) = \\begin{cases} k \\cdot \\sin(3 p), & p \\in [- \\frac{\\pi}{2}, \\frac{\\pi}{6}] \\\\ k, & p \\in (-\\infty, -\\frac{\\pi}{2}) \\cup (\\frac{\\pi}{6}, \\infty) \\end{cases}$\n",
    "\n",
    "- System dynamics of 1d mountain car model (in state space representation): \n",
    "   - state vector $\\boldsymbol{x} = [p, v]^T$\n",
    "   - input vector $u$\n",
    "   - system dynamics:\n",
    "   \\begin{align*}\n",
    "     \\begin{bmatrix} \\dot{p} \\\\ \\dot{v} \\end{bmatrix} = \\begin{bmatrix} v \\\\ - g \\sin(\\theta) \\cos(\\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\cos(\\theta)  \\end{bmatrix} u\n",
    "   \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparation: Mountain Car Environment and the System Dynamics**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi, including the definition of the mountain profile as a function of $p$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establishing the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "   - case: 1 (linear case)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "   - input constraints: $ \\mathcal{U} \\in [-4.0, 4.0]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 \n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# State bounds\n",
    "state_lbs = np.array([initial_position-0.8, -3.5])\n",
    "state_ubs = np.array([target_position+0.2, 3.5])\n",
    "\n",
    "# Input bounds\n",
    "input_lbs = -4.0\n",
    "input_ubs = 4.0\n",
    "\n",
    "# Define the control frequency for controller\n",
    "freq = 20\n",
    "dt = 1.0/freq\n",
    "\n",
    "# Constrained case\n",
    "env_constr = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 input_lbs=input_lbs, input_ubs=input_ubs, state_lbs=state_lbs, state_ubs=state_ubs)\n",
    "dynamics_constr = Dynamics(env_constr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization I\n",
    "### **Discretizing State and Input Spaces**\n",
    "\n",
    "A Markov decision process (MDP), is the main assumption in RL. An MDP is the tuple $(S, U, p, r)$, where $S$ is the discrete set of states, $U$ is the discrete set of actions (can be state-dependent), $p$ describes the transition probabilities (conditional probability on the next state $x'$ given a state $x$ and an action $u$: $p(x' \\vert x, u)$), and $r$ represents the reward for each transition with $r(x', x, u)$.  \n",
    "\n",
    "However, many real-world problems (e.g., in robotics) have continuous state and action spaces or a mix of continuous and discrete spaces. In this section, we discuss the discretization of the state and input spaces. Then we will show how we compute the transition probabilities and rewards based on these discrete sets. We highlight two approaches: the nearest neighbor approach and the linear interpolation approach. Both approaches yield a stochastic MDP, that is, typically $p(x' \\vert x, u) \\leq 1$ for any $x'$, $x$, and $u$.\n",
    "\n",
    "The selection of the discretization plays an important role. Typically, a finer discretization will lead to more accurate results that more closely represent the underlying continuous-time system. However, any additional discretization will lead to increased computation time. This computation scales poorly for higher dimensional state and action spaces (referred to as the curse of dimensionality). \n",
    "\n",
    "In this example, we choose a uniform discretization of states and inputs. However, we do make sure that the discrete states and actions exactly include the target state. In practice you may also consider using non-uniform discretizations, where the discretization steps vary. We show the discretization of the state and action spaces next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose discretization\n",
    "num_states = np.array([21, 21])\n",
    "num_actions = 21\n",
    "\n",
    "# function to create a linspace that includes a specific value called target\n",
    "def linspace_include_target(target, lb, ub, num):\n",
    "    assert target >= lb and target <= ub, \"Target is out of bounds\"\n",
    "    # Create the original linspace\n",
    "    linspace_array = np.linspace(lb, ub, num)\n",
    "\n",
    "    # Find the closest index to target_pos\n",
    "    closest_idx = np.argmin(np.abs(linspace_array - target))\n",
    "\n",
    "    # Calculate the difference and shift the entire array\n",
    "    difference = target - linspace_array[closest_idx]\n",
    "    linspace_array += difference\n",
    "\n",
    "    return linspace_array\n",
    "\n",
    "num_pos = num_states[0]\n",
    "num_vel = num_states[1]\n",
    "\n",
    "pos_partitions = linspace_include_target(target_position, state_lbs[0], state_ubs[0], num_pos)  # make sure to include target_position\n",
    "vel_partitions = linspace_include_target(target_velocity, state_lbs[1], state_ubs[1], num_vel)  # make sure to include target_velocity\n",
    "acc_partitions = linspace_include_target(0.0, input_lbs, input_ubs, num_actions)  # make sure to include 0.0\n",
    "\n",
    "plot_discrete_state_space(pos_partitions, vel_partitions, [target_position, target_velocity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reward Function**\n",
    "For any transition we incur a reward. For this example, we use the reward function:\n",
    "$r(x', x, u) = \\begin{cases} 10, & \\text{if }x' \\text{ is target state} \\\\ -1, & \\text{if }x' \\text{ is not target state} \\end{cases}$\n",
    "\n",
    "If the discrete state space is not inlcuded in the target state, we could assign the maximum reward to the discrete state closest to the target state. However, in our case, we have already made sure that they are part of the discrete set by design. \n",
    "\n",
    "While the RL agent assumes a discrete set of states and actions, the underlying system is still continuous and is potentially not even constrained to the same state and action bounds. That's why any resulting states $x' \\notin S$, will be projected back onto the closest state in $S$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kdtree is a data structure that allows for efficient nearest neighbor search in a high-dimensional space.\n",
    "kdtree_pos = cKDTree(pos_partitions.reshape(-1, 1))\n",
    "kdtree_vel = cKDTree(vel_partitions.reshape(-1, 1))\n",
    "kdtree_acc = cKDTree(acc_partitions.reshape(-1, 1))\n",
    "\n",
    "def nearest_state_index_lookup_kdtree(state):\n",
    "    \"\"\"\n",
    "    Find the nearest state index in the discrete state space for a given state.\n",
    "    \"\"\"\n",
    "    nearest_pos_idx = kdtree_pos.query(state[0])[1]\n",
    "    nearest_vel_idx = kdtree_vel.query(state[1])[1]\n",
    "    return nearest_pos_idx + nearest_vel_idx * num_pos\n",
    "\n",
    "target_state_index = nearest_state_index_lookup_kdtree(env_constr.target_state)\n",
    "\n",
    "def one_step_forward(cur_state, cur_input):\n",
    "    # Check whether current state and input is within the state space and input space\n",
    "    cur_pos = max(min(cur_state[0], env_constr.pos_ubs), env_constr.pos_lbs)\n",
    "    cur_vel = max(min(cur_state[1], env_constr.vel_ubs), env_constr.vel_lbs)\n",
    "    cur_state = np.array([cur_pos, cur_vel])\n",
    "    cur_input = max(min(cur_input, env_constr.input_ubs), env_constr.input_lbs)\n",
    "    \n",
    "    # Propagate the state\n",
    "    next_state = dynamics_constr.one_step_forward(cur_state, cur_input, dt)  \n",
    "    next_state_raw = next_state\n",
    "\n",
    "    # Check whether next state is within the state space\n",
    "    next_pos = max(min(next_state[0], env_constr.pos_ubs), env_constr.pos_lbs)\n",
    "    next_vel = max(min(next_state[1], env_constr.vel_ubs), env_constr.vel_lbs)\n",
    "    next_state = np.array([next_pos, next_vel])\n",
    "    \n",
    "    # Get reward\n",
    "    next_state_index = nearest_state_index_lookup_kdtree(next_state)\n",
    "    if next_state_index == target_state_index:\n",
    "        reward = 10\n",
    "    elif np.any(next_state_raw != next_state):\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = -1\n",
    "    \n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transition Probabilities**\n",
    "\n",
    "##### **Nearest Neighbor**\n",
    "\n",
    "In the nearest neighbor approach, the next continuous state $x'$ is mapped to the closest node in the uniform mesh. This yields a deterministic MDP. In practice you may also want to add noise to the system and sample the system multiple times to better represent the continuous action space. We leave playing around with these parameters as an exercise to you. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "state_noise_levels = np.array([0.0, 0.0])\n",
    "input_noise_levels = np.array([0.0])\n",
    "\n",
    "pos, vel = np.meshgrid(pos_partitions, vel_partitions)\n",
    "state_space = np.vstack((pos.ravel(), vel.ravel()))  # Shape: (2, num_states)\n",
    "num_total_states = state_space.shape[1]\n",
    "\n",
    "# Define action space (acceleration)\n",
    "input_space = acc_partitions  # Shape: (1, num_actions)\n",
    "num_actions = len(input_space)\n",
    "\n",
    "def nearest_neighbor_approach(cur_state, action):\n",
    "    probs = []\n",
    "    nodes = []\n",
    "    rewards = []\n",
    "    for i in range(num_samples):\n",
    "        # Sample noise\n",
    "        state_noise = np.random.normal(0, 1.0, size=cur_state.shape) * state_noise_levels\n",
    "        input_noise = np.random.normal(0, 1.0, size=1) * input_noise_levels\n",
    "\n",
    "        # Propagate forward to get next state and reward\n",
    "        next_state, reward = one_step_forward(cur_state + state_noise, action + input_noise)\n",
    "\n",
    "        probs.append(1.0 / num_samples)\n",
    "        nodes.append(next_state)\n",
    "        rewards.append(reward / num_samples)\n",
    "\n",
    "    return probs, nodes, rewards\n",
    "\n",
    "def build_stochastic_mdp(approach, verbose=False):\n",
    "    \"\"\"\n",
    "    Construct transition probability (T) and reward (R) matrices for the MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize T and R matrices\n",
    "    T = np.zeros((num_actions, num_total_states, num_total_states))\n",
    "    R = np.zeros((num_actions, num_total_states, num_total_states))\n",
    "\n",
    "    # Iterate over all states\n",
    "    for state_index in range(num_total_states):\n",
    "        cur_state = state_space[:, state_index]\n",
    "        print(f\"Building model... state {state_index + 1}/{num_states}\")\n",
    "\n",
    "        # Apply each possible action\n",
    "        for action_index in range(num_actions):\n",
    "            action = input_space[action_index]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"cur_state: {cur_state}\")\n",
    "                print(f\"action: {action}\")\n",
    "\n",
    "            probs, nodes, rewards = approach(cur_state, action)\n",
    "\n",
    "            # Update transition and reward matrices\n",
    "            for i, node in enumerate(nodes):\n",
    "                node_index = nearest_state_index_lookup_kdtree(node)\n",
    "                \n",
    "                T[action_index][state_index, node_index] += probs[i]\n",
    "                R[action_index][state_index, node_index] += rewards[i]\n",
    "\n",
    "    return T, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the transition probabilities and reward function for our environment and task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_nn, R_nn = build_stochastic_mdp(nearest_neighbor_approach, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the resulting transition probabilities and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_id = int(num_actions / 2.0)\n",
    "state_id = target_state_index\n",
    "\n",
    "def visualize_T_and_R(state_id, action_id, T, R):\n",
    "    pos = pos_partitions[((state_id) % num_pos)]\n",
    "    vel = vel_partitions[((state_id) // num_pos)]\n",
    "    acc = acc_partitions[action_id]\n",
    "\n",
    "    T_next = T[action_id, state_id]\n",
    "    R_next = R[action_id, state_id]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    pos_grid, vel_grid = np.meshgrid(pos_partitions, vel_partitions)\n",
    "\n",
    "    # Plot transition probabilities\n",
    "    T_plot = ax1.scatter(pos_grid.flatten(), vel_grid.flatten(), c=T_next, cmap='viridis', vmin=0, vmax=1)\n",
    "    ax1.set_xlabel('Position')\n",
    "    ax1.set_ylabel('Velocity') \n",
    "    ax1.set_title(f'Transition Probabilities, state ({pos}, {vel}), action {acc}')\n",
    "    plt.colorbar(T_plot, ax=ax1)\n",
    "\n",
    "    # Plot rewards\n",
    "    R_plot = ax2.scatter(pos_grid.flatten(), vel_grid.flatten(), c=R_next, cmap='shifted', vmin=-1, vmax=10)\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Velocity')\n",
    "    ax2.set_title(f'Rewards, state ({pos}, {vel}), action {acc}')\n",
    "    plt.colorbar(R_plot, ax=ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_T_and_R(target_state_index, action_id, T_nn, R_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the arrays that hold the transition probabilities and the rewards are high dimensional, you are encouraged to try different values for the `state_id` and the `action_id` (although the deterministic case won't provide too many surprises). \n",
    "\n",
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Stochastic MDP from Nearest Neighbor**\n",
    "\n",
    "Experiment with the amount of noise added to the state and control inputs to record the state transitions and vary the number of sampled points. this will lead to a stochastic MDP instead of a deterministic one. What do you observe? Can the added noise make up for the coarse discretization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "After setting up the MDP, the common interface to any RL algorithm, we now apply the standard RL algorithms, value and policy iteration to this MDP.  We begin with the value iteration algorithm. The implementation directly follows the lecture notes. To prevent infinite loops, we limit the maximum number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_eps = 1e-6\n",
    "max_vi_ite = 500\n",
    "\n",
    "@timing\n",
    "def value_iteration(T, R, gamma=1.0, max_vi_ite=500):\n",
    "    v_collection = []\n",
    "    policy_collection = []\n",
    "\n",
    "    v = np.zeros((num_total_states))\n",
    "    policy = np.zeros((num_total_states), np.int32)\n",
    "    \n",
    "    current_eps = vi_eps + 1.0  # make sure the first value is larger so at least one iteration is executed. \n",
    "\n",
    "    vi_iteration = 0\n",
    "\n",
    "    while current_eps >= vi_eps and vi_iteration < max_vi_ite:\n",
    "        v_new = np.zeros((num_total_states))\n",
    "\n",
    "        current_eps = 0.0\n",
    "\n",
    "        for state_index in range(num_total_states):\n",
    "            v_actions = np.zeros((num_actions))\n",
    "            for action_index in range(num_actions):\n",
    "                for next_state_index in range(num_total_states):\n",
    "                    v_actions[action_index] += T[action_index, state_index, next_state_index] * (R[action_index, state_index, next_state_index] + gamma * v[next_state_index])\n",
    "            best_action_index = np.argmax(v_actions)\n",
    "            v_new[state_index] = v_actions[best_action_index]\n",
    "            policy[state_index] = best_action_index\n",
    "\n",
    "            current_eps = max(current_eps, np.abs(v_new[state_index] - v[state_index]))\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "        print(f\"Iteration {vi_iteration}: Current eps: {current_eps}\")\n",
    "        vi_iteration += 1\n",
    "        v_collection.append(v)\n",
    "        policy_collection.append(policy.copy())\n",
    "\n",
    "    return v, policy, v_collection, policy_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run value iteration, we will fix the discount factor $\\gamma$ for the remainder of the notebook. We encourage you to play around with different values for $\\gamma$. What effects can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Linear Case\n",
    "Now everything is set up for running the value iteration algorithm. For fine discretizations this can take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_nn_v, vi_nn_policy, vi_nn_v_collection, vi_nn_policy_collection = value_iteration(T_nn, R_nn, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration algorithm also provides an associated policy. We will use this policy as a controller for our mountain car. This policy is a look-up-table. Based on our current state, we pick the action for the discrete state that is closest to the continuous state. This is implemented in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLController(BaseController):\n",
    "    def __init__(self, \n",
    "                 env: Env, \n",
    "                 dynamics: Dynamics, \n",
    "                 freq: float, \n",
    "                 policy: np.ndarray,\n",
    "                 name: str = 'RL', \n",
    "                 type: str = 'RL', \n",
    "                 verbose: bool = True\n",
    "                 ) -> None:\n",
    "        \n",
    "        super().__init__(env, dynamics, freq, name, type, verbose)\n",
    "\n",
    "        self.policy = policy\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        pass\n",
    "\n",
    "    @check_input_constraints\n",
    "    def compute_action(self, current_state: np.ndarray, current_time) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Use the optimal policy to compute the action for the given state.\n",
    "        \"\"\"\n",
    "        # Find the nearest discrete state index\n",
    "        state_index = nearest_state_index_lookup_kdtree(current_state)\n",
    "        \n",
    "        # Get the optimal action from the policy\n",
    "        action_index = self.policy[state_index]\n",
    "        action = input_space[action_index]\n",
    "\n",
    "        return np.array([action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run our policy, we are only left to set the time for the simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_terminal = 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we execute the resulting policy from value iteration to our system: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_nn_controller = RLController(env_constr, dynamics_constr, 1.0/dt, vi_nn_policy)\n",
    "\n",
    "simulator_vi_nn = Simulator(dynamics_constr, vi_nn_controller, env_constr, dt, t_terminal)\n",
    "simulator_vi_nn.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_vi_nn = Visualizer(simulator_vi_nn)\n",
    "visualizer_vi_nn.display_plots()\n",
    "visualizer_vi_nn.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the resulting policy successfully achieves the task and stabilizes the system around the desired state. There are some oscialltions at the end since the policy only has access to a limited number of discrete actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Discount Factor $\\gamma$**\n",
    "\n",
    "To gain a better understanding of the discount factor, try running the cells for smaller and larger values of $\\gamma$. Recall that $\\gamma \\in \\left(0, 1\\right)$. What do you observe? What is the impact of the discount factor? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Nonlinear Case\n",
    "The above task was relatively simple and the linear methods, like LQR, were already able to solve such tasks. Therefore, the question becomes: How does this transfer to more challenging tasks? For the rest of the notebook, we are considering the case of the valley environment. We set up the new environment (using the previous constraints and target state). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 4\n",
    "\n",
    "# Constrained case\n",
    "env_constr = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 input_lbs=input_lbs, input_ubs=input_ubs, state_lbs=state_lbs, state_ubs=state_ubs)\n",
    "dynamics_constr = Dynamics(env_constr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we set up the MDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_nn, R_nn = build_stochastic_mdp(nearest_neighbor_approach, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_id = int(num_actions / 2.0)\n",
    "state_id = target_state_index\n",
    "\n",
    "visualize_T_and_R(target_state_index, action_id, T_nn, R_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run value iteration for this new task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_nn_v, vi_nn_policy, vi_nn_v_collection, vi_nn_policy_collection = value_iteration(T_nn, R_nn, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_nn_controller = RLController(env_constr, dynamics_constr, 1.0/dt, vi_nn_policy)\n",
    "\n",
    "simulator_vi_nn = Simulator(dynamics_constr, vi_nn_controller, env_constr, dt, t_terminal)\n",
    "simulator_vi_nn.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_vi_nn = Visualizer(simulator_vi_nn)\n",
    "visualizer_vi_nn.display_plots()\n",
    "visualizer_vi_nn.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting policy fails to successfully stabilize the system at the top. As you will see in the next part, this is not a limitation of the RL method but rather a limitation of the chosen discretization approach. You may try to increase the number of discrete states, but this will also result in a larger computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization II\n",
    "\n",
    "### Linear Interpolation\n",
    "\n",
    "Since the nearest neighbor approach did not stabilize the system on top of the hill, we turn to a different scheme to generate an MDP from a system with continuous dynamics, state, and inputs. \n",
    "\n",
    "In this section, we introduce the linear interpolation approach. This approach assigns probabilities to adjacent nodes based on the distance of the true continuous state to its adjacent nodes. Therefore, the linear interpolation approach is inherenetly stochastic and accounts for the possibility of landing in different states. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_approach(cur_state, action):\n",
    "    # Propagate forward to get next state and reward\n",
    "    next_state, reward = one_step_forward(cur_state, action)\n",
    "\n",
    "    pos_indices = kdtree_pos.query(next_state[0], k=2)[1]\n",
    "    vel_indices = kdtree_vel.query(next_state[1], k=2)[1]\n",
    "\n",
    "    pos_bounds = [pos_partitions[pos_indices[0]], pos_partitions[pos_indices[1]]]\n",
    "    vel_bounds = [vel_partitions[vel_indices[0]], vel_partitions[vel_indices[1]]]\n",
    "\n",
    "    # Normalize next state within bounds\n",
    "    x_norm = (next_state[0] - min(pos_bounds)) / (max(pos_bounds) - min(pos_bounds))\n",
    "    y_norm = (next_state[1] - min(vel_bounds)) / (max(vel_bounds) - min(vel_bounds))\n",
    "\n",
    "    # Calculate bilinear interpolation probabilities\n",
    "    probs = [\n",
    "        (1 - x_norm) * (1 - y_norm),  # bottom-left\n",
    "        x_norm * (1 - y_norm),        # bottom-right\n",
    "        x_norm * y_norm,              # top-right\n",
    "        (1 - x_norm) * y_norm         # top-left\n",
    "    ]\n",
    "\n",
    "    # Four vertices of the enclosing box\n",
    "    nodes = [\n",
    "        [min(pos_bounds), min(vel_bounds)],  # bottom-left\n",
    "        [max(pos_bounds), min(vel_bounds)],  # bottom-right\n",
    "        [max(pos_bounds), max(vel_bounds)],  # top-right\n",
    "        [min(pos_bounds), max(vel_bounds)]   # top-left\n",
    "    ]\n",
    "\n",
    "    rewards = [reward] * len(nodes)\n",
    "\n",
    "    return probs, nodes, rewards\n",
    "\n",
    "T_interp, R_interp = build_stochastic_mdp(linear_interpolation_approach, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we visualize the resulting transition probabilities and rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_id = action_id = int(num_actions / 2.0)\n",
    "state_id = target_state_index\n",
    "\n",
    "visualize_T_and_R(target_state_index, action_id, T_interp, R_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Discretization Approaches**\n",
    "\n",
    "Compare the resulting slices for the nearest neighbor approach and the linear intepolation approach. What do you see? How do they differ? When would you choose which approach? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run value iteration for the newly created MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_interp_v, vi_interp_policy, vi_interp_v_collection, vi_interp_policy_collection = value_iteration(T_interp, R_interp, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_interp_controller = RLController(env_constr, dynamics_constr, 1.0/dt, vi_interp_policy)\n",
    "\n",
    "simulator_vi_interp = Simulator(dynamics_constr, vi_interp_controller, env_constr, dt, t_terminal)\n",
    "simulator_vi_interp.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_vi_interp = Visualizer(simulator_vi_interp)\n",
    "visualizer_vi_interp.display_plots()\n",
    "visualizer_vi_interp.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, value iteration is able to stabilize the system at the top of the hill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "The Bellman equation can be used to derive different algorithms to find the optimal value function and policy. Another algorithm is the policy iteration algorithm. Following the lecture notes, the algorithm is implmented in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_eps = 1e-6\n",
    "\n",
    "@timing\n",
    "def policy_iteration( T, R, gamma=1.0, max_pe_ite=100, max_pi_ite=100):\n",
    "    v_collection = []\n",
    "    policy_collection = []\n",
    "\n",
    "    v = np.zeros((num_total_states))\n",
    "    median_action = int(num_actions / 2.0)\n",
    "    policy = median_action * np.ones((num_total_states), np.int32)  # initialize the policy with the zero action for each state\n",
    "\n",
    "    policy_converged = False\n",
    "\n",
    "    pi_iteration = 0\n",
    "\n",
    "    while not policy_converged and pi_iteration < max_pi_ite:\n",
    "\n",
    "        v = policy_evaluation(v, policy, T, R, gamma, max_pe_ite)\n",
    "        policy_converged, policy = policy_improvement(v, policy, T, R, gamma)\n",
    "\n",
    "        pi_iteration += 1\n",
    "        v_collection.append(v)  \n",
    "        print(f\"Policy improvement {pi_iteration}\")\n",
    "        policy_collection.append(policy.copy())\n",
    "\n",
    "    return v, policy, v_collection, policy_collection\n",
    "\n",
    "def policy_evaluation(v, policy, T, R, gamma=1.0, max_pe_ite=100):\n",
    "    current_eps = pe_eps + 1.0\n",
    "    pe_iteration = 0\n",
    "\n",
    "    while current_eps >= pe_eps and pe_iteration < max_pe_ite:\n",
    "        v_new = np.zeros((num_total_states))\n",
    "\n",
    "        current_eps = 0.0\n",
    "\n",
    "        for state_index in range(num_total_states):\n",
    "            action_index = policy[state_index]\n",
    "            for next_state_index in range(num_total_states):\n",
    "                v_new[state_index] += T[action_index, state_index, next_state_index] * (R[action_index, state_index, next_state_index] + gamma * v[next_state_index])\n",
    "\n",
    "            current_eps = max(current_eps, np.abs(v_new[state_index] - v[state_index]))\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "        print(f\"Policy evaluation {pe_iteration}: Current eps: {current_eps}\")\n",
    "        pe_iteration += 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def policy_improvement(v, policy, T, R, gamma=1.0):\n",
    "    policy_converged = True\n",
    "\n",
    "    for state_index in range(num_total_states):\n",
    "        u_old = policy[state_index]\n",
    "\n",
    "        v_actions = np.zeros((num_actions))\n",
    "        for action_index in range(num_actions):\n",
    "            for next_state_index in range(num_total_states):\n",
    "                v_actions[action_index] += T[action_index, state_index, next_state_index] * (R[action_index, state_index, next_state_index] + gamma * v[next_state_index])\n",
    "        best_action_index = np.argmax(v_actions)\n",
    "        policy[state_index] = best_action_index\n",
    "\n",
    "        if not policy[state_index] == u_old:\n",
    "            policy_converged = False \n",
    "\n",
    "    return policy_converged, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Nonlinear Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We directly use this algorithm for the new MDP to find the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_interp_v, pi_interp_policy, pi_interp_v_collection, pi_interp_policy_collection = policy_iteration(T_interp, R_interp, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_interp_controller = RLController(env_constr, dynamics_constr, 1.0/dt, pi_interp_policy)\n",
    "\n",
    "simulator_pi_interp = Simulator(dynamics_constr, pi_interp_controller, env_constr, dt, t_terminal)\n",
    "simulator_pi_interp.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_pi_interp = Visualizer(simulator_pi_interp)\n",
    "visualizer_pi_interp.display_plots()\n",
    "visualizer_pi_interp.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting policy from policy iteration also stabilizes the system to the top of the hill. Here, both algorithms yield the same policy. This is not generally the case for both algorithms. Furthermore, policy iteration took only a quarter of the time compared to value iteration but results in the same optimal policy here. It is not always guaranteed that policy iteration will take less time to run. Rather it depends on which step (policy evaluation or policy improvement) takes more time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To highlight the differences between the two iterative algorithms, we visualize the optimal policy and optimal cost over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_value_policy_iterations(v_collection, policy_collection, pos_partitions, vel_partitions, animation_time=10.0, title='Value and Policy Iterations'):\n",
    "    \"\"\"\n",
    "    Animate the optimal policy and optimal cost over time as resulting from value and policy iteration.\n",
    "    Ensures consistent axes across all frames and subplots.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Create colorbars only once, outside the update function\n",
    "    policy = policy_collection[0].reshape(num_pos, num_vel)\n",
    "    v = v_collection[0].reshape(num_pos, num_vel)\n",
    "    \n",
    "    # Initial plots to create colorbars\n",
    "    im1 = axs[0].imshow(policy, extent=[\n",
    "        pos_partitions[0], pos_partitions[-1],\n",
    "        vel_partitions[0], vel_partitions[-1]\n",
    "    ], origin='lower', aspect='auto', cmap='viridis')\n",
    "    cbar1 = fig.colorbar(im1, ax=axs[0], orientation='vertical')\n",
    "    \n",
    "    im2 = axs[1].imshow(v, extent=[\n",
    "        pos_partitions[0], pos_partitions[-1],\n",
    "        vel_partitions[0], vel_partitions[-1]\n",
    "    ], origin='lower', aspect='auto', cmap='viridis')\n",
    "    cbar2 = fig.colorbar(im2, ax=axs[1], orientation='vertical')\n",
    "\n",
    "    # --- Animation frame update ---\n",
    "    def update(frame_idx):\n",
    "\n",
    "        v = v_collection[frame_idx]\n",
    "        policy = policy_collection[frame_idx]\n",
    "\n",
    "        axs[0].clear()\n",
    "        axs[1].clear()\n",
    "\n",
    "        fig.suptitle(f\"{title}\\nIteration = {frame_idx}\", fontsize=14)\n",
    "\n",
    "        # reshape policy and value function to 2D arrays\n",
    "        policy = policy.reshape(num_pos, num_vel)\n",
    "        v = v.reshape(num_pos, num_vel)\n",
    "\n",
    "        # Plot policy (U)\n",
    "        im1 = axs[0].imshow(policy, extent=[\n",
    "            pos_partitions[0], pos_partitions[-1],\n",
    "            vel_partitions[0], vel_partitions[-1]\n",
    "        ], origin='lower', aspect='auto', cmap='viridis')\n",
    "        axs[0].set_title('Optimal Policy')\n",
    "        axs[0].set_xlabel('Car Position')\n",
    "        axs[0].set_ylabel('Car Velocity')\n",
    "\n",
    "        # Plot cost-to-go (J)\n",
    "        im2 = axs[1].imshow(v, extent=[\n",
    "            pos_partitions[0], pos_partitions[-1],\n",
    "            vel_partitions[0], vel_partitions[-1]\n",
    "        ], origin='lower', aspect='auto', cmap='viridis')\n",
    "        axs[1].set_title('Optimal Cost')\n",
    "        axs[1].set_xlabel('Car Position')\n",
    "        axs[1].set_ylabel('Car Velocity')\n",
    "    \n",
    "    num_frames = len(v_collection)\n",
    "    interval_time = int(animation_time / num_frames * 1000.0)  # in milliseconds\n",
    "    anim = FuncAnimation(fig, update, frames=num_frames, interval=interval_time, repeat=False)\n",
    "    plt.close(fig)\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_value_policy_iterations(vi_interp_v_collection, vi_interp_policy_collection, pos_partitions, vel_partitions, title='Value Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_value_policy_iterations(pi_interp_v_collection, pi_interp_policy_collection, pos_partitions, vel_partitions, title='Policy Iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the approaches take different numbers of iterations to converge and that the pattern of convergence is also different among the two approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Value and Policy Iteration Variants**\n",
    "\n",
    "1. The utils.py include an implementation of generalized policy iteration (GPI) as introduced in the lecture notes. Can you make GPI behave like value iteration of policy iteration for the nonlinear case? \n",
    "\n",
    "2. Implement the Gauss-Seidel approach. What do you observe when you use this modification when calculating the policy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "#### Model-Based RL vs. Nonlinear MPC\n",
    "\n",
    "Let's compare the resulting RL policy with the nonlinear MPC we saw in chapter 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "Q_weight = np.diag([1.0, 1.0])\n",
    "R_weight = np.array([[0.1]])\n",
    "Qf = Q_weight\n",
    "\n",
    "N = 60\n",
    "\n",
    "# Instantiate the MPC controller class (controller name must be different from the rest controllers)\n",
    "controller_mpc_N60 = MPCController(env_constr, dynamics_constr, Q_weight, R_weight, Qf, freq, N, name=\"MPC_N60\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_N60 = Simulator(dynamics_constr, controller_mpc_N60, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc_N60.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_N60 = Visualizer(simulator_mpc_N60)\n",
    "visualizer_mpc_N60.display_plots(title='N = 60')\n",
    "visualizer_mpc_N60.display_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show comparison between all cases\n",
    "visualizer_vi_interp.display_contrast_plots(simulator_mpc_N60)\n",
    "visualizer_vi_interp.display_contrast_animation_same(simulator_mpc_N60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model-based RL is able to stabilize the system to the top of the hill, the MPC is not able to do so. In comparison to chapter 5, we have set up the environment with tighter input constraints. For these input constraints, the MPC is no longer able to bring the mountain car to the top of the hill. The advantage of the model-based RL is to determine a policy over the state space offline hile the MPC only determines an input for the current state online. Therefore, RL requires significant computation offline but its execution online is fast. \n",
    "\n",
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: Model Mismatch**\n",
    "\n",
    "Consider the case where the MDP does not precisely model the environment. What happens if you use an MDP designed for the linear case but apply it to the system with the nonlinear slope? Is the RL policy still able to achieve the stabilization goal? "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
