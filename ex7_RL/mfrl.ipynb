{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf6157a",
   "metadata": {},
   "source": [
    "### **Chapter 7.2: Model-Free Reinforcement Learning**\n",
    "\n",
    "\n",
    "\n",
    "In this chapter, we introduce reinforcement learning, a powerful and widely used framework for solving constrained optimal control problems in both linear and nonlinear systems.\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 7.2 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 1 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Sampling</td>\n",
    "    <td>Sampling from a stochastic Policy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Monte-Carlo Method</td>\n",
    "    <td>Implementation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Training Curve and Key Metrics</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simulation on a Hilly Terrain</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Exploration vs. Exploitation</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 3 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">Q-Learning</td>\n",
    "    <td>Implementation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simulation on a Flat Terrain</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simulation on a Hilly Terrain</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Comparison I</td>\n",
    "    <td>Fair Comparison & Statistical Evaluation in RL</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Monte-Carlo Method vs. Q-Learning</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"1\">Comparison II</td>\n",
    "    <td>Comparison of All Control and Learning Paradigms</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "First, we need to set up our Python environment and import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27d4cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Mountain Car Problem Setup:**\n",
    "\n",
    "- Task: starting from given initial position $p_0$, reach a given target position $p_T$ (stabilization)\n",
    "\n",
    "- Slope profile (height $h$ with reference to horizontal displacement $p$):  \n",
    "   - case 1: zero slope (linear case), $h(p) = c$\n",
    "   - case 2: constant slope (linear case), $h(p) = \\frac{\\pi}{18} \\cdot p$\n",
    "   - case 3: varying slope for small disturbances (nonlinear case), $h(p) = k \\cdot \\cos(18 p)$\n",
    "   - case 4: varying slope for under actuated case (nonlinear case), $h(p) = \\begin{cases} k \\cdot \\sin(3 p), & p \\in [- \\frac{\\pi}{2}, \\frac{\\pi}{6}] \\\\ k, & p \\in (-\\infty, -\\frac{\\pi}{2}) \\cup (\\frac{\\pi}{6}, \\infty) \\end{cases}$\n",
    "\n",
    "- System dynamics of 1d mountain car model (in state space representation): \n",
    "   - state vector $\\boldsymbol{x} = [p, v]^T$\n",
    "   - input vector $u$\n",
    "   - system dynamics:\n",
    "   \\begin{align*}\n",
    "     \\begin{bmatrix} \\dot{p} \\\\ \\dot{v} \\end{bmatrix} = \\begin{bmatrix} v \\\\ - g \\sin(\\theta) \\cos(\\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\cos(\\theta)  \\end{bmatrix} u\n",
    "   \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2e92d",
   "metadata": {},
   "source": [
    "### **Preparation: specify some common task parameters**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi, including the definition of the mountain profile as a function of $p$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establishing the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "   - case: 1 (flat terrain) / 4 (hilly terrain)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "   - state constraints (discretization space in RL): $ \\mathcal{X}_1 = [-1.7, 1.3]$,  $ \\mathcal{X}_2 = [-3.5, 3.5]$\n",
    "\n",
    "   - input constraints (discretization space in RL): $ \\mathcal{U} = [-5.0, 5.0]$\n",
    "\n",
    "   - state / input space discretization: 31 units for $x_1$, 21 units for $x_2$, 21 units for $u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial / target state\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# State bounds\n",
    "state_lbs = np.array([initial_position-1.2, -3.5])\n",
    "state_ubs = np.array([target_position+0.7, 3.5])\n",
    "\n",
    "# Input bounds\n",
    "input_lbs = -5.0\n",
    "input_ubs = 5.0\n",
    "\n",
    "# Define time length for simulation\n",
    "t_terminal = 8.0\n",
    "\n",
    "# Define the control frequency for controller\n",
    "freq = 10\n",
    "dt = 1.0/freq\n",
    "\n",
    "# Define the number of states and actions\n",
    "num_states = np.array([31, 21])\n",
    "num_actions = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fb9d5",
   "metadata": {},
   "source": [
    "Meanwhile, at the end of this chapter, we will compare the performance of Model-Free RL with traditional model-based control methods, such as MPC. Therefore, we can predefine a few MPC controller configurations here in advance for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMPC parameters (as reference)\n",
    "# To find a feasible solution, the MPC requires a bit difference discretization and larger bounds\n",
    "freq_mpc = 20\n",
    "dt_mpc = 1.0/freq\n",
    "state_lbs_mpc = np.array([-2.0, -4.0])\n",
    "state_ubs_mpc = np.array([2.0, 4.0])\n",
    "\n",
    "# Define weight matrix in stage and terminal cost and the horizon for MPC (reference controller)\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "N = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff1225",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Sampling from a stochastic Policy: A Discrete CDF Approach**\n",
    "\n",
    "In reinforcement learning, an agent often follows a stochastic policy rather than a fully greedy one, i.e. a probability-mass function $\\pi(u|x)$ that assigns a weight to every action in the current state. An $\\varepsilon$-soft policy is one of the simplest case: with probability $1-\\varepsilon$ the agent takes the greedy action and with $\\varepsilon$ it explores the rest according to that distribution. Sampling from such a policy is crucial, without it the agent can‚Äôt keep a balance between exploration and exploitation, expecially in a on-policy RL method.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Define a $\\varepsilon$-soft policy based on given $Q$-values as testbed, compute cumulative distribution function (CDF) based on probabilistic distribution function (PDF),\n",
    "\n",
    "- Introduce how to sample an action from a discrete probability distribution using its CDF and implement the method manually from scratch,\n",
    "\n",
    "- Compare it with `np.random.choice`, which is the convenience routine provided by Python‚Äôs NumPy library that performs the same draw in a single call.\n",
    "\n",
    "*Note: In this section we only consider the distribution of a discrete random variable. As continuous distributions require different machinery, they are not covered here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e84bac",
   "metadata": {},
   "source": [
    "For a $\\varepsilon$-soft policy defined over $x \\in \\mathcal{X}$ and $u \\in \\mathcal{U}$, it can be formulated as:\n",
    "\n",
    "$$\n",
    "p_{\\pi}(u \\mid x) =\n",
    "\\begin{cases}\n",
    "\\dfrac{\\varepsilon}{\\lvert \\mathcal{U} \\rvert}, \n",
    "    & \\text{for an action associated with non-optimal } Q, \\\\[6pt]\n",
    "1-\\varepsilon \\Bigl(1-\\dfrac{1}{\\lvert \\mathcal{U} \\rvert}\\Bigr), \n",
    "    & \\text{for the action associated with optimal } Q.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Q-values for 4 actions\n",
    "action_index = np.array([0, 1, 2, 3])\n",
    "Q_table = np.array([5.0, 2.0, 1.0, 2.5]) # assume the Q-value is only a function of actions\n",
    "\n",
    "# Define a stochastic policy (Œµ-soft as example)\n",
    "epsilon = 0.2\n",
    "num_u = len(Q_table)\n",
    "action_probs = np.ones(num_u) * (epsilon / num_u)\n",
    "best_action = np.argmax(Q_table)\n",
    "action_probs[best_action] += 1 - epsilon\n",
    "\n",
    "print(\"Œµ-soft action probabilities:\", action_probs)\n",
    "\n",
    "# Convert PDF to CDF\n",
    "cdf = np.cumsum(action_probs)\n",
    "\n",
    "# Plotting the PDF and CDF\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "x = np.arange(num_u)\n",
    "ax1.set_xticks(x)\n",
    "ax1.bar(x, action_probs, color='C0', width=0.4, label='PDF', align='center')\n",
    "ax1.set_ylabel(\"Probability\", color='C0')\n",
    "ax1.set_xlabel(\"Action index\")\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.tick_params(axis='y', labelcolor='C0')\n",
    "ax1.set_title(\"PDF and CDF of Œµ-soft Policy\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.step(x, cdf, where='post', color='C3', label='CDF', linewidth=2)\n",
    "ax2.scatter(x, cdf, color='C3', marker='o')\n",
    "ax2.set_ylabel(\"Cumulative Probability\", color='C3')\n",
    "ax2.tick_params(axis='y', labelcolor='C3')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c1efd",
   "metadata": {},
   "source": [
    "\n",
    "At each step in a training episode, you need to sample an action from the $\\varepsilon$-soft policy that is represented as a probability distribution. Denote $\\hat{f}_x(x)$ as a generic PDF over a discrete random variable $x \\in \\mathcal{X}$ and $\\hat{F}_x(x)=\\sum_{\\tilde{x}=-\\infty}^{x}\\hat{f}(\\tilde{x})$ as the associated CDF. One approach to sample from the distribution $\\hat{f}_x(x)$ is to:\n",
    "\n",
    "- Generate a random number $u_s$ from a uniform distribution $f_u(u)$ supported on the closed interval $[0,1]$, \n",
    "\n",
    "- Take the smallest value that satisfies $\\hat{F}_x(x_s-1) < u_s \\le \\hat{F}_x(x_s)$ as the sample $x_s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_cdf(cdf):\n",
    "    \"\"\"\n",
    "    Sample an index from a discrete probability distribution using its cumulative distribution function (CDF).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cdf : array-like of shape (n,)\n",
    "        The cumulative distribution function values of the discrete distribution, which must be non-decreasing and end at 1.0.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    index : int\n",
    "        The sampled index based on the CDF.\n",
    "    \"\"\"\n",
    "\n",
    "    u = np.random.rand()  # uniform sample from [0,1)\n",
    "    \n",
    "    for i in range(len(cdf)):\n",
    "        if u < cdf[i]:\n",
    "            return i\n",
    "        \n",
    "    return len(cdf) - 1  # fallback in case of round-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1140c",
   "metadata": {},
   "source": [
    "Building on the `sample_from_cdf` implementation above, here we will draw 10 000 actions from the same CDF to test the sampler. By listing the empirical probabilities next to the theoretical ones for each action, we can immediately see whether the hand-crafted sampler reproduces the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the manual sampling method to sample from CDF for 10000 times, and compare the empirical distribution with the theoritical distribution\n",
    "samples = [sample_from_cdf(cdf) for _ in range(10000)]\n",
    "\n",
    "# Statistics of the samples\n",
    "counts = np.bincount(samples, minlength=num_u)\n",
    "empirical_probs = counts / len(samples)\n",
    "\n",
    "# Print each action's probabilities\n",
    "print(f\"{'Action':>6} | {'Theoretical':>12} | {'Empirical':>10}\")\n",
    "print(\"-\" * 36)\n",
    "for i in range(num_u):\n",
    "    print(f\"{i:>6} | {action_probs[i]:>12.4f} | {empirical_probs[i]:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e18cf7",
   "metadata": {},
   "source": [
    "> <br>\n",
    "> üìå <b>Note:</b> Try varying the sampling size! You can replace `10000` with `10`, `100`, `500`, etc., and observe how the empirical probabilities change.\n",
    ">\n",
    "> - A **smaller** number of samples may result in larger **deviations** from the theoretical distribution due to randomness.\n",
    "> \n",
    "> - A **larger** sample size will generally lead to a **closer match** with the theoretical probabilities.\n",
    ">\n",
    "> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb6332",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Alternaltively, NumPy provides a built-in one-liner for drawing samples from a discrete probability distribution‚Äîessentially replacing the hand-rolled CDF routine:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <code>numpy.random.choice(a,&nbsp;size=None,&nbsp;replace=True,&nbsp;p=None)</code>\n",
    "</p>\n",
    "\n",
    "Arguments:\n",
    " - `a`: if an integer N, samples are drawn from the set {0,‚Ä¶,N-1}; if an array, it is treated as the list of labels to sample.\n",
    "\n",
    " - `size`: number (or shape) of samples to return; defaults to a single draw.\n",
    "\n",
    " - `replace`:sample with replacement (True, default) or without (False).\n",
    "\n",
    " - `p`: 1-D array of probabilities that must sum to 1; if omitted, sampling is uniform.\n",
    "\n",
    "Return value:\n",
    "\n",
    " - An ndarray whose shape is given by `size`, containing the drawn labels (dtype matches `a`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In python, we can use numpy's random choice to sample from the action probabilities\n",
    "samples_np = np.random.choice(num_u, size=10000, p=action_probs)\n",
    "\n",
    "# Statistics of the samples\n",
    "counts_np = np.bincount(samples_np, minlength=num_u)\n",
    "empirical_probs_np = counts_np / len(samples_np)\n",
    "\n",
    "# Print each action's probabilities\n",
    "print(f\"{'Action':>6} | {'Theoretical':>12} | {'Empirical':>10}\")\n",
    "print(\"-\" * 36)\n",
    "for i in range(num_u):\n",
    "    print(f\"{i:>6} | {action_probs[i]:>12.4f} | {empirical_probs_np[i]:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5f3a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Monte-Carlo Method**\n",
    "\n",
    "In the last section we learned how to draw actions from an $\\varepsilon$-soft stochastic policy. Based on that now we can further explore the Model-Free RL methods that engage shochastic policy to balance the exploration and exploitation. Most textbooks split reinforcement-learning algorithms that do not need an explicit model into a handful of archetypes ‚Äì each updates the value (or policy) estimate in a different way.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Family                              | Core idea                                                                       | Example methods                                       |\n",
    "| ----------------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| **Monte-Carlo (MC)**                | Wait until an episode ends, then use the **total return** to update $Q$ or $V$. | every/first-visit MC Method |\n",
    "| **Temporal-Difference (TD)**        | Update after **every step** by boot-strapping from the next state‚Äôs estimate.   | SARSA, Q-learning, TD(Œª)                              |\n",
    "| **n-step / Œª-return hybrids**       | Blend MC and TD: update after $n$ steps or with eligibility traces.             | n-step TD, TD(Œª)                                      |\n",
    "| **Policy-gradient / Actor‚ÄìCritic**  | Optimise the policy parameters directly via ‚àá-estimates.                        | REINFORCE, PPO, A2C                                   |\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Among these, Monte-Carlo methods stand out by requiring no bootstrap targets and being naturally unbiased‚Äîbut they must wait for the episode to finish, giving them higher variance and slower online feedback. To tame that variance we often adopt an $var\\epsilon$-soft policy so every action is sampled infinitely often, and we use a small, constant stepsize $\\alpha$ to form a recursive MC update.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{$\\varepsilon$-Soft, On-Policy Monte-Carlo Algorithm}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{1. Initialization: }\\\\\n",
    "\\text{01: } \\text{\\quad - \\quad Define a discount factor $\\gamma$}\\\\\n",
    "\\text{02: } \\text{\\quad - \\quad Choose a constant learning rate $\\alpha$ and rollout length $N$}\\\\\n",
    "\\text{03: } \\text{\\quad - \\quad Initialise arbitrary $Q(x,u)\\in\\mathbb{R}$ for all $x\\in\\mathcal{X},\\,u\\in\\mathcal{U}(x)$}\\\\\n",
    "\\text{04: } \\text{\\quad - \\quad Initialise an arbitrary $\\varepsilon$-soft policy $\\pi(u\\mid x)$ for every $x$}\\\\\n",
    "\\text{2. Episode-Based Action Value Function and Policy Updates: }\\\\\n",
    "\\text{05: } \\quad \\textbf{while } \\textit{not converged} \\textbf{ do} \\\\\n",
    "\\text{06: } \\quad | \\quad \\text{a) Rollout}\\\\\n",
    "\\text{07: } \\quad | \\quad \\text{Generate an episode using the $\\varepsilon$-soft policy $p_{\\pi}(u|x)$ and record $\\mathcal{D}=\\{(x_k,u_k,r_k)\\}_{k=0}^{N}$}\\\\\n",
    "\\text{08: } \\quad | \\quad \\text{b) Policy Evaluation}\\\\\n",
    "\\text{09: } \\quad | \\quad \\textbf{for } \\textit{each pair $(x,u)$ in $\\mathcal{D}$} \\textbf{ do}\\\\\n",
    "\\text{10: } \\quad | \\quad | \\quad R(x,u) \\leftarrow \\text{the cumulative return following the first occurrence of} (x,u)\\\\\n",
    "\\text{11: } \\quad | \\quad | \\quad Q(x,u) \\leftarrow Q(x,u) + \\alpha(R(x,u) - Q(x,u))\\\\\n",
    "\\text{12: } \\quad | \\quad \\textbf{end}\\\\\n",
    "\\text{13: } \\quad | \\quad \\text{c) Policy Improvement}\\\\\n",
    "\\text{14: } \\quad | \\quad \\textbf{for } \\textit{each $x$ in $\\mathcal{D}$} \\textbf{ do}\\\\\n",
    "\\text{15: } \\quad | \\quad | \\quad \\pi^*(x) \\leftarrow \\text{argmax}_{u \\in \\mathcal{U}(x)} Q(x,u)\\\\\n",
    "\\text{16: } \\quad | \\quad | \\quad \\text{Update $\\varepsilon$-soft policy $p_{\\pi}(u|x)$}\\\\\n",
    "\\text{17: } \\quad | \\quad \\textbf{end}\\\\\n",
    "\\text{18: } \\quad | \\quad \\text{d) Optionally reduce $\\varepsilon$}\\\\\n",
    "\\text{19: } \\quad \\textbf{end}\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b7bba",
   "metadata": {},
   "source": [
    "#### **Reward Function**\n",
    "\n",
    "In model-free RL, due to the absence of explicit dynamics guidance, reward shaping plays a critical role in determining whether the algorithm converges. A well-designed reward should provide informative, dense, and consistent feedback to effectively guide the agent‚Äôs behavior. In this example, we design the reward function as follows:\n",
    "\n",
    "$$\n",
    "r(x, u, x') = \\begin{cases} \n",
    "10, & \\text{if }x' \\text{ is target state} \\\\\n",
    "-1, & \\text{if }x' \\text{ is not target state} \\\\\n",
    "-10, & \\text{if }x' \\notin \\mathcal{X}\n",
    "\\end{cases},\n",
    "$$\n",
    "\n",
    "where $x$ and $u$ denote the current state and input, and $x'$ denote the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085eb16",
   "metadata": {},
   "source": [
    "> <br>\n",
    "> üìå <b>Note:</b> Cost Design in Model-Free RL vs. Cost Design in Model-based Control\n",
    ">\n",
    "> In traditional model-based control methods such as LQR or MPC, the cost function is typically a smooth, differentiable quadratic form:\n",
    "> $$\n",
    "> J = \\sum_{k=0}^{N-1} \\left( x_k^\\top Q x_k + u_k^\\top R u_k \\right) + x_N^\\top Q_f x_N\n",
    "> $$\n",
    "> which penalizes deviations from the desired state and excessive control effort. These formulations require the cost to be continuous and differentiable to ensure the optimization problem is well-posed and efficiently solvable. <b>In contrast, RL methods can flexibly handle sparse or discrete rewards‚Äîsuch as binary success/failure signals‚Äîwithout requiring differentiability, making them more versatile for problems where such feedback is more natural or practical to define.</b>\n",
    ">\n",
    "> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3b727",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Implementation**\n",
    "\n",
    "Before we can roll out episodes, we must define the $\\varepsilon$-soft sampling policy, which can be formulated as:\n",
    "$$\n",
    "\n",
    "p_{\\pi}(u \\mid x) =\n",
    "\\begin{cases}\n",
    "\\dfrac{\\varepsilon}{\\lvert \\mathcal{U} \\rvert}, \n",
    "    & \\text{for an action associated with non-optimal } Q, \\\\[6pt]\n",
    "1-\\varepsilon \\Bigl(1-\\dfrac{1}{\\lvert \\mathcal{U} \\rvert}\\Bigr), \n",
    "    & \\text{for the action associated with optimal } Q.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Given a discrete state index, the routine first looks up the action with the highest Q-value in that row of the table. It then constructs a probability vector in which every action receives a uniform exploration mass of $\\varepsilon / \\lvert \\mathcal{U} \\rvert$, while the greedy action gets an extra slice $1-\\varepsilon$. The resulting vector therefore sums to $1$, places most of the probability on the current greedy action, yet guarantees that every action is sampled with non-zero probability. This vector is returned and can be passed directly to `np.random.choice` (or any other sampler) to sample an exact action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_action_probabilities(self, state_index: int) -> np.ndarray:\n",
    "    \"\"\"Calculate the action probabilities using epsilon-soft policy.\"\"\"\n",
    "\n",
    "    probabilities = np.ones(self.dim_inputs) * (self.epsilon / self.dim_inputs)\n",
    "    best_action = np.argmax(self.Q[state_index, :])\n",
    "    probabilities[best_action] += (1.0 - self.epsilon)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0477fea",
   "metadata": {},
   "source": [
    "Based on the Œµ-soft sampling rule, the whole **MCRL training loop** in the `setup()` routine is built around three consecutive phases:\n",
    "\n",
    "1) **Episode rollout:** \n",
    "   Starting from the specified initial state, the agent uses the current Œµ-soft policy to generate a complete episode, storing each $(x,u,r)$ triple until it reaches the goal, fails, or hits the step cap.\n",
    "\n",
    "2) **Recursive Monte-Carlo update:** \n",
    "   After the episode ends the code scans the trajectory backwards, accumulates the discounted return $G$, and updates the table with\n",
    "   $Q(x,u)\\leftarrow Q(x,u)+\\omega\\bigl(G-Q(x,u)\\bigr)$,\n",
    "   turning the raw return into an exponential moving-average estimate.\n",
    "\n",
    "3) **Policy refresh & exploration decay:** \n",
    "   With the new $Q$ the greedy action for every visited state is recomputed and plugged back into an Œµ-soft distribution; Œµ is then multiplied by a decay factor (but never below `epsilon_min`) so that each subsequent episode strikes a gradually tighter explore-exploit balance.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üìå <b>Note:</b> Here we restrict initial state distribution to boost the training, but one can also randomly initialize state given sufficient interaction to improve the generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c49c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(self) -> None:\n",
    "\n",
    "    for iteration in range(self.max_iterations):\n",
    "\n",
    "        episode = []  # storage state, action and reward for current episode\n",
    "        total_reward = 0  # total reward for current episode\n",
    "        total_steps = 0  # total steps for current episode\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "\n",
    "            if iteration != 0:\n",
    "                # Record the SR_100epsd, F_100epsd, TO_100epsd\n",
    "                self.SR_100epsd.append(SR_100epsd)\n",
    "                self.F_100epsd.append(F_100epsd)\n",
    "                self.TO_100epsd.append(TO_100epsd)\n",
    "            \n",
    "            SR_100epsd = 0\n",
    "            F_100epsd = 0\n",
    "            TO_100epsd = 0\n",
    "\n",
    "        # Start from init state\n",
    "        # Note: Here we restrict initial state distribution to boost the training but one can also \n",
    "        #       randomly initialize state given sufficient interaction to improve the generalization.\n",
    "        current_state = self.mdp.init_state\n",
    "        current_state_index = self.mdp.nearest_state_index_lookup(current_state)\n",
    "        \n",
    "        # Generate an episode\n",
    "        for step in range(self.max_steps_per_episode):\n",
    "            # Choose action based on epsilon-soft policy\n",
    "            action_probabilities = self._get_action_probabilities(current_state_index)\n",
    "            action_index = np.random.choice(np.arange(self.dim_inputs), p=action_probabilities)\n",
    "            current_input = self.mdp.input_space[action_index]\n",
    "\n",
    "            # Take action and observe the next state and reward\n",
    "            next_state, reward = self.mdp.one_step_forward(current_state, current_input)\n",
    "            next_state_index = self.mdp.nearest_state_index_lookup(next_state)\n",
    "            total_reward += reward\n",
    "            total_steps += 1 \n",
    "\n",
    "            # Store the state, action and reward for this step\n",
    "            episode.append((current_state_index, action_index, reward))\n",
    "\n",
    "            # Check if the episode is finished\n",
    "            terminate_condition_1 = next_state[0]>self.mdp.pos_partitions[-1]\n",
    "            terminate_condition_2 = next_state[0]<self.mdp.pos_partitions[0]\n",
    "            terminate_condition_3 = np.all(self.mdp.state_space[:, next_state_index]==self.target_state)\n",
    "            \n",
    "            if terminate_condition_1 or terminate_condition_2 or terminate_condition_3:\n",
    "                if terminate_condition_3:\n",
    "                    SR_100epsd += 1\n",
    "                    if self.verbose:\n",
    "                        print(f\"Iteration {iteration + 1}/{self.max_iterations}: finished successfully! epsilon: {self.epsilon:.4f}, residual reward: {total_reward:.2f}\")\n",
    "                else:\n",
    "                    F_100epsd += 1\n",
    "                    if self.verbose:\n",
    "                        print(f\"Iteration {iteration + 1}/{self.max_iterations}: episode failed! epsilon: {self.epsilon:.4f}, residual reward: {total_reward:.2f}\")\n",
    "                break\n",
    "\n",
    "            if step == self.max_steps_per_episode-1:\n",
    "                TO_100epsd +=1\n",
    "                if self.verbose:\n",
    "                    print(f\"Iteration {iteration + 1}/{self.max_iterations}: time out! epsilon: {self.epsilon:.4f}, residual reward: {total_reward:.2f}\")\n",
    "                \n",
    "            # Move to the next state\n",
    "            current_state_index = next_state_index\n",
    "            current_state = self.mdp.state_space[:, current_state_index]\n",
    "\n",
    "        # Update Q table using Monte Carlo method\n",
    "        G = 0  # Return\n",
    "        for state_index, action_index, reward in reversed(episode):\n",
    "            # Cumulative return\n",
    "            G = reward + self.gamma * G\n",
    "            \n",
    "            # Factor in recursive estimation\n",
    "            self.state_action_counts[state_index, action_index] += 1\n",
    "            alpha = self.learning_rate\n",
    "\n",
    "            # Update Q using MC and log TD error\n",
    "            td_error = G - self.Q[state_index, action_index]\n",
    "            self.Q[state_index, action_index] += alpha * td_error\n",
    "\n",
    "        # Decrease epsilon\n",
    "        self.epsilon *= self.k_epsilon\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        self.epsilon_list.append(self.epsilon)\n",
    "\n",
    "        # Record the residual reward and loss\n",
    "        self.residual_rewards.append(total_reward)\n",
    "        self.step_list.append(total_steps)\n",
    "\n",
    "    # Return the deterministic policy and value function\n",
    "    self.policy = np.argmax(self.Q, axis=1)\n",
    "    self.value_function = np.max(self.Q, axis=1)\n",
    "\n",
    "    # Repeat success/failure stats for plotting\n",
    "    self.SR_100epsd = np.repeat(self.SR_100epsd, 100)/100\n",
    "    self.F_100epsd = np.repeat(self.F_100epsd, 100)/100\n",
    "    self.TO_100epsd = np.repeat(self.TO_100epsd, 100)/100\n",
    "\n",
    "    if self.verbose:\n",
    "        print(\"Training finishedÔºÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6a1bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Training Curve and Key Metrics**\n",
    "\n",
    "When analysing a reinforcement-learning run we usually track a handful of standard metrics:\n",
    "\n",
    "1) **Episode Return / Reward:** the total discounted reward collected in one episode; the primary signal for whether the agent is learning the task.\n",
    "\n",
    "2) **Success Rate (SR):** fraction of episodes that reach the goal under the current policy; complements reward on sparse-reward tasks.\n",
    "\n",
    "3) **Episode Length:** number of steps per episode; often drops as the policy becomes more efficient.\n",
    "\n",
    "4) **TD Error (or Loss):** the squared temporal-difference error (or MC error); useful for diagnosing convergence and instability.\n",
    "\n",
    "Using the hyper-parameter settings listed below we train the agent and then plot the learning curves. \n",
    "\n",
    "*Note that: In this Jupyter Notebook we will mainly focus on* **Episode Reward** *and* **Success Rate (SR)**, *as these two curves give the clearest picture of both sample-efficiency and eventual task performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 1\n",
    "\n",
    "gamma = 0.90\n",
    "epsilon = 1.0\n",
    "k_epsilon = 0.995\n",
    "learning_rate = 0.1\n",
    "max_iterations = 2000\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "# Instantiate class 'Env', 'Dynamics', and 'Env_rl_d'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_d(env=env, dynamics=dynamics, num_states=num_states, num_actions=num_actions, dt=1/freq, build_stochastic_mdp=False)\n",
    "\n",
    "# Instantiate the MCRL controller class\n",
    "controller_mcrl = MCRLController(mdp, freq, epsilon=epsilon, k_epsilon=k_epsilon, \n",
    "                                    learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations)\n",
    "controller_mcrl.setup()\n",
    "controller_mcrl.plot_training_curve(\"Training Curve before Smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25831a",
   "metadata": {},
   "source": [
    "The raw-reward trace on the left is **very jagged**‚Äîindividual episode returns oscillate wildly, making the underlying learning trend hard to see. To reveal that trend we usually **smooth** the curve, e.g. with a simple moving average:\n",
    "\n",
    "$$\n",
    "\\tilde R_t \\;=\\; \\frac{1}{w}\\sum_{k=0}^{w-1} R_{t-k},\n",
    "$$\n",
    "\n",
    "where $w$ is a fixed window (50 episodes in this case). Averaging neighbouring points damps the high-frequency noise so the long-term rise in performance becomes much clearer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(self, window=20):\n",
    "    \n",
    "    \"\"\"Postprocess the training results, including smoothing the reward curve.\"\"\"\n",
    "\n",
    "    residual_rewards = np.array(self.residual_rewards)\n",
    "    if window <= 1:\n",
    "        return residual_rewards.copy()\n",
    "    \n",
    "    kernel = np.ones(window)\n",
    "    z = np.ones(len(residual_rewards))        \n",
    "    self.residual_rewards_smoothed = np.convolve(residual_rewards, kernel, mode='same') / np.convolve(z, kernel, mode='same')\n",
    "\n",
    "    return self.residual_rewards_smoothed, self.SR_100epsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_mcrl.postprocessing(window=50)\n",
    "controller_mcrl.plot_training_curve(\"Training Curve after Smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b423d",
   "metadata": {},
   "source": [
    "The training curves make the learning dynamics of the car controller easy to follow. First, $\\varepsilon$ decays roughly as a power-law with the episode index; this gradual reduction turns the $\\varepsilon$-soft policy from heavy exploration toward exploitation until it finally converges. In parallel, the success-rate measured every 100 episodes rises steadily with more training, ultimately reaching about $60 \\%$. Together these plots show the Monte Carlo method agent learning to trade exploration for performance and converging to a reliably successful driving policy, which will be shown detailedly in the following simlulation section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac51371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_mcrl = Simulator(dynamics, controller_mcrl, env, 1/freq, t_terminal)\n",
    "simulator_mcrl.run_simulation()\n",
    "\n",
    "# Also setup a NMPC controller for reference\n",
    "env_mpc = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs_mpc, state_ubs=state_ubs_mpc, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_mpc = Dynamics(env_mpc)\n",
    "controller_mpc = MPCController(env_mpc, dynamics_mpc, Q, R, Qf, freq_mpc, N, name=\"NMPC_reference\")\n",
    "simulator_mpc = Simulator(dynamics_mpc, controller_mpc, env_mpc, 1/freq_mpc, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mcrl = Visualizer(simulator_mcrl)\n",
    "visualizer_mcrl.display_contrast_plots(\"Simulation of Monte Carlo Method on Flat Terrain\", simulator_mpc, if_gray=True)\n",
    "visualizer_mcrl.display_contrast_animation_same(simulator_mpc, if_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20732d",
   "metadata": {},
   "source": [
    "\n",
    "Overall, the trained policy is able to steer the car to the target position without violating any constraints, althrough it has not yet converged to the optimal policy, which should be a bang-bang policy under the current cost design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624ab80",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üîç Hands-on Exploration: hyperparameters in training**\n",
    "\n",
    "Haperparameters are user-chosen knobs, which are set before training starts and not updated within the period. Together they dictate the speed, stability and final quality of learning. **Which hyperparameters appear in our implementation of Monte Carlo method? How does each one influence the training curves?** Change the value and rerun the code block above to verify your understandings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3413b6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Simulation on a Hilly Terrain**\n",
    "\n",
    "The figures above illustrate in the **flat-terrain scenario**. In addition, we can carry out the same training and simulation procedure on the under-actuated hilly terrain. **Here the input is constrained to the under-actuated case**, which means the car cannot generate enough kinetic energy in a single burst to crest the hill; instead it must swing back and forth several times to build momentum, making the task markedly more challenging. Run the training script below, inspect the resulting learning curves, and then launch a simulation to see how the converged policy performs in this tougher setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cec501",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 4\n",
    "\n",
    "gamma = 0.90\n",
    "epsilon = 1.0\n",
    "k_epsilon = 0.995\n",
    "learning_rate = 0.05\n",
    "max_iterations = 1500\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "# Instantiate class 'Env', 'Dynamics', and 'Env_rl_d'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_d(env=env, dynamics=dynamics, num_states=num_states, num_actions=num_actions, dt=1/freq, build_stochastic_mdp=False)\n",
    "\n",
    "# Instantiate the MCRL controller class\n",
    "controller_mcrl = MCRLController(mdp, freq, epsilon=epsilon, k_epsilon=k_epsilon, \n",
    "                                    learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations)\n",
    "controller_mcrl.setup()\n",
    "controller_mcrl.postprocessing(window=100)\n",
    "controller_mcrl.plot_training_curve()\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_mcrl = Simulator(dynamics, controller_mcrl, env, 1/freq, t_terminal)\n",
    "simulator_mcrl.run_simulation()\n",
    "\n",
    "# Also setup a NMPC controller for reference\n",
    "env_mpc = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs_mpc, state_ubs=state_ubs_mpc, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_mpc = Dynamics(env_mpc)\n",
    "controller_mpc = MPCController(env_mpc, dynamics_mpc, Q, R, Qf, freq_mpc, N, name=\"NMPC_reference\")\n",
    "simulator_mpc = Simulator(dynamics_mpc, controller_mpc, env_mpc, 1/freq_mpc, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mcrl = Visualizer(simulator_mcrl)\n",
    "visualizer_mcrl.display_contrast_plots(\"Simulation of Monte Carlo Method on Hilly Terrain\", simulator_mpc, if_gray=True)\n",
    "visualizer_mcrl.display_contrast_animation_same(simulator_mpc, if_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80e0af",
   "metadata": {},
   "source": [
    "We observe that, after training, the car has learned to swing back and forth to accumulate kinetic energy and then crest the hill in one final push.\n",
    "Compared with the flat-terrain experiment, the hilly case converges much faster‚Äîthe Success-Rate curve climbs earlier and reaches a stable plateau sooner.\n",
    "Moreover, although we're still using Monte Carlo such a high-variance estimator, the reward trace on hilly terrain shows far fewer late-stage spikes; even without smoothing the overall downward noise, the curve clearly trends upward toward convergence. In contrast, the reward curve for the flat-terrain case still displays many sharp spikes in the later stages, signalling lingering high variance. **All in all, training‚Äîand therefore inspecting the policy‚Äôs progress‚Äîis noticeably easier in the hilly-terrain scenario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70825d4d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Exploration vs. Exploitation**\n",
    "\n",
    "In Model-Free Reinforcement Learning, you have no prior knowledge about the system dynamcis or the transition property, so you will need some policy to conduxt some exploration on this. From there raise the fundamental trade-off between **exploration** (trying new actions to discover potentially better strategies) and **exploitation** (leveraging the current best-known policy to maximize reward). To manage this balance, we adopt an **$\\varepsilon$-soft policy**, where the agent chooses the best-known action with high probability $1 - \\varepsilon$, but with small probability $\\varepsilon$ it randomly explores other actions.\n",
    "\n",
    "To investigate the impact of this trade-off, we compare the learning performance under three different settings:\n",
    "\n",
    "* **Run 1**: $\\varepsilon = 1.0$ with decay ($k$ = 0.995), a gradually shifting balance from pure exploration to exploitation.\n",
    "\n",
    "* **Run 2**: $\\varepsilon = 0.0$ (pure exploitation), no exploration throughout training.\n",
    "\n",
    "* **Run 3**: $\\varepsilon = 1.0$ (pure exploration), the agent never exploits.\n",
    "\n",
    "Run 1 represents a well-balanced exploration strategy, while Runs 2 and 3 illustrate extreme cases. These variations help us understand the importance of proper exploration in ensuring convergence and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 4\n",
    "\n",
    "gamma = 0.90\n",
    "learning_rate = 0.1\n",
    "max_iterations = 1500\n",
    "\n",
    "seed_list = [42]\n",
    "\n",
    "save_dir = \"./mfrl_results\"\n",
    "\n",
    "# Run 1: decayng epsilon\n",
    "epsilon_1 = 1.0\n",
    "k_epsilon_1 = 0.995\n",
    "\n",
    "# Run 2: epsilon = 0.0\n",
    "epsilon_2 = 0.0\n",
    "k_epsilon_2 = 0.0\n",
    "\n",
    "# Run 3: epsilon = 1.0\n",
    "epsilon_3 = 1.0\n",
    "k_epsilon_3 = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class 'Env', 'Dynamics', and 'Env_rl_d'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_d(env=env, dynamics=dynamics, num_states=num_states, num_actions=num_actions, dt=1/freq, build_stochastic_mdp=False)\n",
    "\n",
    "# Instantiate the MCRL controller class\n",
    "controller_mcrl_1 = MCRLController(mdp, freq, epsilon=epsilon_1, k_epsilon=k_epsilon_1, learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations, name=\"decaying_eps\")\n",
    "controller_mcrl_2 = MCRLController(mdp, freq, epsilon=epsilon_2, k_epsilon=k_epsilon_2, learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations, name=\"eps=0\")\n",
    "controller_mcrl_3 = MCRLController(mdp, freq, epsilon=epsilon_3, k_epsilon=k_epsilon_3, learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations, name=\"eps=1\")\n",
    "\n",
    "# Set up the controllers\n",
    "controller_instances = {\n",
    "    \"mcrl_decaying_eps\": controller_mcrl_1,\n",
    "    \"mcrl_eps=0\": controller_mcrl_2,\n",
    "    \"mcrl_eps=1\": controller_mcrl_3,\n",
    "}\n",
    "\n",
    "# Set up the RLExperimentRunner\n",
    "runner = RLExperimentRunner(\n",
    "    controller_instances=controller_instances,\n",
    "    seed_list=seed_list,\n",
    "    save_dir=save_dir\n",
    ")\n",
    "\n",
    "# Run all controllers with all seeds and save the results\n",
    "runner.run_all()\n",
    "runner.plot(\"Exploration vs. Exploitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained controllers for simulation\n",
    "controller_mcrl_1 = runner.get_trained_controller(name=\"mcrl_decaying_eps\", seed=seed_list[0])\n",
    "simulator_mcrl_1 = Simulator(dynamics, controller_mcrl_1, env, 1/freq, t_terminal)\n",
    "simulator_mcrl_1.run_simulation()\n",
    "\n",
    "controller_mcrl_2 = runner.get_trained_controller(name=\"mcrl_eps=0\", seed=seed_list[0])\n",
    "simulator_mcrl_2 = Simulator(dynamics, controller_mcrl_2, env, 1/freq, t_terminal)\n",
    "simulator_mcrl_2.run_simulation()\n",
    "\n",
    "controller_mcrl_3 = runner.get_trained_controller(name=\"mcrl_eps=1\", seed=seed_list[0])\n",
    "simulator_mcrl_3 = Simulator(dynamics, controller_mcrl_3, env, 1/freq, t_terminal)\n",
    "simulator_mcrl_3.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mcrl = Visualizer(simulator_mcrl_1)\n",
    "visualizer_mcrl.display_contrast_plots(\"Exploration vs. Exploitation\", simulator_mcrl_2, simulator_mcrl_3)\n",
    "visualizer_mcrl.display_contrast_animation_same(simulator_mcrl_2, simulator_mcrl_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5407f7",
   "metadata": {},
   "source": [
    "- **Run 1: Decaying Œµ (Œµ=1.0 ‚Üí Œµ\\_min, k=0.995)**\n",
    "\n",
    "  -  **Reward Curve**: Significant fluctuations early on indicate active exploration; gradually converges to a higher reward, showing stable policy learning.\n",
    "  -  **Success Rate**: Rapidly increases around episode 500 and approaches 100%, indicating good convergence and robustness.\n",
    "  -  **Trajectory Analysis**: Shows trial-and-error behavior initially, but converges to smooth and efficient motion towards the target in later stages.\n",
    "\n",
    "   **Summary**: The decaying Œµ-soft strategy effectively balances exploration and exploitation. It allows the agent to explore sufficiently at the beginning while exploiting learned knowledge later, leading to the best overall performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Run 2: Constant Œµ = 0.0 (Pure Exploitation)**\n",
    "\n",
    "  -  **Reward Curve**: Initially poor due to suboptimal policy, but once a good policy is accidentally discovered, it converges quickly.\n",
    "  -  **Success Rate**: Approaches 100% eventually, but this is largely attributed to the long episode length.\n",
    "  -  **Trajectory Analysis**: Shows repetitive and conservative behavior with no variation, lacking adaptability.\n",
    "\n",
    "   **Summary**: Pure exploitation may converge quickly if lucky with initialization, but it risks getting stuck in local optima due to lack of exploration.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Run 3: Constant Œµ = 1.0 (Pure Exploration)**\n",
    "\n",
    "  -  **Reward Curve**: Highly volatile throughout, with no sign of convergence.\n",
    "  -  **Success Rate**: Remains low (<20%) across episodes, indicating that the agent continues to explore without learning from past experience.\n",
    "  -  **Trajectory Analysis**: Trajectories remain random and inconsistent, showing no effective strategy formation.\n",
    "\n",
    "    **Summary**: Full exploration without exploitation prevents the agent from consolidating learning. Although it covers the state space well, it fails to use acquired knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70832401",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üí° Take-away: Exploration‚ÄìExploitation Trade-off in Model-free RL**\n",
    "\n",
    "**A proper balance between exploration and exploitation is critical in Model-Free RL.** Pure exploitation may lead to premature convergence, while pure exploration prevents policy stabilization. A decaying Œµ-soft strategy enables effective learning by exploring early and exploiting later.\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also setup a NMPC controller for reference\n",
    "env_mpc = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs_mpc, state_ubs=state_ubs_mpc, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_mpc = Dynamics(env_mpc)\n",
    "controller_mpc = MPCController(env_mpc, dynamics_mpc, Q, R, Qf, freq_mpc, N, name=\"NMPC_reference\")\n",
    "simulator_mpc = Simulator(dynamics_mpc, controller_mpc, env_mpc, 1/freq_mpc, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Compare the best result from Monte Carlo method with NMPC\n",
    "visualizer_mcrl = Visualizer(simulator_mcrl_1)\n",
    "visualizer_mcrl.display_contrast_plots(\"The best Epsilon-decaying Group from Monte Carlo Method vs. NMPC\", simulator_mpc, if_gray=True)\n",
    "visualizer_mcrl.display_contrast_animation_same(simulator_mpc, if_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db0e27",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Q-Learning**\n",
    "\n",
    "Q-Learning is one of the most widely used **Model-Free reinforcement learning algorithms**, based on the *temporal-difference (TD)* learning paradigm. Unlike the Monte Carlo Reinforcement Learning (MCRL) method discussed earlier‚Äîwhich waits until the end of each episode to update value estimates‚ÄîQ-Learning performs **incremental updates** at every step based on the current state, action, reward, and the maximum expected future reward.\n",
    "\n",
    "This **off-policy** nature and step-wise update strategy allow Q-Learning to converge more efficiently and with lower variance compared to MCRL, especially in environments with long or stochastic episodes. Below is the pseudocode for the Q-Learning algorithm.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Q-Learning (Off-Policy Temporal-Difference Method)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{1. Initialization: }\\\\\n",
    "\\text{01: } \\text{\\quad - \\quad Define a discount factor $\\gamma$}\\\\\n",
    "\\text{02: } \\text{\\quad - \\quad Choose a constant learning rate $\\alpha$ and a small $\\epsilon > 0$}\\\\\n",
    "\\text{03: } \\text{\\quad - \\quad Initialise arbitrary $Q(x,u)\\in\\mathbb{R}$ for all $x\\in\\mathcal{X},\\,u\\in\\mathcal{U}(x)$}\\\\\n",
    "\\text{2. Episode-Based Action Value Function Updates: }\\\\\n",
    "\\text{04: } \\quad \\textbf{while } \\textit{not converged} \\textbf{ do} \\\\\n",
    "\\text{0: } \\quad | \\quad \\text{Initialize state $x$}\\\\\n",
    "\\text{06: } \\quad | \\quad \\textbf{while } \\textit{episode not terminated} \\textbf{ do}\\\\\n",
    "\\text{07: } \\quad | \\quad | \\quad \\text{Choose $u$ based on a policy $p_{\\pi}(u|x)$ derived from $Q(x,u)$ (e.g., an $\\epsilon$-soft policy)}\\\\\n",
    "\\text{08: } \\quad | \\quad | \\quad \\text{Take action $u$ and observe $r$ and $x'$}\\\\\n",
    "\\text{09: } \\quad | \\quad | \\quad Q(x,u) \\leftarrow Q(x,u) + \\alpha(r + \\gamma \\text{max}_{u'}Q(x', u') - Q(x,u))\\\\\n",
    "\\text{10: } \\quad | \\quad | \\quad \\text{Goto next state $x \\leftarrow x'$}\\\\\n",
    "\\text{11: } \\quad | \\quad \\textbf{end}\\\\\n",
    "\\text{12: } \\quad \\textbf{end}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d05cf",
   "metadata": {},
   "source": [
    "\n",
    "#### **Implementation**\n",
    "\n",
    "The `setup()` routine for **Q-Learning** revolves around three core phases executed iteratively across episodes:\n",
    "\n",
    "1. **Step-wise interaction:**\n",
    "   Starting from the initial state (typically fixed to accelerate training), the agent repeatedly selects actions based on the current Œµ-soft policy, steps through the environment, and records the reward and next state until a termination condition is reached or the episode times out.\n",
    "\n",
    "2. **Temporal-Difference (TD) update:**\n",
    "   Immediately after each transition \\$(x, u, r, x')\\$, the agent performs an online update to the Q-table using the TD rule: $Q(x,u) \\leftarrow Q(x,u) + \\alpha \\bigl(r + \\gamma \\max_{u'} Q(x', u') - Q(x,u)\\bigr)$, which incorporates both immediate rewards and bootstrapped estimates of future returns.\n",
    "\n",
    "3. **Policy evaluation & exploration decay:**\n",
    "   After each episode, the greedy policy (used implicitly inside the Œµ-soft action sampler) becomes more reliable as the Q-table improves. The exploration rate Œµ is reduced geometrically to gradually shift from exploration to exploitation, while all key statistics like success rate and average reward are logged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556417da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "def setup(self) -> None:\n",
    "\n",
    "    for iteration in range(self.max_iterations):\n",
    "\n",
    "        total_reward = 0  # To accumulate rewards for this episode\n",
    "        total_steps = 0  # To count total steps in this episode\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "\n",
    "            if iteration != 0:\n",
    "                # Record the SR_100epsd, F_100epsd, TO_100epsd\n",
    "                self.SR_100epsd.append(SR_100epsd)\n",
    "                self.F_100epsd.append(F_100epsd)\n",
    "                self.TO_100epsd.append(TO_100epsd)\n",
    "            \n",
    "            SR_100epsd = 0\n",
    "            F_100epsd = 0\n",
    "            TO_100epsd = 0\n",
    "\n",
    "        # Start from init state\n",
    "        # Note: Here we restrict initial state distribution to boost the training but one can also \n",
    "        #       randomly initialize state given sufficient interaction to improve the generalization.\n",
    "        current_state = self.mdp.init_state\n",
    "        current_state_index = self.mdp.nearest_state_index_lookup(current_state)\n",
    "\n",
    "        for step in range(self.max_steps_per_episode):\n",
    "\n",
    "            # Choose action based on epsilon-soft policy\n",
    "            action_probabilities = self._get_action_probabilities(current_state_index)\n",
    "            action_index = np.random.choice(np.arange(self.dim_inputs), p=action_probabilities)\n",
    "            current_input = self.mdp.input_space[action_index]\n",
    "\n",
    "            # Take action and observe the next state and reward\n",
    "            next_state, reward = self.mdp.one_step_forward(current_state, current_input)\n",
    "            next_state_index = self.mdp.nearest_state_index_lookup(next_state)\n",
    "            total_reward += reward  # Accumulate total reward\n",
    "            total_steps += 1  # Increment step count\n",
    "            \n",
    "            # Update Q table and compute TD error\n",
    "            td_error = reward + self.gamma * np.max(self.Q[next_state_index, :]) - self.Q[current_state_index, action_index]\n",
    "\n",
    "            # Factor in recursive estimation\n",
    "            self.state_action_counts[current_state_index, action_index] += 1\n",
    "            alpha = self.learning_rate\n",
    "\n",
    "            # Update Q table and log TD error\n",
    "            self.Q[current_state_index, action_index] += alpha * td_error\n",
    "            \n",
    "            # Check if the episode is finished\n",
    "            terminate_condition_1 = next_state[0]>self.mdp.pos_partitions[-1]\n",
    "            terminate_condition_2 = next_state[0]<self.mdp.pos_partitions[0]\n",
    "            terminate_condition_3 = np.all(self.mdp.state_space[:, next_state_index]==self.target_state)\n",
    "\n",
    "            if terminate_condition_1 or terminate_condition_2 or terminate_condition_3:\n",
    "                if terminate_condition_3:\n",
    "                    SR_100epsd += 1\n",
    "                    if self.verbose:\n",
    "                        print(f\"Iteration {iteration + 1}/{self.max_iterations}: finished successfully at step {step}! epsilon: {self.epsilon:.4f}, residual reward: {total_reward:.2f}\")\n",
    "                else:\n",
    "                    F_100epsd += 1\n",
    "                    if self.verbose:\n",
    "                        print(f\"Iteration {iteration + 1}/{self.max_iterations}: episode failed at step {step}! epsilon: {self.epsilon:.4f}, residual reward: {total_reward:.2f}\")\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # Move to the next state\n",
    "                current_state_index = next_state_index\n",
    "                current_state = self.mdp.state_space[:, current_state_index]\n",
    "            \n",
    "            if step == self.max_steps_per_episode-1:\n",
    "                TO_100epsd +=1\n",
    "                if self.verbose:\n",
    "                    print(f\"Iteration {iteration + 1}/{self.max_iterations}: time out (step: {step})! epsilon: {self.epsilon:.4f}, residual reward: {total_reward:.2f}\")\n",
    "                \n",
    "        # Decrease epsilon\n",
    "        self.epsilon *= self.k_epsilon\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        self.epsilon_list.append(self.epsilon)\n",
    "\n",
    "        # Record the residual reward and loss\n",
    "        self.residual_rewards.append(total_reward)\n",
    "        self.step_list.append(total_steps)\n",
    "    \n",
    "    # Return the deterministic policy and value function\n",
    "    self.policy = np.argmax(self.Q, axis=1)\n",
    "    self.value_function = np.max(self.Q, axis=1)\n",
    "\n",
    "    # Repeat success/failure stats for plotting\n",
    "    self.SR_100epsd = np.repeat(self.SR_100epsd, 100)/100\n",
    "    self.F_100epsd = np.repeat(self.F_100epsd, 100)/100\n",
    "    self.TO_100epsd = np.repeat(self.TO_100epsd, 100)/100\n",
    "    \n",
    "    if self.verbose:\n",
    "        print(\"Training finishedÔºÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f347dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Simulation on a Flat Terrain**\n",
    "\n",
    "Similar to the Monte Carlo RL setup, we first train and simulate the agent on a flat terrain to establish a clean and ideal baseline. Notably, since Q-Learning uses recursive Temporal-Difference updates, the learning signal (TD error) tends to have smaller variance, leading to more stable updates. This allows us to adopt a relatively higher learning rate, accelerating the convergence process without sacrificing stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 1\n",
    "\n",
    "gamma = 0.90\n",
    "epsilon = 1.0\n",
    "k_epsilon = 0.995\n",
    "learning_rate = 0.3\n",
    "max_iterations = 1500\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fea193",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "# Instantiate class 'Env', 'Dynamics', and 'Env_rl_d'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_d(env=env, dynamics=dynamics, num_states=num_states, num_actions=num_actions, dt=1/freq, build_stochastic_mdp=False)\n",
    "\n",
    "# Instantiate the Q-learning controller class\n",
    "controller_ql = QLearningController(mdp, freq, epsilon=epsilon, k_epsilon=k_epsilon,\n",
    "                                    learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations)\n",
    "controller_ql.setup()\n",
    "controller_ql.postprocessing(window=50)\n",
    "controller_ql.plot_training_curve()\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_ql = Simulator(dynamics, controller_ql, env, 1/freq, t_terminal)\n",
    "simulator_ql.run_simulation()\n",
    "\n",
    "# Also setup a NMPC controller for reference\n",
    "env_mpc = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs_mpc, state_ubs=state_ubs_mpc, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_mpc = Dynamics(env_mpc)\n",
    "controller_mpc = MPCController(env_mpc, dynamics_mpc, Q, R, Qf, freq_mpc, N, name=\"NMPC_reference\")\n",
    "simulator_mpc = Simulator(dynamics_mpc, controller_mpc, env_mpc, 1/freq_mpc, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ql = Visualizer(simulator_ql)\n",
    "visualizer_ql.display_contrast_plots(\"Simulation of Q-Learning Controller on Flat Terrain\", simulator_mpc, if_gray=True)\n",
    "visualizer_ql.display_contrast_animation_same(simulator_mpc, if_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1da049",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Simulation on a Hilly Terrain**\n",
    "\n",
    "We likewise conduct training on the hilly terrain and visualize the simulation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 4\n",
    "\n",
    "gamma = 0.90\n",
    "epsilon = 1.0\n",
    "k_epsilon = 0.995\n",
    "learning_rate = 0.2\n",
    "max_iterations = 2000\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b806d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "# Instantiate class 'Env', 'Dynamics', and 'Env_rl_d'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_d(env=env, dynamics=dynamics, num_states=num_states, num_actions=num_actions, dt=1/freq, build_stochastic_mdp=False)\n",
    "\n",
    "# Instantiate the Q-learning controller class\n",
    "controller_ql = QLearningController(mdp, freq, epsilon=epsilon, k_epsilon=k_epsilon,\n",
    "                                    learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations)\n",
    "controller_ql.setup()\n",
    "controller_ql.postprocessing(window=50)\n",
    "controller_ql.plot_training_curve()\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_ql = Simulator(dynamics, controller_ql, env, 1/freq, t_terminal)\n",
    "simulator_ql.run_simulation()\n",
    "\n",
    "# Also setup a NMPC controller for reference\n",
    "env_mpc = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs_mpc, state_ubs=state_ubs_mpc, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_mpc = Dynamics(env_mpc)\n",
    "controller_mpc = MPCController(env_mpc, dynamics_mpc, Q, R, Qf, freq_mpc, N, name=\"NMPC_reference\")\n",
    "simulator_mpc = Simulator(dynamics_mpc, controller_mpc, env_mpc, 1/freq_mpc, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ql = Visualizer(simulator_ql)\n",
    "visualizer_ql.display_contrast_plots(\"Simulation of Q-Learning Controller on Hilly Terrain\", simulator_mpc, if_gray=True)\n",
    "visualizer_ql.display_contrast_animation_same(simulator_mpc, if_gray=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b4532",
   "metadata": {},
   "source": [
    "The Q-learning agent demonstrates a smooth and stable learning curve even on the more challenging hilly terrain. The reward increases steadily, and the success rate reaches nearly 100% after sufficient training. Although the final policy is not perfectly optimal, it satisfies the task requirements without triggering input saturation or constraint violations, showing strong generalization despite the terrain's complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e52bdb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Monte Carlo Method vs. Q-Learning**\n",
    "\n",
    "In this section, we aim to empirically compare the performance of two fundamental Model-Free RL algorithms: Monte Carlo Reinforcement Learning (MCRL) and Q-Learning. While both methods share the same objective‚Äîlearning an optimal policy without explicit model knowledge‚Äîthey differ in how value updates are performed: MCRL uses full-episode returns, whereas Q-Learning adopts a recursive temporal-difference approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3dba07",
   "metadata": {},
   "source": [
    "#### **Fair Comparison & Statistical Evaluation**\n",
    "\n",
    "To ensure a fair and rigorous comparison, we follow best practices inspired by recent reproducibility efforts in RL research, particularly:\n",
    "\n",
    "> **[JMLR 2024, ‚ÄúRevisiting RL Evaluation: Reproducibility, Fairness, and Statistical Significance‚Äù](https://www.jmlr.org/papers/volume25/23-0183/23-0183.pdf)**\n",
    "\n",
    "Key recommendations adopted from the paper:\n",
    "\n",
    "* **Multiple Random Seeds:** Since RL algorithms are sensitive to initialization and stochastic transitions, we fix a list of diverse random seeds (e.g., `[42, 63, 8042, 10328, 4174]`) and evaluate performance across them.\n",
    "\n",
    "* **Mean & Variance Reporting:** After training under each seed, we aggregate the reward and success rate curves, and report **mean ¬± 3 * standard deviation** to reflect central tendency and variability.\n",
    "\n",
    "* **Identical Environment Setup:** Both algorithms use the same reward function, state/input space, discount factor `Œ≥`, and initial conditions to ensure structural fairness.\n",
    "\n",
    "* **Controlled Hyperparameters:** Only algorithm-specific parameters (like update rule or exploration strategy) are allowed to differ.\n",
    "\n",
    "This ensures that the observed performance differences can be attributed to algorithmic factors rather than confounding implementation artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5870af0",
   "metadata": {},
   "source": [
    "#### **Experimental Design**\n",
    "\n",
    "The hyperparameter setting and chosen random seeds are specifed in the following code block. Under the setup mentioned above, both MCRL and Q-Learning are trained and evaluated independently for each seed. The final results are visualized using reward curves, success rate evolution, and state-input trajectories under the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acf71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 4\n",
    "\n",
    "gamma = 0.90\n",
    "epsilon = 1.0\n",
    "k_epsilon = 0.995\n",
    "learning_rate = 0.1\n",
    "\n",
    "max_iterations = 2000\n",
    "\n",
    "seed_list = [42, 63, 8042, 10328, 4174]\n",
    "\n",
    "save_dir = \"./mfrl_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class 'Env', 'Dynamics', and 'Env_rl_d'\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics = Dynamics(env)\n",
    "mdp = Env_rl_d(env=env, dynamics=dynamics, num_states=num_states, num_actions=num_actions, dt=1/freq, build_stochastic_mdp=False)\n",
    "\n",
    "# Instantiate the MCRL controller class\n",
    "controller_mcrl = MCRLController(mdp, freq, epsilon=epsilon, k_epsilon=k_epsilon, \n",
    "                                    learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations, name=\"Monte Carlo Method\")\n",
    "\n",
    "# Instantiate the Q-learning controller class\n",
    "controller_ql = QLearningController(mdp, freq, epsilon=epsilon, k_epsilon=k_epsilon,\n",
    "                                    learning_rate=learning_rate, gamma=gamma, max_iterations=max_iterations, name=\"Q-Learning\")\n",
    "\n",
    "# Set up the controllers\n",
    "controller_instances = {\n",
    "    \"MCRL\": controller_mcrl,\n",
    "    \"QLearning\": controller_ql\n",
    "}\n",
    "\n",
    "# Set up the RLExperimentRunner\n",
    "runner = RLExperimentRunner(\n",
    "    controller_instances=controller_instances,\n",
    "    seed_list=seed_list,\n",
    "    save_dir=save_dir\n",
    ")\n",
    "\n",
    "# Run all controllers with all seeds and save the results\n",
    "runner.run_all()\n",
    "runner.plot(\"Monte Carlo Method vs. Q-Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf17975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained controllers for simulation\n",
    "controller_mcrl = runner.get_trained_controller(name=\"MCRL\", seed=seed_list[0])\n",
    "simulator_mcrl = Simulator(dynamics, controller_mcrl, env, 1/freq, t_terminal)\n",
    "simulator_mcrl.run_simulation()\n",
    "\n",
    "controller_ql = runner.get_trained_controller(name=\"QLearning\", seed=seed_list[0])\n",
    "simulator_ql = Simulator(dynamics, controller_ql, env, 1/freq, t_terminal)\n",
    "simulator_ql.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mcrl = Visualizer(simulator_mcrl)\n",
    "visualizer_mcrl.display_contrast_plots(\"Monte Carlo Method vs. Q-Learning\", simulator_ql)\n",
    "visualizer_mcrl.display_contrast_animation_same(simulator_ql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30e9c4",
   "metadata": {},
   "source": [
    "The comparative results between **Monte Carlo RL (MCRL)** and **Q-Learning** on the same hilly terrain task yield the following insights:\n",
    "\n",
    "* **MCRL** benefits from **unbiased return estimation**, which helps it eventually reach a near-optimal solution. However, its reliance on full-episode returns leads to **higher variance**, especially in the early stage of training. This can result in less stable learning curves and lower repeatability across runs. Despite this, MCRL sometimes outperforms Q-Learning in final performance when training is sufficiently long.\n",
    "\n",
    "* **Q-Learning**, by contrast, exhibits **faster initial convergence** due to its recursive temporal-difference update mechanism. It has **lower variance** across seeds, resulting in **smoother and more stable training**. However, its bootstrapping nature may introduce bias, which occasionally limits its ability to reach the globally optimal value compared to MCRL.\n",
    "\n",
    "Overall, while both algorithms can achieve comparable success rates after sufficient training, Q-Learning offers better training stability and efficiency, whereas MCRL may achieve slightly better peak performance if variance is well managed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671a772",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **üí° Take-away: Monte Carlo Method vs. Q-Learning**\n",
    "\n",
    "* **Monte Carlo RL** provides **unbiased return estimation** by averaging full-episode rewards, which can lead to highly accurate value predictions in the long run. However, its reliance on complete episodes and high variance may result in unstable or slow convergence.\n",
    "\n",
    "* **Q-Learning** offers **more stable and sample-efficient learning** through recursive temporal-difference updates. Though slightly biased, its lower variance and online nature allow faster convergence and smoother training dynamics in practice.\n",
    "\n",
    "</blockquote> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e700b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Comparison of Control and Reinforcement Learning Paradigms**\n",
    "\n",
    "In the previous sections, we have explored three different paradigms for sequential decision-making: **Model-Based Control** (e.g., MPC), **Model-Based Reinforcement Learning** (e.g., Value Iteration, Policy Iteration, GPI), and **Model-Free Reinforcement Learning** (e.g., Monte Carlo method, Q-learning). Each of these methods offers distinct strengths and trade-offs, depending on the availability of system dynamics, the nature of the state and action spaces, and computational constraints. Below we provide a structured comparison across several key dimensions:\n",
    "\n",
    "<br>\n",
    "\n",
    "- In terms of Model:\n",
    "\n",
    "| **Dimension**            | **Model-Based Control (MPC)**                                  | **Model-Based RL (VI/PI/GPI)**                              | **Model-Free RL (Monte Carlo method/Q-learning)**                         |\n",
    "| ------------------------ | -------------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Need for dynamics**     | System dynamics required                                         | MDP required (transition probabilities & rewards) | No (all learned from samples)                                             |\n",
    "| **Type of dynamics**     | Continuous / discrete-time                                     | Discrete-time | Discrete-time                                       |\n",
    "| **State/Action Space**   | Continuous | Discrete (tabular, continuous requires discretization)       | Discrete (tabular, continuous requires discretization)                                          |\n",
    "\n",
    "<br>\n",
    "\n",
    "- In terms of computational burden:\n",
    "\n",
    "| **Dimension**            | **Model-Based Control (MPC)**                                  | **Model-Based RL (VI/PI/GPI)**                              | **Model-Free RL (Monte Carlo method/Q-learning)**                         |\n",
    "| ------------------------ | -------------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Optimization Timing**  | Online (solves OCP in real-time)                                  | Offline (solves DP using model)                             | Offline (learns directly from interaction)                  |\n",
    "| **Offline Computation Time**     | -                       | High (but depends on grid size)                    | Moderate (depends on number of episodes) |\n",
    "| **Online Computation Time**     | High (due to online QPs)                       | Low                   | Low |\n",
    "| **Scalability**     | Moderate (porpotional to state/input dimension)                       | Poor (exponential in number of bins)                   | Better than the other two (learning through sampling) |\n",
    "\n",
    "<br>\n",
    "\n",
    "- In terms of learning properties:\n",
    "\n",
    "| **Dimension**            | **Model-Based Control (MPC)**                                  | **Model-Based RL (VI/PI/GPI)**                              | **Model-Free RL (Monte Carlo method/Q-learning)**                         |\n",
    "| ------------------------ | -------------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Data Requirement**     | Low (default not learning from data)                                 | Low (default not learning from data)                               | High (samples collected through interaction)                |\n",
    "| **Exploration Strategy** | -                                     | Implicit via value improvement                              | Explicit via Œµ-soft policy or random sampling               |\n",
    "| **Convergence & Stability**     | Deterministic and stable under proper design                                 | Can converge, but may be unstable if model is inaccurate                               | Slower convergence, sensitive to hyperparameters                |\n",
    "| **Optimality** | High (provably optimal under known system dynamics and cost design) | High (provably optimal under known MDP model and reward structure) | Often suboptimal (limited by data efficiency and exploration challenges) |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Summary of Each Method**\n",
    "\n",
    "* **Model-Based Control:**\n",
    "  Provides high performance with accurate models and works well in continuous domains. However, real-time optimization can be computationally intensive, especially in constrained or nonlinear problems. No learning is involved‚Äîrelies entirely on predictive models and open-loop optimization at each step.\n",
    "\n",
    "* **Model-Based RL:**\n",
    "  Bridges control and learning by computing value functions from a known model (MDP). It guarantees convergence to the optimal policy in discrete settings but is limited by discretization and computational burden in large-scale problems. It serves as a theoretical benchmark but is impractical in continuous domains.\n",
    "\n",
    "* **Model-Free RL:**\n",
    "  Trades off model accuracy for direct interaction-based learning. It is more robust to model mismatch and easier to implement when dynamics are unknown. However, it typically requires large amounts of data and careful exploration strategy design to achieve comparable performance. Similar to model-based RL, its classical tabular formulation is limited to discrete domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704b7fe",
   "metadata": {},
   "source": [
    "> <br>\n",
    "> üìå <b>Bridging the Gap: From Classical Reinforcement Learning to Deep Reinforcement Learning</b>\n",
    ">\n",
    "> These limitations highlight the necessity of **Deep Reinforcement Learning (DRL)**, which combines neural function approximation with model-free learning paradigms to scale RL to high-dimensional, continuous problems.\n",
    ">\n",
    "> <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
